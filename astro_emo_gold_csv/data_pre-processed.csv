paperId,title,year,pdf_url,motivation_free_txt,motivations,datasets,emotions
069ae43eab55150e2b54785cfdbe12b04b80ce9f,State of Mind: Classification through Self-reported Affect and Word Use in Speech,2018,https://opus.bibliothek.uni-augsburg.de/opus4/files/44179/2043.pdf,"""Human state–of-mind (SOM; e.g.: perception, cognition, attention) constantly shifts due to internal and external demands. Mental health is influenced by the habitual use of either adaptive or maladaptive SOM...Continuous and adaptive regulation of SOM is pivotal for mental functioning and behaviour regulation [1]. One key aspect of the human experience are our emotions...Psychotherapy is a method to change ones SOM through the use of language and a corrective therapeutical relationship...This principle could be used in e- and m-health intervention or in the classical psychotherapeutic setting to enhance individuals’ emotion regulation abilities""",['Healthcare (Mental health)'],['Custom collected for this work'],"['Arousal/Activation', 'Valence or sentiment']"
60e14f8b4d6a35aa8ca08987921c3dced2ebdc83,Detecting Unipolar and Bipolar Depressive Disorders from Elicited Speech Responses Using Latent Affective Structure Model,2020,https://ieeexplore.ieee.org/document/8288569/,"MOOD disorder is a kind of mental illness which severely affects how you feel and think...As a mood is an emotional state, emotion expression or perception are relevant to mood disorder, and has been used for mood disorder detection [24]",['Healthcare (Mental health)'],['Custom collected for this work'],"['Happy', 'Fearful', 'Angry', 'Surprised', 'Sad', 'Disgusted']"
5573638a1ac9de73a37fb07b0fb30296aafa2083,Towards Disorder-Independent Automatic Assessment of Emotional Competence in Neurological Patients with a Classical Emotion Recognition System: Application in Foreign Accent Syndrome,2021,http://bth.diva-portal.org/smash/get/diva2:1460935/FULLTEXT01,"""Lesions in the brain getting in the way of thevspeech program can cause failures in the production of emotive speech at prosodic or segmental levels....In many disorders, emotive speech is sensitive to
disease progression and recovery. In other words, it is a good candidate biomarker to be included into the panel of others that are taken into account.""",['Healthcare (Mental health)'],"['Custom collected for this work', 'Interface Database']","['Angry', 'Sad', 'Happy', 'Neutral']"
04e9d0189dfee6e876a12e618f123aa66d6f8615,Detecting anger in automated voice portal dialogs,2006,https://www.isca-archive.org/interspeech_2006/burkhardt06_interspeech.html,"Anger detection is a topic that is gaining more and more attention with voice portal carriers, as it can be useful for quality measurement and empathic dialog strategies [1, 2]. In the context of customer care voice portals it can be helpful to detect potential problems that arise from a unsatisfactory course of interaction in order to help the customer by either offering the assistance of human operators or trying to react with appropriate dialog strategies.",['Call center / call screening'],['Custom collected for this work'],['Angry']
f073bad3c36706de5e42083a4241b2d7c12ca691,Two-stream emotion recognition for call center monitoring,2007,https://www.isca-archive.org/interspeech_2007/gupta07_interspeech.html,"""Over the last decade, enterprises have started to use human operated call-centers to provide improved services to their customers...So the supervisors monitor a subset of calls and identify if any of them had extreme emotional characteristics (such as happy or angry moods)""",['Call center / call screening'],"['EPST', 'Custom collected for this work']","['Angry', 'Happy', 'Neutral']"
3f99b618194b27dc144e18c390037be315faffe9,Frequency distribution based weighted sub-band approach for classification of emotional/stressful content in speech,2003,https://www.isca-archive.org/eurospeech_2003/rahurkar03_eurospeech.html,"The
results suggest a important step forward in establishing an
effective processing scheme for developing generic
models of neutral and emotional speech. ...would take us a step closer to a generic processing sequence for obtaining models of new emotions in a fixed speaker environment of new input speakers for voice telephony or interactive systems. Another motivation for addressing emotional/stressful speech classification is to better understand physical speech modeling and associated speech feature estimation procedures....This property is especially useful here
because we believe that when a person is under stress,
physiological changes occur in their vocal fold movement,
which modulates airflow in the vocal tract. These changes
in the airflow properties are perceived by listener as
emotional content.
Such an emotion classification system could not only
be used to increase robustness in speech recognition it
could also contribute to improved systems for to handhelds, interactive books, and intelligent dialogue
systems. These are some of the many areas where such a
system can play a vital role. ","['Paralinguistics / behavioral studies', 'Responsive bots: voice assistants', 'Video Games, Toys, Entertainment', 'Responsive bots: other HCI systems']",['SOQ'],"['Neutral', 'Stressed']"
50b26f3e8e8f1cca4a0ccb19559fca302ef94f57,Multistyle classification of speech under stress using feature subset selection based on genetic algorithms,2007,https://pdf.sciencedirectassets.com/271578/1-s2.0-S0167639307X0164X/1-s2.0-S0167639307000830/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEML%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIBlrxXaD%2Br6%2FGTwJfV%2FP7G6J9Cc43V6YBU39odt6LNrIAiEArq4D6qnohVlcS5tkt0p%2BzeUi9TI44dRkfRBnZO4P1IcqvAUI2v%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDPID9Hj51RNpJ%2Fe7myqQBevypPvpA2uA0zDrTbXO%2By3Ez1arP%2BKbAqnFnwgoY8Xnf2YGLN1U2%2FLx6R3FoLvAlKtu0f1OOh2JbX%2FdXhNwI4sqLhMInFT39DhopGMbYb5em3KUdM5VjfblM9nH6l6G21hYbFKnClmJyHLEtZfXPag6DkE9zLTO0OXrz8idhXkwo5UzzQ%2BW0mJTcLg3QrT%2FTU7u5R9D3qnfeDVG7M%2FEBgN5%2BIRrcJ10u0Y7r0hT8VJwCVUFfcGBR8%2BPrpHnSTtWAneFXtnsVzr8cn1FuTY58iVmle64V45IzNctqqfiq3uEjWphpzf1Bpsh4sk0AzW9PGrcgVrHrbvIu%2FYPrQt3gHBi3%2BEG0%2BU5SjpqrncQRvM0H1a40E8ZJKMo3OHUw%2FwcQgy1oNGR4XWGjSwatCUAIuImpgAXwN4Vylm4C1mri2C4o5eYhjyNo%2FG%2BuUELl6iPxHg%2FjT8dqU7c1Z8DwuIdvMv367BGWgXWeUSuSNjHWRpoWdS02H9na%2BaVw1C5GxAS4w2d67QrjArPUBQHmQVYJQFspUws%2BB%2FE7QDBhvUlVQP5Zh5Ze62aITeDvKAW5lAn5l32tDboYGK35bxOR%2BXNk9%2B59%2FES2P2rEA8szPc34lVxbVFqVEi7cFXmGC5el4qoIXGKnnYUqUJEwAuLTEuoCIfbj%2F6Av2%2FX1c%2FAEt51Boas4Zx991iwG7wdS6bBcnZx%2B9Krq3fzL1fsh7V06MNA%2FYVc%2FQ8BgPpYNxuHqKgC8qcG7D%2BqjoM%2BcYkfXmp%2B5ycGeKpN0AaRGqDothZYzSodRrEDtfWZE6Npm9hWWvJjHqUG%2FitRovVfemmgDbxZ7ex98liI5HOJG0Bl5mmZiI79EBuktWGNJSS0RVEjl8pjFDYSMIOp4rYGOrEBa5ikuakidPMHVTvYSRQxAXPA41O5ENQ6pzJB5L6JqCSsZdRMxY%2BKKX0ctTxF4tIiB6nydcBfLvlu0Q0YnhHWi0h97LWEO7Eh9phl5JD0zu6gijulaJ3ziItvON3vZ8mCw%2FoWNqUpQMlu7ZafrAFHqjTuZJ8vJUjjNCzfgDi%2BEZ80j6ND9PrQDKIArbwNNgvPf60cS3FmH8qQagNYasaLN7nnpklIJ9WmJBIMnqeJkvGm&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240904T180435Z&X-Amz-SignedHeaders=host&X-Amz-Expires=299&X-Amz-Credential=ASIAQ3PHCVTY5MKMAYWO%2F20240904%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=b13ac483cff835de57fc3f4fb9a08fc0b0b1bc2859341be7efa63192dd599fc8&hash=a2a445c3fac846e66b286c683993532741e644540c430f7b9c43799c897a6e5a&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0167639307000830&tid=spdf-fd0d71c5-895d-4c2f-8bb6-d83877967b40&sid=372daaf19cd1054ddc395a71803abde0afc0gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=10145d07030600555d53&rr=8bdfe7213e5238aa&cc=us,"One important research challenge in the last few years has thus been automatic recognition of the emotional state of a speaker through speech; knowledge of this state can have a series of applications, such as (Cowie et al., 2001): • automatic answering machines that can adapt their voice tone to that of the speaker; • speech recognition systems that can correctly interpret the meaning of words spoken in an ironic or sarcastic manner;• systems which are able to detect the emotional state of
an interlocutor (for clinical diagnosis purposes, for
example);
• synthesizers that can generate speech giving the sensation of a particular emotion;
• automatic tutoring systems that can establish a learner’s
degree of boredom, irritation or intimidation;
• systems that can prevent speakers in a particularly
altered emotional state from interacting with automatic
recognition systems;
• systems to generate alarms on the basis of the different
emotional states of people being monitored;
• entertainment systems and games that can determine a
user’s emotional state.

The determination of an emotional state is therefore an
important task from several points of view. What is studied
more frequently than the recognition of the speaker’s emotional state in general is classification of speech under stress
(e.g., Hansen and Womack, 1996; Bou-Ghazale and
Hansen, 2000; Zhou et al., 2001; Nwe et al., 2003). Several
elements introduce stressed voice tones in the production
of speech, for example background noise, emergencies,
high workloads, strong emotional excitement, etc. It is
well-known that the performance of speech recognition
algorithms is greatly influenced by the stressful conditions
in which speech is produced. Workload task stress significantly impacts recognition performance. Effects of different
stressful conditions on speech recognition and efforts to
improve the performance of speech recognition algorithms
under stressful conditions can be found in literature. Stress
classification cannot only be used to improve the robustness of speech recognition systems, other scenarios can
also benefit, such as telecommunications, military applications, medical applications, and law enforcement. In telecommunications, in addition to its potential to improve
the telephone-based speech recognition performance, stress
classification can be used to route 911 emergency call services for high priority emergency calls. The integration of
speech recognition technology has already been seen in
many military voice communication and control applications. Since many such applications involve stressful environments (e.g., aircraft cockpits, military peacekeeping/
battlefield setting), stress classification and assessment
become crucial to improve the system robustness in these
applications. Finally, stress classification can also be
employed in forensic speech analysis by law enforcement
to assess the state of telephone callers or as an aid in
suspect interviews.","['Responsive bots: other HCI systems', 'Healthcare (Mental health)', 'Other', 'Video Games, Toys, Entertainment', 'Education, Tutoring']",['SUSAS'],"['Angry', 'Stressed', 'Neutral']"
53c306825752cb0562f19736980a8412bae7cc34,Stress and Emotion Classification using Jitter and Shimmer Features,2007,https://epublications.marquette.edu/cgi/viewcontent.cgi?article=1008&context=data_drdolittle,"""This task has application to a number of important areas, including security systems, lie detection, video games and psychiatric aid, among others.
Similarly, the analysis of arousal levels from animal vocalizations can significantly improve the ability of researchers in bioacoustics to understand animal behavior.""
""... both jitter and shimmer can be indicators
of underlying stress in human speech and animal vocalizations. The investigation of both human speech and animal vocalizations under the same framework will give the features wider applicability and better support their effectiveness""","['Healthcare (Mental health)', 'Other', 'Lie detection', 'Video Games, Toys, Entertainment']","['SUSAS', 'African Elephant Emotional Arousal Dataset', 'Custom collected for this work']","['Other or Unspecified', 'Angry', 'Arousal/Activation', 'Neutral']"
293fd134ba4bee4dddb7e60c223da0737cff2263,Why and how to control the authentic emotional speech corpora,2003,https://www.isca-archive.org/eurospeech_2003/auberge03_eurospeech.html,"""In such a view, the verbal communication needs a coherent affective processing in order to be adapted to the situation. It would mean in particular that not to choose to control the affective speech information in synthesis and recognition implies not only naturalness or agreement lacks, but above that, it perturbs the goals of the interaction. The interest for affective technologies is increasing [3], as well for the conversational chatterbots (Embodied Conversational Interface Agents of Cassel, Believable Social and Emotional Agents of Bryan Loyall, Affective Agents of Picard…)""","['Responsive bots: other HCI systems', 'Responsive bots: voice assistants']",['Custom collected for this work'],['Other or Unspecified']
9c9e452376340e26faadfd4af300aab2cc53621a,Integrating information from speech and physiological signals to achieve emotional sensitivity,2005,https://opus.bibliothek.uni-augsburg.de/opus4/files/47270/Integrating Information from Speech and Physiological Signals to Achieve.pdf,"In the past few years, increasing attempts have been made to exploit emotional cues in man-machine communication. The driving force behind this work is the insight that a user interface is more likely to be accepted by the user if it is sensitive towards the user’s affective states. A necessary prerequisite to realize such a behavior is the availability of robust methods for the recognition of emotions from various channels of expression as well as the context of interaction. Most research so far has focused on the analysis of a single modality or an integrated analysis of audio-visual data (see [1] for a comprehensive overview). In this paper, we will concentrate on speech in combination with physiological measures, a topic which has been largely neglected so far.",['Responsive bots: other HCI systems'],['Custom collected for this work'],"['Arousal/Activation', 'Valence or sentiment']"
571c6d8a10c0cb11724f20b18e65aa67a58a28bc,Robust speech recognition using inter-speaker and intra-speaker adaptation,2002,,"However, in spontaneous speech or in other styles of speech, the intra-speaker variation of acoustic features comes larger, degrading the performance of speech recognition a lot. ...To improve recognition performance for speech other than calmly read one, the issue of intra-speaker variation should be addressed.",['Prior work'],['Custom collected for this work'],"['Angry', 'Other or Unspecified', 'Sad', 'Neutral']"
e25c3160fa98ee7400683ebee16358c4adc74360,Speech interaction with an emotional robotic dog,2008,https://www.isca-archive.org/interspeech_2008/jones08_interspeech.html,"Entertainment robots have found there way into the home.
Most common are robot pets [1] that can be controlled
remotely or programmed with simple actions [2]. In the 2004
film I, Robot, an android called Sonny interrupts a row
between the main characters, saying Excuse me. I note there
are elevated stress patterns in your speech. Although much of
the autonomous intelligence and abilities for complex
communications may be some way off for consumer robots,
recognising stress and emotion in the human voice is possible.
Acoustic emotion recognition [3] or voice stress analysis [4]
offers a range of commercial benefits. Call centres can track
the conversations between callers and agents for emotive
events to help agents better support caller frustrations, aid
agent training and detect fraudulent insurance claims [5]. Incar systems can recognise driver emotion and react to drowsy or angry drivers to help them drive more safely [6-7].
Computer games can recognise player emotion and adapt game
play to maintain active engagement without stressing the
player with over challenging game play [8-9].","['Call center / call screening', 'Other', 'Responsive bots: car voice assistants', 'Video Games, Toys, Entertainment', 'Lie detection']",['Custom collected for this work'],"['Happy', 'Boredom', 'Sad', 'Surprised', 'Angry', 'Other or Unspecified', 'Excited', 'Fearful', 'Frustrated']"
e856f4a5609b676e15fffd82487dc04dc17b9a09,Balancing spoken content adaptation and unit length in the recognition of emotion and interest,2008,https://opus.bibliothek.uni-augsburg.de/opus4/files/76685/vlasenko08_interspeech.pdf,"Detecting non-lexical or paralinguistic cues from speech is one of the major challenges in the development of usable human machine interfaces (HMI). Notable among these cues are the universal categorical emotional states (e.g. anger, boredom, disgust, fear, joy, neutral, sadness, etc.) and/or level of interest (neutrality, interest, curiosity), prevalent in day-to-day scenarios. Knowing such emotional states and/or levels of interest can help adjust system responses so that the user of such a system can be more engaged and have a more effective interaction with the system.",['Responsive bots: other HCI systems'],"['EMO-DB', 'SUSAS']","['Angry', 'Boredom', 'Disgusted', 'Fearful', 'Joy', 'Neutral', 'Sad', 'Stressed']"
06885af16f856ea5513026850dbb91c13d5c0dde,Processing affected speech within human machine interaction,2009,https://www.isca-archive.org/interspeech_2009/vlasenko09_interspeech.html,"The importance of human behavior based dialog strategies in
human machine interaction (HMI) lies in existing limitations
of automatic speech recognition technology. Current state-ofthe-art Automatic Speech Recognition (ASR) approaches still
cannot deal with flexible, unrestricted user’s language and emotional prosody colored speech[1]. Therefore, problems caused
by misunderstanding a user who refuses to follow a predefined,
and usually restricting, set of communicational rules seems to
be inevitable.
In the domain of human-machine interaction [2], we witness the rapid increase of research interest in affected user behavior. However, some aspects of affected user behavior during
HMI still turns out to be a challenge for developers of Spoken
Dialog Systems (SDS).",['Responsive bots: other HCI systems'],"['Kiel Corpus of Read Speech', 'EMO-DB', 'FAU Aibo Emotion Corpus']","['Angry', 'Other or Unspecified', 'Neutral', 'Joy']"
e34305120d7eef2f8cf24e44d95dc38b44f5c788,Evaluating evaluators: a case study in understanding the benefits and pitfalls of multi-evaluator modeling,2009,https://www.isca-archive.org/interspeech_2009/mower09_interspeech.html,"""These models could lead to computer agents and robots that more naturally and functionally blend into human society""",['Responsive bots: other HCI systems'],['IEMOCAP'],"['Angry', 'Happy', 'Sad', 'Neutral', 'Valence or sentiment', 'Arousal/Activation']"
c8de7c60c4ade1af1023127dd060be1a02147215,Domain adaptation and compensation for emotion detection,2010,https://www.isca-archive.org/interspeech_2010/sanchez10_interspeech.html,"""we are interested in predicting the emotional status of 911 emergency-hotline
callers""",['Call center / call screening'],['Custom collected for this work'],"['Fearful', 'Sad', 'Neutral']"
78e838bcd2268260ddce6be6db4907df6f29f04f,Data-driven generation of text balloons based on linguistic and acoustic features of a comics-anime corpus,2014,https://www.isca-archive.org/interspeech_2014/matsumiya14_interspeech.html,"Most automatic speech recognition systems existing today are still limited to recognizing what is being said, without being concerned with how it is being said. On the other hand, research on emotion recognition from speech has recently gained considerable interest, but how those emotions could be expressed in text-based communication has not been widely investigated. Our long-term goal is to construct expressive speech-to-text systems that conveys all information from acoustic speech, including verbal message, emotional state, speaker condition, and background noise, into unified text-based communication. In this preliminary study, we start with developing a system that can convey emotional speech into text-based communication by way of text balloons. As there exist many possible ways to generate the text balloons, we propose to utilize linguistic and acoustic features based on comic books and anime films. Experimental results reveal that expressive text is more preferable than static text, and the system is able to estimate the shape of text balloons with 87.01% accuracy.",['Other'],['Custom collected for this work'],"['Angry', 'Sad', 'Happy', 'Fearful', 'Neutral']"
994217e31e6321fa247537100d3598da8ea6786a,A New Evaluation Methodology for Speech Emotion Recognition With Confidence Output,2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6926050,"An emotion conveys important information to comprehend the meaning of an utterance expressed by a person [1]. This can be seen among other things in the fact that vocal emotions can be comprehended by young children even before they can understand words [2]. As a consequence, speech emotion recognition receives more and more attention to improve the performance of human-machine communication in recent years [1].",['Responsive bots: other HCI systems'],['EMO-DB'],"['Fearful', 'Disgusted', 'Happy', 'Boredom', 'Neutral', 'Sad', 'Angry']"
8b4e77a70ed4b3587a5e8f9c736d94544762e257,System supporting speaker identification in emergency call center,2015,https://www.isca-archive.org/interspeech_2015/gaka15_interspeech.pdf,"To get help in emergency situations, we usually call a publicsafety answering point (PSAP) in the first place. This essential
service has to be very fast and as reliable as possible given current technology capabilities. ",['Call center / call screening'],['Custom collected for this work'],"['Angry', 'Stressed', 'Neutral']"
171777316ca28e5bd8abea06bc451008ff1b849d,Predicting Arousal and Valence from Waveforms and Spectrograms Using Deep Neural Networks,2018,https://www.isca-archive.org/interspeech_2018/yang18c_interspeech.html,"""In recent years, increasing attention has been given to the study of the emotional content in speech signals, and many systems have been proposed for automatic emotion recognition in speech.""",['Prior work'],"['RECOLA', 'includes AVEC 2012']","['Valence or sentiment', 'Arousal']"
cea07906fa0d9a2710ed3b1fb44d43e4a65f2c06,A Cross-Corpus Study on Speech Emotion Recognition,2019,https://arxiv.org/pdf/2207.02104.pdf,"However, the majority of this cross-corpus work has been
cross-lingual and occasionally both cross-lingual and crossage. In the past this has been arguably necessary given the
sparsity of emotional databases. But now with the introduction of larger datasets, such as the MOSEI database which
has around 65 hours of natural emotion data [20], more effort can be put on cross-corpus research focussing on a single
language and a single age group.",['Prior work'],"['eNTERFACE', 'RAV', 'IEMOCAP', 'MOSEI']","['Angry', 'Happy', 'Sad', 'Surprised', 'Disgusted', 'Fearful', 'Neutral', 'Frustrated', 'Excited', 'Calm', 'Other or Unspecified']"
8efcf96dcb93f992025e5cb5ad9d72b1e1795481,CycleGAN-Based Emotion Style Transfer as Data Augmentation for Speech Emotion Recognition,2019,https://www.isca-archive.org/interspeech_2019/bao19_interspeech.html,Automatic speech emotion recognition (SER) is an important and rapidly growing research field with great potential to improve naturalistic voice-based human-computer interfaces,['Responsive bots: other HCI systems'],"['IEMOCAP', 'MSP-IMPROV', 'release 2']","['Angry', 'Happy: Happy + Excited', 'Sad', 'Neutral']"
5a72ea19ac47f5cb3d049cc7356e4f644e46c86c,Deep Learning of Segment-Level Feature Representation with Multiple Instance Learning for Utterance-Level Speech Emotion Recognition,2019,,"Especially in the field of human-machine interaction (HCI), growing interest can be observed in recent years. In addition, the detection of lies, monitoring of call centers and medical diagnoses are often claimed as promising application scenarios for speech emotion recognition.","['Healthcare (Mental health)', 'Call center / call screening', 'Lie detection']","['IEMOCAP', 'CASIA']","['Sad', 'Neutral', 'Angry', 'Fearful', 'Happy', 'Surprised']"
269906a5bf303780a7f5060e4da8dc9cd28dd5e1,Feature Selection Based Transfer Subspace Learning for Speech Emotion Recognition,2020,https://ieeexplore.ieee.org/document/8276251/,"In affective computing field, emotion recognition from speech plays a very important role, and has received much attention over the past few decades. Speech emotion recognition aims to recognize emotions from speech into the following categories, e.g., neutral, happiness, sadness and surprise [1]. It has been proven useful in many applications which require human-machine interaction, e.g., in-car board system, diagnostic tool for therapists, automatic translation systems, computer tutorial applications [1], [2], [3]","['Healthcare (Mental health)', 'Other', 'Responsive bots: voice assistants', 'Education, Tutoring']","['EMO-DB', 'eNTERFACE', 'FAU Aibo Emotion Corpus']","['Angry', 'Boredom', 'Disgusted', 'Fearful', 'Happy', 'Sad', 'Surprised']"
6460db30997412afb41e1e3a2286606eddcf3ee0,Does Visual Self-Supervision Improve Learning of Speech Representations for Emotion Recognition?,2020,https://arxiv.org/pdf/2005.01400.pdf,"Intro: ""Deep neural networks trained in a supervised manner are a popular contemporary choice for various speech related tasks such as automatic speech recognition (ASR), emotion recognition and age/gender recognition. However they are a double-edged sword by virtue of providing extremely good performance given that large scale annotated data is available, which is usually expensive""
RW: ""Audiovisual emotion recognition has also seen a significant amount of recent research efforts. Automatic affect recognition has a variety of applications in various fields; from detecting depression [41], to more emotionally relevant advertising [42], [43], [44].""","['Healthcare (Mental health)', 'Other']","['CREMA-D', 'RECOLA', 'RAV', 'SEWA', 'IEMOCAP']","['Angry', 'Fearful', 'Disgusted', 'Happy', 'Sad', 'Neutral', 'Calm', 'Surprised', 'Arousal/Activation', 'Valence or sentiment']"
f6b80025294415c9a0dd760387cf1c34e465058a,"ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents",2020,https://www.aclweb.org/anthology/2020.acl-demos.31.pdf,"""Dialog systems or chatbots, both text-based and multi-modal, have received much attention in recent years, with an increasing number of dialog systems in both industrial contexts such as Amazon Alexa, Apple Siri, Microsoft Cortana, Google Duplex, XiaoIce...The purpose we envision for dialog systems developed using our toolkit is not the same as the objective of a social chatbot such as XiaoIce (Zhou et al., 2018). Rather than promoting “an AI
companion with an emotional connection to satisfy the human need for communication, affection, and social belonging” (Zhou et al., 2018), ADVISER helps develop dialog systems that support users in efficiently fulfilling concrete goals, while at the same time considering social signals such as emotional states and engagement levels so as to remain friendly and likeable.""","['Responsive bots: voice assistants', 'Social Companion bots']",['IEMOCAP'],"['Angry', 'Happy', 'Sad', 'Neutral', 'Arousal/Activation', 'Valence or sentiment']"
5300c2b50ae776488bd773ccdf4924fba5cd8867,"Deep Architecture Enhancing Robustness to Noise, Adversarial Attacks, and Cross-corpus Setting for Speech Emotion Recognition",2020,https://opus.bibliothek.uni-augsburg.de/opus4/files/90748/latif20b_interspeech.pdf,"""Despite the significant progress in Speech Emotion Recognition
(SER) through Deep Neural Networks (DNNs), SER systems
still perform poorly in noisy environments [1, 2], and when the
imperceptible adversarial perturbation is added to test examples
[3]""

The performance of state-of-the-art SER also degrades in the cross-corpus setting when an acoustic mismatch between training and testing exists [4]. This shows that SER systems lack robustness and generalisation which makes them susceptible to unknown test data shifts.",['Prior work'],"['IEMOCAP', 'MSP-IMPROV', 'DEMAND']","['Angry', 'Neutral', 'Sad', 'Happy: Happy + Excited']"
5c6942e94661dd67933c660b3661e9e5cb26ccbd,A Multi-Scale Fusion Framework for Bimodal Speech Emotion Recognition,2020,https://www.isca-archive.org/interspeech_2020/chen20b_interspeech.html,"Recent years have witnessed significant advances in the artificial intelligence field, the human-computer interaction (HCI), which is an important part, is being performed through a variety of softwares, smart devices and so on. Understanding user intention is an essential factor in enriching user experience and it has become a hot topic in both research and industry areas. Just as people communicate with each other, speech plays a crucial role in capturing the explicit content messages and even the underlying intentions. That is because speech contains rich linguistic and para-linguistic information conveys the implicit information such as emotions [1].",['Responsive bots: other HCI systems'],['IEMOCAP'],"['Happy: Happy + Excited', 'Neutral', 'Angry', 'Sad']"
54cd75e17690e051fcac3ee0503ea35be406efa5,An Attribute-Aligned Strategy for Learning Speech Representation,2021,,"The first is privacy due to concerns of sensitive information leakage: for example, users may not expect to disclose their identity information while using a speech emotion recognition (SER) system; on the other hand, users may not wish to share their emotional condition when being assessed by a speaker recognition (SR) system. Moreover, the collective social norm would create unwanted and often detrimental self-exaggerated issues around equality, e.g., unfair biases toward gender types [6] or race [7], when using data-driven approaches for speech technology. Speech is an informative signal which contains personal sensitive attributes by nature; hence developing appropriate methods either to protect privacy information, such as identity and emotion, or to mitigate the undesired biases, like gender and race, is critical in the current era.",['Prior work'],['MSP-Podcasts'],"['Neutral', 'Angry', 'Sad', 'Happy', 'Disgusted']"
969d8d2742b945cbbc56099a2ee9c67216c107e3,A Novel Markovian Framework for Integrating Absolute and Relative Ordinal Emotion Information,2021,https://arxiv.org/pdf/2108.04605.pdf,"Speech is one of the most natural form of human communication and a key modality through with emotions are expressed. Consequently, speech emotion recognition (SER) has received increasing interest within the speech processing and Human-Computer Interaction research communities [1-4]",['Prior work'],"['RECOLA', 'IEMOCAP']","['Arousal/Activation', 'Valence or sentiment']"
6d54bbf5e23f1c6fe14c09962bbe40f3c0c0c52b,Exploration of a Self-Supervised Speech Model: A Study on Emotional Corpora,2022,https://www.pure.ed.ac.uk/ws/files/305860313/SLT_2022_final.pdf,"""With these questions in mind, we study wav2vec 2.0 [17] on emotional corpora, demonstrating how this type of selfsupervised model can be explored for downstream tasks.""",['Prior work'],"['IEMOCAP', 'RAV']","['Angry', 'Happy: Happy + Excited', 'Neutral', 'Happy', 'Calm', 'Sad', 'Fearful', 'Surprised', 'Disgusted']"
5c08a6a21716a3daca70f7a5b040656466549643,Learning Continuous Facial Actions From Speech for Real-Time Animation,2022,https://ieeexplore.ieee.org/document/9186776/,"""Nowadays when some aspects of life are intertwined with the virtual world, it is increasingly important for the computer to understand not only what the human wants but also how the human “feels”, via their facial expressions, so that the machine is able to produce a meaningful, pleasant feedback.""
""Furthermore, the audio recording carries not only the contextual sound units (phonemes), but also emotions of the speaker reflected in speed or intensity of her speech. Hence, it is beneficial for the computer to comprehend emotional states of the speaker, for instance, to make a joke when it perceives the user is happy, in an imaginary mutual human-machine conversation.""",['Responsive bots: other HCI systems'],"['RAV', 'SAVEE', 'VidTIMIT', 'GRID', 'GEMEP']","['Neutral', 'Calm', 'Happy', 'Sad', 'Angry', 'Fearful', 'Surprised', 'Disgusted']"
59e35539a085052df0e8eb61a2b7e79b84ef2981,End-to-End Label Uncertainty Modeling in Speech Emotion Recognition using Bayesian Neural Networks and Label Distribution Learning,2022,,"In the context of reliability in real-world applications, SER systems not only need to model ground-truth labels but also account for the subjectivity inherent in these labels [2], [8]",['Prior work'],"['AVEC’16', 'MSP-Conversation']","['Valence or sentiment', 'Arousal']"
0831fec02f37b327198bdbaa6c7de8f0c63f207d,Music Theory-Inspired Acoustic Representation for Speech Emotion Recognition,2023,https://ieeexplore.ieee.org/document/10163251/,"Research on emotion recognition can advance many applications, like distance education [30], social robots [31], video games[32], affective mirrors[33], and many others[34].","['Other', 'Social Companion bots', 'Video Games, Toys, Entertainment', 'Education, Tutoring']",['IEMOCAP'],"['Neutral', 'Happy: Happy + Excited', 'Sad', 'Angry', 'Valence or sentiment', 'Arousal/Activation', 'Dominance']"
4767943db0c3526a16b2e18c5338233e5a07c229,Achieving Fair Speech Emotion Recognition via Perceptual Fairness,2023,https://ieeexplore.ieee.org/document/10094984/,"""Emotion AI is a fast-developing contemporary technology, and the inclusion of a speech emotion recognition (SER) module enables machines to intelligently interact with humans [1, 2].""",['Responsive bots: other HCI systems'],['IEMOCAP'],"['Neutral', 'Happy', 'Angry', 'Frustrated', 'Sad']"
f278036c2546252222c2e8df454b2cd45789a9e6,What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions,2023,https://arxiv.org/pdf/2404.06702,"""To investigate what is learnt by LEAF, we train LEAF on three of the tasks where its performance has been perviously reported: emotion recognition, keyword spotting, and language identification [6]. The tasks were chosen to represent a range of speech processing applications as well as prediction accuracies ranging from quite high to somewhat low (refer Table 1).""",['Prior work'],['CREMA-D'],['Other or Unspecified']
5c112d7afee764a1466755417aa02e11bbec7c9c,Cross-Lingual Cross-Age Adaptation for Low-Resource Elderly Speech Emotion Recognition,2023,https://www.isca-archive.org/interspeech_2023/cahyawijaya23_interspeech.html,"Most studies on speech emotion recognition are centered on young-adult people, mainly originating from English-speaking countries [3, 4, 5, 6]. This demographic bias causes existing commercial emotion recognition systems to inaccurately perceived emotion in the elderly [7]. Despite the fast-growing elderly demographic in many countries [8], only a few studies with a fairly limited amount of data work on emotion recognition for elderly [9, 10, 11], especially from non-English-speaking countries [12].",['Prior work'],"['CREMA-D', 'ElderReact', 'ESD', 'TESS', 'YueMotion', 'CSED', 'IEMOCAP']","['Happy', 'Sad', 'Neutral', 'Disgusted', 'Fearful', 'Angry']"
5e5e5c59b96a37b097f91c9bda3206210a7e7c7c,Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations,2023,,"However, most AER systems operate on manually segmented utterances although manual segmentations are not generally available in practical use cases. Besides, automatic speech recognition systems trained on standard speech can give poor recognition performance on emotional speech",['Prior work'],['IEMOCAP'],"['Happy', 'Sad', 'Angry', 'Neutral']"
c2f3c3e07628709184ba7cd247dd226e684155e1,Classifying emotions in speech: a comparison of methods,2001,https://www.isca-archive.org/eurospeech_2001/amir01_eurospeech.html,"Various recent studies have dealt with characterization of
emotional speech and automatic classification of emotional
utterances [1,2,3,4]. ",['Prior work'],"['Douglas-Cowie, 2000']","['Angry', 'Sad', 'Happy', 'Neutral']"
52074194ceeec6a0eb20bea44a96f1372a08aa36,Speech emotion recognition using hidden Markov models,2001,,"Possible applications include from help to psychiatric diagnosis to intelligent toys, and is a subject of recent but rapidly growing interest","['Healthcare (Mental health)', 'Video Games, Toys, Entertainment']",['Interface Database'],"['Surprised', 'Joy', 'Angry', 'Fearful', 'Disgusted', 'Sad', 'Neutral']"
b07871e1607c518c0a99a7711a38456fbb868627,RUSLANA: a database of Russian emotional utterances,2002,https://www.isca-archive.org/icslp_2002/makarova02_icslp.html,"""Ways of expression and recognition of emotions by humans and animals have intrigued researchers for a long time. Charles Darwin published the first monograph devoted to the topic in the nineteenth century [1]. After this milestone work, psychologists have gradually accumulated knowledge in the field offering various descriptions of emotive and affective states (e.g., [2])...We also witness an urge to enrich human-computer interface from point-and-click to sense-and-feel. Lifelike software agents expand in our computerized working space, while our homes are invaded by robotic interactive toys, such as Tiger’s Furbies and Sony’s Aibo, who are supposed to be able to express and understand emotions. A new field of research in AI known as affective computing has recently been identified [3].""","['Video Games, Toys, Entertainment', 'Other']",['RUSLANA'],"['Neutral', 'Surprised', 'Happy', 'Angry', 'Fearful', 'Sad']"
9329ca4c37ef89c521234debe819056baf1a7e28,Emotion recognition by speech signals,2003,https://www.isca-archive.org/eurospeech_2003/kwon03_eurospeech.html,"If an entertainment robot recognize emotion, it could respond to its owner differently according to his/her emotional state. Emotion recognition by speech is one of research fields for emotional human-computer interaction or affective computing [1]","['Video Games, Toys, Entertainment', 'Responsive bots: other HCI systems']","['SUSAS', 'FAU Aibo Emotion Corpus']","['Stressed', 'Angry', 'Boredom', 'Happy', 'Neutral', 'Sad']"
bcd7a3d7e35ead16d51199e61f58ebcb4dab083e,A comparison of classifiers for detecting emotion from speech,2005,http://ieeexplore.ieee.org/document/1415120/,Accurate detection of emotion from speech has clear benefits for the design of more natural human-machine speech interfaces or for the extraction of useful information from large quantities of speech data. It can help design more natural spoken-dialog systems than those currently deployed in call centers or used in tutoring systems. The speaker’s emotion can be exploited by the system’s dialog manager to provide more suitable responses thereby achieving better task completion rates. Emotion detection can also be used to rapidly identify relevant speech or multimedia documents from a large data set.,"['Responsive bots: other HCI systems', 'Other', 'Call center / call screening', 'Education, Tutoring']",['Custom collected for this work'],['Valence or sentiment']
747e4663dfe353e8d9468aa8e72e792d8776fc88,Combining frame and turn-level information for robust recognition of emotions within speech,2007,https://opus.bibliothek.uni-augsburg.de/opus4/files/76717/76717.pdf,"Apart from a few attempts to classify emotions within speech dynamically [1,2], current approaches usually employ static feature vectors",['Prior work'],"['EMO-DB', 'SUSAS']","['Happy', 'Angry', 'Fearful', 'Boredom', 'Disgusted', 'Stressed', 'Neutral']"
b59d14e80dc5cdd126e61ea6146e95f58b14cb46,Support Vector Regression for Automatic Recognition of Spontaneous Emotions in Speech,2007,http://ieeexplore.ieee.org/document/4218293/,"Emotion recognition is important for human-machine interaction applications. For example, it is a key ingredient in the design of humanoid robots [1], where “emotional intelligence” is being increasingly added to artificial intelligence design [2].",['Responsive bots: other HCI systems'],['VAM'],"['Valence or sentiment', 'Dominance', 'Other or Unspecified']"
0279ecc913017e14f78b2b3494fab98344aa150a,Speech emotion recognition via a max-margin framework incorporating a loss function based on the Watson and Tellegen's emotion model,2009,http://ieeexplore.ieee.org/document/4960547/,"""Compared to other modalities, speech signal can be obtained more easily and inexpensively. For this
reason, it has a wider range of HMI applications: a service
robot that responds to the owner’s emotion, a computer game
that controls the game status by game-player’s emotion, and
an audio response system of the call center that automatically
connects the customer to the expert counsellor if the customer
is angry.""","['Call center / call screening', 'Video Games, Toys, Entertainment', 'Responsive bots: other HCI systems']",['EMO-DB'],"['Angry', 'Fearful', 'Disgusted', 'Sad', 'Boredom', 'Neutral', 'Happy']"
9f9e29e4b85b54d0c0dca97eef4d59059508ddd6,Speaker dependent emotion recognition using prosodic supervectors,2009,https://repositorio.uam.es/bitstream/10486/663471/1/speaker_lopez-moreno_Interspeech_2009.pdf,"Emotion recognition from the speech signal is an increasingly interesting task in human-machine interaction, with diverse applications in the speech technologies field such as call centres, intelligent auto-mobile systems, speaker intra-variability compensation or entertainment industry","['Responsive bots: car voice assistants', 'Video Games, Toys, Entertainment', 'Call center / call screening']",['SUSAS'],"['Neutral', 'Stressed']"
bfc50a913dfb3143c9861af55a2d41bdd34e6e70,A dimensional approach to emotion recognition of speech from movies,2009,http://ieeexplore.ieee.org/document/4959521/,"""For example, recognizing affective content of music signals ([3], [4]) can be used in a system, where the users will be able to retrieve musical data with regard to affective content. In a similar way, affective content recognition in video data could be used for retrieving videos that contain specific emotions. In this paper, we emphasize on affective content that can be retrieved from the speech information of movies.""",['Other'],['Custom collected for this work'],"['Valence or sentiment', 'Arousal']"
2d2c8825cbc8e5f9b18001cbe01b80b4edcffd30,Emotion Recognition of Affective Speech Based on Multiple Classifiers Using Acoustic-Prosodic Information and Semantic Labels,2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5674019,"""However, the applications of SDSs are still limited to simple informational dialog systems, such as navigation systems and air travel information systems [1], [2]. To enable more complex applications (e.g., home nursing [3], educational/tutoring, and chatting [4]), new capabilities such as affective interaction are needed.""","['Healthcare (Mental health)', 'Education, Tutoring']",['Custom collected for this work'],"['Angry', 'Happy', 'Neutral', 'Sad']"
537ba7a8fb9e97ea77b671e69e6c0d2c93e8dd73,A Preliminary Study on Cross-Databases Emotion Recognition using the Glottal Features in Speech,2012,https://www.isca-archive.org/interspeech_2012/sun12c_interspeech.html,"""While the majority of traditional research in emotional speech recognition has utilized a single database for analysis, it is becoming clear that the lack of sufficiently large databases with varied emotion types is a significant hindrance to the design of a robust emotion classification system.""",['Prior work'],"['EMO-DB', 'EPST', 'EMA']","['Neutral', 'Angry', 'Sad', 'Happy']"
63c52911721fd00916f96dbc132de54bdae6e164,Active learning for dimensional speech emotion recognition,2013,https://www.isca-archive.org/interspeech_2013/han13_interspeech.html,"Dimensional speech emotion recognition (SER) research has
recently gained increasing attention in the SER community",['Prior work'],['includes AVEC 2012'],"['Arousal/Activation', 'Dominance', 'Valence or sentiment', 'Other or Unspecified']"
cc4a7ea983513b965bf68999861018f9c19482e9,Recognition of emotion in a realistic dialogue scenario,2000,https://opus.bibliothek.uni-augsburg.de/opus4/files/67139/i00_1665.pdf,"""In a call-center, such a system should be able to determine in a critical phase of the dialogue if the call should be passed over to a human operator.""",['Call center / call screening'],['Custom collected for this work'],"['Angry', 'Other or Unspecified']"
ecaac39e9242a0587d3d78a7100d5775c371fce0,MFCC Global Features Selection in Improving Speech Emotion Recognition Rate,2016,http://link.springer.com/10.1007/978-3-319-32213-1_13,"""Human emotions can be understood by the intelligent machine, but information will not be successfully delivered correctly if the emotion is mis- interpreted...If the computer is given the ability to identify human emotions in the same way as a human does, communication is bound to be more effective.""",['Responsive bots: other HCI systems'],['EMO-DB'],"['Happy', 'Sad', 'Fearful', 'Angry', 'Disgusted', 'Boredom', 'Neutral']"
46e8d8da8fa7a90b35f354efd429c0aead610e79,Interaction and Transition Model for Speech Emotion Recognition in Dialogue,2017,https://www.isca-archive.org/interspeech_2017/zhang17b_interspeech.html,"In recent years, emotion recognition has become more and more important in the research topic area and focus is being placed on classifying emotion in audio signals [2, 3]. Emotion recognition is seen as critical in Human Computer Interface (HCI) when it comes to applications in the domains of software engineering, website customization, education and gaming, for it helps the machine better understand humans [4]. The aim of this research is speech emotion recognition in human-to-human dialogues.","['Responsive bots: other HCI systems', 'Other', 'Video Games, Toys, Entertainment', 'Education, Tutoring']",['IEMOCAP'],"['Angry', 'Happy', 'Sad', 'Neutral', 'Other or Unspecified']"
7e3c3d7b5cddd207d94986d4b3eb235147d33620,Learning utterance-level representations for speech emotion and age/gender recognition using deep neural networks,2017,http://ieeexplore.ieee.org/document/7953138/,"Speech emotion recognition is becoming more and more important for many applications related to human computer interactions, especially for spoken dialogue systems. With emotion recognition from users' speech, a better user experience can be achieved","['Responsive bots: other HCI systems', 'Responsive bots: voice assistants']",['Custom collected for this work'],"['Neutral', 'Happy', 'Sad', 'Angry']"
fb6efaeb71095655309fdd00eea9b41a1b5723ba,A first look into a Convolutional Neural Network for speech emotion detection,2017,http://ieeexplore.ieee.org/document/7953131/,"""Recognizing the emotions expressed in a speech signal, as well as in other modes, is an hard task. It is characterized by a level of subjectivity in defining and perceiving an emotion, as well as by a lack of a univocal definition of standard descriptors for each specific emotion [1, 2].""",['Prior work'],['Custom collected for this work'],"['Angry', 'Happy', 'Sad']"
3aad0044b5f39342090709e4bb932a164b32c54a,Towards Conditional Adversarial Training for Predicting Emotions from Speech,2018,https://opus.bibliothek.uni-augsburg.de/opus4/files/45055/45055.pdf,Deep learning has shown great potential for Speech Emotion Recognition (SER),['Prior work'],['RECOLA'],"['Arousal/Activation', 'Valence or sentiment']"
460c6277a9ad97b10c161c56aacc25ef71b56832,Exploring Spatio-Temporal Representations by Integrating Attention-based Bidirectional-LSTM-RNNs and FCNs for Speech Emotion Recognition,2018,https://digitalcommons.fairfield.edu/cgi/viewcontent.cgi?article=1182&context=engineering-facultypubs,"Speech emotion recognition, referring to the process of detecting the emotional state of a speaker, has become a very active research topic in the affective computing field and has had a wide range of applications recently. Automatic emotion recognition from speech largely depends on the effectiveness of the speech features used for classification. Due to the subtleties of human emotion, how to extract discriminative and affect-salient features from the speech signals is still one of the important research topics [1, 2].",['Prior work'],"['CHEAVD', 'IEMOCAP']","['Angry', 'Disgusted', 'Happy', 'Sad', 'Surprised', 'Neutral', 'Other or Unspecified']"
518bbd182875fdb921ab41f86d86ec87fd8dc2c3,Speech Emotion Recognition from Variable-Length Inputs with Triplet Loss Function,2018,https://www.isca-archive.org/interspeech_2018/huang18b_interspeech.html,"""Emotion recognition has been attracting increasing attention recently, owing to its essential role in human behavioral signal processing""",['Paralinguistics / behavioral studies'],['IEMOCAP'],"['Angry', 'Happy: Happy + Excited', 'Neutral', 'Sad']"
810d2a659a599572825d62dbabf28d233ce0d8b1,Speech Emotion Recognition Using Spectrogram & Phoneme Embedding,2018,https://www.isca-archive.org/interspeech_2018/yenigalla18_interspeech.html,"A significant segment of natural language processing
problem involves speech or audio input, such as Chatbot
applications for automatic reply or personal assistance, emotion
analysis and classification, voice activated systems etc. For
some of the above tasks speech or audio is converted into texts
at the first step (using Automatic Speech Recognition or ASR
mechanism) followed by classification and other learning
operations based on text data. When an ASR module generates
texts from an audio, it (generated text) becomes speaker
independent. ASR takes care of differences in audio from
different users using probabilistic acoustic and language
models. As a result, robust text processing technologies proved
successful in various applications. Text processing through
deep learning is already an established domain of research [1]-
[2]. Though the state-of-the-art ASR techniques work with high
accuracy, we lose significant information while converting the
audio into text. Specifically, the emotion component present in
the audio signal is missed in the converted text. To address this
issue, speech-based emotion recognition (SER) became a
research of interest in last few decades.",['Prior work'],['IEMOCAP'],"['Neutral', 'Happy', 'Sad', 'Angry']"
270b40579e0154f51d28f1d9b2e3dd6cafe958de,Robust Speech Emotion Recognition Under Different Encoding Conditions,2019,https://opus.bibliothek.uni-augsburg.de/opus4/files/71710/oates19_interspeech.pdf,"""With the rapid rise of speech emotion recognition (SER) research and its imminent advance into industry applications """,['Prior work'],"['EMO-DB', 'RECOLA', 'eNTERFACE', 'Polish Emotional Speech Database']","['Angry', 'Boredom', 'Fearful', 'Joy', 'Neutral', 'Sad', 'Disgusted', 'Happy', 'Surprised', 'Arousal/Activation', 'Valence or sentiment']"
57a1d3042b3485c7c071e6fee087c35c98b11773,Multi-Task Semi-Supervised Adversarial Autoencoding for Speech Emotion Recognition,2019,,"The success of SER will redefine human-computer interactions, enabling, for example, effective service delivery in many sectors. Call centres now track customers’ emotions for better service delivery [2]. Speech based diagnostic systems are being developed for diagnosis of depression [3], distress [4], and monitoring of mood states for bipolar patients [5].","['Healthcare (Mental health)', 'Call center / call screening', 'Responsive bots: other HCI systems']","['IEMOCAP', 'MSP-IMPROV']","['Angry', 'Happy', 'Neutral', 'Sad']"
77f418061bea95cb3ecad1e5287468e1d811538c,Speech Emotion Recognition Using Multi-hop Attention Mechanism,2019,https://arxiv.org/pdf/1904.10788.pdf,"""To enrich the user experience, the system is often required to detect human emotion and produce a response with proper emotional context [1, 2]. The first step in such an HCI involves building a system that recognizes emotion from the speech utterance. A speech emotional system aims to identify audio recording as belonging to one of the categories, like happy, sad, angry or neutral. Beside HCI, the output of emotion recognition engine is beneficial in the paralinguistic area as well""","['Responsive bots: other HCI systems', 'Paralinguistics / behavioral studies']",['IEMOCAP'],"['Happy: Happy + Excited', 'Angry', 'Neutral', 'Sad']"
920f779bef257922d7685244f1a7afc1e4d6ad86,Attention-Enhanced Connectionist Temporal Classification for Discrete Speech Emotion Recognition,2019,https://opus.bibliothek.uni-augsburg.de/opus4/files/66877/1649.pdf,"Automatic speech emotion recognition (SER), which focuses on
the identification of discrete emotion states, can be a challenging task, and relies heavily on the effectiveness of the speech
features for classification. Many works in this field treat the
task as a typical sequence classification problem, in which each
chunk of speech such as an utterance has exactly one label.",['Prior work'],"['IEMOCAP', 'FAU Aibo Emotion Corpus']","['Happy: Happy + Excited', 'Angry', 'Sad', 'Neutral', 'Emphatic', 'Positive', 'Rest']"
a31dd89ac8258b2fc27c82d30dcb5c7beb49dc92,Towards Robust Speech Emotion Recognition Using Deep Residual Networks for Speech Enhancement,2019,https://opus.bibliothek.uni-augsburg.de/opus4/files/71714/triantafyllopoulos19_interspeech.pdf,"Speech enhancement architectures are usually evaluated using some speech quality metric like PESQ [3] or STOI [4], or
focus on improving metrics like word error rate (WER) for ASR [5, 6, 7]. In contrast, we consider the application of these architectures in the field of SER, where additive noise and reverberation have been shown to severely degrade the performance of algorithms [8]",['Prior work'],"['RECOLA', 'EMO-DB', 'eNTERFACE', 'Mozilla Common Voice Audio Set']","['Arousal/Activation', 'Other or Unspecified']"
c4310f6c4c1edd76f1898c560e1a1d2b82f50069,Multi-Modal Learning for Speech Emotion Recognition: An Analysis and Comparison of ASR Outputs with Ground Truth Transcription,2019,https://www.isca-archive.org/interspeech_2019/sahu19_interspeech.html,"It has applications in several fields including building intelligent voice-assistants, psychiatry, analysis of human interaction and other behavioral studies [1].","['Healthcare (Mental health)', 'Paralinguistics / behavioral studies', 'Responsive bots: voice assistants']","['IEMOCAP', 'MSP-IMPROV']","['Angry', 'Happy', 'Neutral', 'Sad']"
eebd7e974f84c49edbee62b1c1c8b2557d254b9e,Unsupervised Low-Rank Representations for Speech Emotion Recognition,2019,https://arxiv.org/pdf/2104.07072.pdf,"""Human-machine interaction is constantly evolving towards the use of more natural interfaces, like speech. Still the key difference between human-human and human-machine communication is the ability of humans to recognize the emotion of their conversation peers and modify their communication strategy based on that.""",['Responsive bots: other HCI systems'],"['EMO-DB', 'IEMOCAP']","['Angry', 'Disgusted', 'Fearful', 'Joy', 'Boredom', 'Sad', 'Neutral', 'Happy']"
6b61a7bcf043306ea8cdfed0a3721a2cf748ecac,Multi-Head Attention for Speech Emotion Recognition with Auxiliary Learning of Gender Recognition,2020,https://ieeexplore.ieee.org/document/9054073/,"Voice assistants are becoming ubiquitous as more and more devices supporting them come into the market. Their traditional focus has been majorly towards understanding the user commands and replying to user queries and hence major efforts went into making them understand the language part of the speech. However, in the recent past, there is a tremendous push to make the devices to become true companions rather than just being assistants. An essential and fundamental requirement for becoming a true companion is the ability to understand human emotions. Emotions often convey deeper and even different meaning compared to what spoken words convey. Emotions are abound in speech, speech content, facial reactions, and body language. Though the task of automatic speech recognition is approaching near human levels, speech emotion recognition (SER) still has a long way to go.","['Responsive bots: voice assistants', 'Social Companion bots', 'Responsive bots: other HCI systems']",['IEMOCAP'],"['Sad', 'Angry', 'Neutral', 'Happy']"
1b4ab3d3b66ff45920bb537cd0109996fd2e30b9,Speech Emotion Recognition Using Semantic Information,2021,https://arxiv.org/pdf/2103.02993.pdf,"Automatic affect recognition is a vital component in human-to human communication affecting our social interaction, perception among others [1]. In order to accomplish a natural interaction between human and machine, intelligent systems need to recognise the emotional state of individuals.",['Responsive bots: other HCI systems'],['SEWA'],"['Arousal/Activation', 'Other or Unspecified', 'Valence or sentiment']"
60ff7e3107836e9aead12006e386ebecb021db9b,A Conditional Cycle Emotion Gan for Cross Corpus Speech Emotion Recognition,2021,https://ieeexplore.ieee.org/document/9383512/,"Speech emotion recognition (SER) is important in enabling personalized services and multimedia applications in our life. It also becomes a prevalent topic of research with its potential in creating a better user experience across many modern technologies.

Rapid progress in deep learning algorithms is key in driving
advancement of technologies for human-centered services,
e.g., vision-based behavior detection [1], sound-based multimedia applications [2], and natural language understanding [3]. As these services become more integrated into our daily
life, the ability to automatically sense emotional states has
become a critical component. For example, speech emotion
recognition (SER) technology has enabled many applications,
such as homecare platforms or devices [4], and intelligent
vehicle assistance [5], to become more ubiquitous and pervasive.","['Responsive bots: other HCI systems', 'Responsive bots: car voice assistants']","['IEMOCAP', 'MSP-IMPROV', 'CIT']","['Arousal/Activation', 'Valence or sentiment']"
7b62461b87c6ba17fa3a185f617ea920316e8550,Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention,2021,https://arxiv.org/pdf/2106.04133.pdf,"It has been an important sub-task in building an intelligent system in many fields, such as customer support call review and analysis, mental health surveillance, human-machine interaction, etc","['Call center / call screening', 'Healthcare (Mental health)', 'Responsive bots: other HCI systems']",['IEMOCAP'],"['Angry', 'Sad', 'Neutral', 'Happy: Happy + Excited']"
2d7b4ebbb7b8cdc172b92b9f42e4e158dc2306a0,Multiple Enhancements to LSTM for Learning Emotion-Salient Features in Speech Emotion Recognition,2022,https://www.isca-archive.org/interspeech_2022/hu22e_interspeech.html,"Speech is the most convenient and efficient way of human communication, and the emotion information contained in speech plays a vital role in communication. Enabling machines to speak, think, and feel like humans has long been pursued in the field of artificial intelligence. The research of SER will promote the realization of this goal",['Responsive bots: other HCI systems'],['IEMOCAP'],"['Neutral', 'Angry', 'Happy', 'Sad']"
4b0ed13a47edca1feef8616c0ab9feca28ec27a2,SpeechEQ: Speech Emotion Recognition based on Multi-scale Unified Datasets and Multitask Learning,2022,http://arxiv.org/pdf/2206.13101.pdf,"Speech is an important carrier of emotion and the easiest way to record information completely. Modern people usually communicate by phone or through APPs. Unfortunately, existing technologies lack standards for speech emotion information. If designed a unified SER framework that automatically recognizes the human emotional state categories (ESC), such as happy, sad, anger, and its emotional intensity scales (EIS) expressed in natural speech, the intelligence systems can better perceive human thoughts and have better interactivity.",['Responsive bots: other HCI systems'],['Custom collected for this work'],['Other or Unspecified']
52b8fdda324448f8523ae8c5d77593501a52a3f9,Semi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On Federated Learning using Multiview Pseudo-Labeling,2022,https://arxiv.org/pdf/2203.08810.pdf,"""Speech emotion recognition (SER) has gained enormous interest in many diverse applications such as smart virtual assistants [1], medical diagnoses [2, 3], and education [4].""","['Other', 'Healthcare (Mental health)', 'Responsive bots: voice assistants', 'Education, Tutoring']","['IEMOCAP', 'MSP-IMPROV']","['Happy', 'Neutral', 'Angry', 'Other or Unspecified']"
5653e9bf7ddd49f5e713e913769a1aaa7ff6cbf6,GM-TCNet: Gated Multi-scale Temporal Convolutional Network using Emotion Causality for Speech Emotion Recognition,2022,,"For HCI applications, the identification of these factors behind the speech signals, especially the emotion, can enhance the understanding of the user’s intent and improve the experience during the interaction.",['Responsive bots: other HCI systems'],"['CASIA', 'EMO-DB', 'RAV', 'SAVEE']","['Angry', 'Boredom', 'Calm', 'Disgusted', 'Fearful', 'Happy', 'Neutral', 'Sad', 'Surprised']"
7d01ea8d989ecd8c0b3107a97740d6e02cb1e75b,Multi-View Speech Emotion Recognition Via Collective Relation Construction,2022,https://ieeexplore.ieee.org/document/9645304/,"Affective interaction is the high-level featured function of human-machine interaction. Affective computing, a fundamental technique of affective interaction, has been established as an emerging research field to deal with the automatic perception, recognition, and understanding of human emotions from different biological characteristics, such as facial expression, speech, physiological signals and body gestures [1]. Among them, speech is the dominant form of communication and is also recognized as one of the most effective ways to convey emotions clearer and faster than other modalities. In recent years, a wealth of research has been devoted to automatically recognize emotions from human speech [2]. However, the complicated emotion always requires comprehensive understanding of speech from multiple perspectives, and it is pertinent to develop efficient emotion recognition systems on multi-view speech to enhance the capabilities of affective interaction.",['Responsive bots: other HCI systems'],"['IEMOCAP', 'EMO-DB']","['Angry', 'Sad', 'Neutral', 'Boredom', 'Fearful', 'Disgusted', 'Happy: Happy + Excited']"
a36e2ef5afe23ff56ddc40ce8c84aa6371a503f9,Improving Speech Emotion Recognition Using Self-Supervised Learning with Domain-Specific Audiovisual Tasks,2022,https://www.isca-archive.org/interspeech_2022/goncalves22_interspeech.html,"Human communication relies on many signals to effectively transmit a message with the intended semantic content, intended emphasis, and emotional content. We use multimodal cues to convey expressive information during daily interactions, including acoustic, visual and other sensory modalities [1]. Therefore, we expect that combining and leveraging the relationship across different multimodal cues can help us in generating meaningful representations for robust emotion recognition.",['Prior work'],"['CREMA-D', 'MSP-Face']","['Happy', 'Fearful', 'Disgusted', 'Angry', 'Sad', 'Neutral']"
33e1e484ff561bbfa539a3713b95cd540787dd9a,Fusion-based speech emotion classification using two-stage feature selection,2023,https://linkinghub.elsevier.com/retrieve/pii/S0167639323000894,"Speech emotion classification plays an important role in human– computer interaction, which has been widely used for various applications including robot interfaces, audio surveillance, web-based E-learning, call centers, carboard systems, computer games (Kerkeni et al., 2019).","['Responsive bots: other HCI systems', 'Other', 'Call center / call screening', 'Video Games, Toys, Entertainment', 'Responsive bots: car voice assistants', 'Education, Tutoring']","['RAV', 'EMO-DB', 'SAVEE', 'EMOVO']","['Angry', 'Boredom', 'Fearful', 'Disgusted', 'Joy', 'Sad', 'Neutral', 'Happy', 'Surprised']"
4f312d02c6450d36c8eb3e0991e6363693aab802,DST: Deformable Speech Transformer for Emotion Recognition,2023,https://arxiv.org/pdf/2302.13729.pdf,"Emotion is one of the most essential characteristics that distinguishes humans from robots [1] and speech is the most basic tool for daily communication [2]. Therefore, analyzing emotion states through speech signals is a continuing concern for the research community.",['Paralinguistics / behavioral studies'],"['IEMOCAP', 'MELD']","['Happy: Happy + Excited', 'Angry', 'Sad', 'Neutral', 'Other or Unspecified']"
565b42dcb2b1f38019a0bab78e13d2e1e55497aa,Leveraging Semantic Information for Efficient Self-Supervised Emotion Recognition with Audio-Textual Distilled Models,2023,https://arxiv.org/pdf/2305.19184.pdf,"""Speech signals carry rich information on an individual’s emotional states, expressed through both paralinguistic and semantic cues.""",['Paralinguistics / behavioral studies'],['MSP-Podcasts'],"['Dominance', 'Arousal/Activation', 'Valence or sentiment']"
60bd26b2b926bcc9817506dff9895540a7b0a856,On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion and Automatic Speech Recognition,2023,http://arxiv.org/pdf/2305.12540.pdf,"""Nowadays, modern AI-based conversational agents employing ASR are also equipped with emotional awareness using SER to improve the overall user experience. These agents are indispensable in real-world applications such as call centers, automotive voice assistants, and voice-enabled home automation systems.""","['Responsive bots: other HCI systems', 'Responsive bots: car voice assistants', 'Call center / call screening']",['IEMOCAP'],"['Angry', 'Happy', 'Sad', 'Neutral']"
6a0fe607abbd8ec2c474d460b835a9f5ee33d248,Investigating salient representations and label Variance in Dimensional Speech Emotion Analysis,2023,https://arxiv.org/pdf/2312.16180,"""Real-time speech-emotion models can help to improve human-computer interaction experiences, such as voice
assistants [1, 2] and facilitate health/wellness applications such as health diagnoses [3, 4].""","['Responsive bots: voice assistants', 'Responsive bots: other HCI systems', 'Healthcare (Mental health)']",['MSP-Podcasts'],"['Dominance', 'Valence or sentiment']"
c40e3331d374cf18e96878fafe6477b5030519aa,ASR and Emotional Speech: A Word-Level Investigation of the Mutual Impact of Speech and Emotion Recognition,2023,http://arxiv.org/pdf/2305.16065.pdf,"Speech Emotion Recognition (SER) has rapidly developed over the past two decades, and the research on the corpora, features, algorithms, and training models has boomed [1]. ...Although some researchers have also attempted to translate this approach into the wild, e.g., in-car voice systems [4], it is still rare to see SER applications in our daily lives. One of the major reasons is that the majority of SER research uses human annotation, i.e., gold-standard manual transcripts. In contrast, even for the ‘lab’ emotion corpora (e.g., IEMOCAP), transcripts from a state-of-the-art Automatic Speech Recognition (ASR) system can result in Word Error Rates (WERs) higher than 35%, depending on the emotion label [5]. This means that very few of the findings obtained in the lab can be replicated in the wild.","['Prior work', 'Responsive bots: car voice assistants']","['IEMOCAP', 'MELD', 'CMU-MOSI']","['Angry', 'Happy: Happy + Excited', 'Neutral', 'Valence or sentiment', 'Disgusted', 'Fearful', 'Sad', 'Surprised', 'Joy']"
d6d2f6d901e83a2699b317b9a54c79658a1d064d,A Dual Attention-based Modality-Collaborative Fusion Network for Emotion Recognition,2023,https://www.isca-archive.org/interspeech_2023/zhang23g_interspeech.html,"""Emotion plays a crucial role in our daily communication and emotion recognition finds applications in different domains like customer services [1], social robots and dialogue systems. In recent years, multimodal emotion recognition (MER) has attracted increasing attention.""","['Responsive bots: voice assistants', 'Social Companion bots']","['IEMOCAP', 'MELD']","['Happy: Happy + Excited', 'Angry', 'Neutral', 'Sad', 'Joy', 'Disgusted', 'Fearful', 'Surprised']"
35542e70c1abca996222e3ccf58e25534c6a64fe,Two-stage Finetuning of Wav2vec 2.0 for Speech Emotion Recognition with ASR and Gender Pretraining,2023,https://www.isca-archive.org/interspeech_2023/gao23d_interspeech.html,"""Understanding the emotional state from speech is crucial for
conversational robots and intelligent devices to generate empathetic responses to the user [1]""
",['Responsive bots: other HCI systems'],['IEMOCAP'],"['Angry', 'Neutral', 'Sad', 'Happy: Happy + Excited']"
2bb70628107ab32705b5c8e891a8c3bc75d80930,Parameter Efficient Finetuning for Speech Emotion Recognition and Domain Adaptation,2024,https://ieeexplore.ieee.org/ielx7/10445798/10445803/10446272.pdf,"Emotion is a complex process influencing human interaction and cognitive processes [1, 2]. Speech emotion recognition (SER) enables systems to perceive and respond to emotional cues in spoken language and plays a crucial role in human-computer interaction.",['Responsive bots: other HCI systems'],['IEMOCAP'],"['Happy: Happy + Excited', 'Sad', 'Angry', 'Neutral', 'Other or Unspecified']"
6aae002e4cbb909d7b2819a1521a7514d1fce88d,Improving Domain Generalization in Speech Emotion Recognition with Whisper,2024,https://ieeexplore.ieee.org/ielx7/10445798/10445803/10446997.pdf,"Speech-based Emotion Recognition (SER) has become crucial in diverse fields such as human-computer interaction [1], mental health monitoring [2], and customer service optimization [3],","['Responsive bots: other HCI systems', 'Healthcare (Mental health)', 'Call center / call screening']","['ASVP-ESD', 'IEMOCAP', 'RAV', 'CREMA-D', 'CAFE', 'EMO-DB']","['Happy: Happy + Excited', 'Happy', 'Sad', 'Angry', 'Neutral']"
8d335188ad0d2bcfbbc161a693ace453975c4357,Gradient-Based Dimensionality Reduction for Speech Emotion Recognition Using Deep Networks,2024,https://ieeexplore.ieee.org/ielx7/10445798/10445803/10447616.pdf,"With the advancement of human-machine interaction technology, we’ve witnessed the emergence of various dialogue systems in our daily lives, such as Alexa, Siri, and Cortana [1]. Ensuring machines
can perceive human emotions is crucial for making these human machine interactions feel natural [2], and one practical approach towards achieving this is through Speech Emotion Recognition (SER)
systems.","['Responsive bots: other HCI systems', 'Responsive bots: voice assistants']","['RAV', 'SAVEE', 'TESS']","['Calm', 'Happy', 'Sad', 'Angry', 'Fearful', 'Surprised', 'Disgusted', 'Neutral']"
