quote_id,section,labels,quotation,title,url
2,Introduction,['Exactness'],"While exact computation of this probability is in- tractable, several estimators for this prob- ability have been used in the topic model- ing literature, including the harmonic mean method and empirical likelihood method.",Evaluation Methods for Topic Models,http://dirichlet.net/pdf/wallach09evaluation.pdf
3,Introduction,"['Accuracy', 'Efficiency']","In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held- out documents, and propose two alternative methods that are both accurate and eﬃcient.",Evaluation Methods for Topic Models,http://dirichlet.net/pdf/wallach09evaluation.pdf
4,Introduction,"['Large scale', 'Useful']",Statistical topic modeling is an increasingly useful tool for analyzing large unstructured text collections.,Evaluation Methods for Topic Models,http://dirichlet.net/pdf/wallach09evaluation.pdf
7,Introduction,['Important'],Evaluation is an important issue: the unsupervised nature of topic models makes model se- lection diﬃcult.,Evaluation Methods for Topic Models,http://dirichlet.net/pdf/wallach09evaluation.pdf
8,Introduction,"['Performance', 'Applies to real world']","For some applications there may be extrinsic tasks, such as information retrieval or docu- ment classiﬁcation, for which performance can be eval- uated.",Evaluation Methods for Topic Models,http://dirichlet.net/pdf/wallach09evaluation.pdf
9,Introduction,"['Generalization', 'Accuracy', 'Efficiency']","However, there is a need for a universal method that measures the generalization capability of a topic model in a way that is accurate, computationally eﬃ- cient, and independent of any speciﬁc application.",Evaluation Methods for Topic Models,http://dirichlet.net/pdf/wallach09evaluation.pdf
18,Discussion,"['Performance', 'Understanding (for researchers)']","Estimating the probability of held-out documents provides a clear, interpretable metric for evaluating the performance of topic models relative to other topicbased models as well as to other non-topic-based generative models.",Evaluation Methods for Topic Models,http://dirichlet.net/pdf/wallach09evaluation.pdf
19,Discussion,"['Quantitative evidence (e.g. experiments)', 'Accuracy']",We provide empirical evidence that several recently-used methods for estimating the probability of held-out documents are inaccurate and can change the results of model comparison.,Evaluation Methods for Topic Models,http://dirichlet.net/pdf/wallach09evaluation.pdf
20,Discussion,['Accuracy'],"In contrast, the Chib-style estimator and “left-to-right” algorithm presented in this paper provide a clear methodology for accurately assessing and selecting topic models.",Evaluation Methods for Topic Models,http://dirichlet.net/pdf/wallach09evaluation.pdf
22,Abstract,['Used in practice/Popular'],"By allowing arbitrary structures on the feature set, this concept generalizes the group sparsity idea that has become popular in recent years.",Learning with Structured Sparsity,https://www.jmlr.org/papers/volume12/huang11b/huang11b.pdf
24,Abstract,['Performance'],"It is shown that if the coding complexity of the target signal is small, then one can achieve improved performance by using coding complexity regularization methods, which generalize the standard sparse regu- larization.",Learning with Structured Sparsity,https://www.jmlr.org/papers/volume12/huang11b/huang11b.pdf
25,Abstract,['Efficiency'],"Moreover, a structured greedy algorithm is proposed to efﬁciently solve the structured sparsity problem.",Learning with Structured Sparsity,https://www.jmlr.org/papers/volume12/huang11b/huang11b.pdf
27,Abstract,"['Quantitative evidence (e.g. experiments)', 'Applies to real world']",Experiments are included to demonstrate the advantage of structured sparsity over standard sparsity on some real applications.,Learning with Structured Sparsity,https://www.jmlr.org/papers/volume12/huang11b/huang11b.pdf
34,Introduction,['Approximation'],We consider the situation that the true mean of the observation Ey can be approximated by a sparse linear combination of the basis vectors.,Learning with Structured Sparsity,https://www.jmlr.org/papers/volume12/huang11b/huang11b.pdf
43,Introduction,['Used in practice/Popular'],"In practical applications, one often knows a structure on the coefﬁcient vector ¯b in addition to sparsity.",Learning with Structured Sparsity,https://www.jmlr.org/papers/volume12/huang11b/huang11b.pdf
52,Introduction,"['Theoretical guarantees', 'Performance']",One is convex relaxation (L1 regularization to replace L0 regularization for standard sparsity); the other is forward greedy selection (also called orthogonal matching pursuit or OMP).,Learning with Structured Sparsity,https://www.jmlr.org/papers/volume12/huang11b/huang11b.pdf
63,Discussion,"['Novelty', 'Efficiency']",The structured greedy algorithm presented in this paper is the first efficient algorithm proposed to handle the general structured sparsity learning,Learning with Structured Sparsity,https://www.jmlr.org/papers/volume12/huang11b/huang11b.pdf
64,Discussion,['Effectiveness'],It is shown that the algorithm is effective under appropriate conditions. F,Learning with Structured Sparsity,https://www.jmlr.org/papers/volume12/huang11b/huang11b.pdf
65,Discussion,['Efficiency'],"Future work include additional computationally efficient methods such as convex relaxation methods (e.g. L1 regularization for standard sparsity, and group Lasso for strong group sparsity) and backward greedy strategies to improve the forward greedy method considered in this paper",Learning with Structured Sparsity,https://www.jmlr.org/papers/volume12/huang11b/huang11b.pdf
66,Abstract,"['Simplicity', 'Effectiveness']",Low-rank matrix approximation methods provide one of the simplest and most eﬀective approaches to collaborative ﬁltering.,Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
67,Abstract,"['Large scale', 'Efficiency', 'Scales up']","Such models are usually ﬁtted to data by ﬁnding a MAP estimate of the model parameters, a procedure that can be performed eﬃciently even on very large datasets.",Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
69,Abstract,['Automatic'],In this paper we present a fully Bayesian treatment of the Probabilistic Matrix Factorization (PMF) model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters.,Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
70,Abstract,"['Large scale', 'Efficiency', 'Scales up']","We show that Bayesian PMF models can be eﬃciently trained us- ing Markov chain Monte Carlo methods by applying them to the Netﬂix dataset, which consists of over 100 million movie ratings.",Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
71,Abstract,['Accuracy'],The resulting models achieve signiﬁcantly higher prediction accuracy than PMF models trained using MAP estimation.,Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
81,Introduction,"['Large scale', 'Efficiency', 'Scales up']",Training such models amounts to maximiz- ing the log-posterior over model parameters and can be done very eﬃciently even on very large datasets.,Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
82,Introduction,['Used in practice/Popular'],"In practice, we are usually interested in predicting rat- ings for new user/movie pairs rather than in estimat- ing model parameters.",Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
85,Introduction,['Novelty'],The distinguishing feature of our work is the use of Markov chain Monte Carlo (MCMC) methods for approximate inference in this model.,Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
86,Introduction,"['Large scale', 'Used in practice/Popular']","In practice, MCMC methods are rarely used on large- scale problems because they are perceived to be very slow by practitioners.",Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
87,Introduction,['Successful'],"In this paper we show that MCMC can be successfully applied to the large, sparse, and very imbalanced Netﬂix dataset, containing over 100 million user/movie ratings.",Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
88,Introduction,['Accuracy'],"We also show that it signiﬁcantly increases the model’s predictive accuracy, especially for the infrequent users, compared to the standard PMF models trained using MAP with regu- larization parameters that have been carefully tuned on the validation set.",Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
90,Introduction,['Simplicity'],"These methods at- tempt to approximate the true posterior distribution by a simpler, factorized distribution under which the user factor vectors are independent of the movie factor vectors.",Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
93,Introduction,['Performance'],This conclusion is supported by the fact that the Bayesian PMF models outperform their MAP trained counterparts by a much larger margin than the variationally trained models do.,Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
95,Conclusion,"['Large scale', 'Accuracy', 'Successful', 'Scales up']","We have also demonstrated that Bayesian PMF models can be successfully applied to a large dataset containing over 100 million movie ratings, and achieve significantly higher predictive accuracy compared to the MAPtrained PMF models with carefully tuned regularization parameters.",Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
100,Conclusion,['Used in practice/Popular'],"In practice, we have to rely on rules of thumb to diagnose convergence, which means that there is a risk of using samples from a distribution that differs from the true posterior distribution, potentially leading to suboptimal predictions.",Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
103,Conclusion,['Parallelizability / distributed'],"However, as mentioned above, sampling the feature vectors for multiple users or movies in parallel provides an easy way to greatly speed up the process of generating samples using multiple cores.",Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo,http://icml2008.cs.helsinki.fi/papers/600.pdf
104,Abstract,['Efficiency'],We describe efﬁcient algorithms for projecting a vector onto the ℓ1-ball.,Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions,http://icml2008.cs.helsinki.fi/papers/361.pdf
108,Abstract,['Useful'],This setting is especially useful for online learning in sparse feature spaces such as text categorization applications.,Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions,http://icml2008.cs.helsinki.fi/papers/361.pdf
109,Abstract,['Effectiveness'],We demonstrate the merits and ef- fectiveness of our algorithms in numerous batch and online learning tasks.,Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions,http://icml2008.cs.helsinki.fi/papers/361.pdf
110,Abstract,"['State-of-the-art', 'Efficiency']","We show that vari- ants of stochastic gradient projection methods augmented with our efﬁcient projection proce- dures outperform interior point methods, which are considered state-of-the-art optimization tech- niques.",Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions,http://icml2008.cs.helsinki.fi/papers/361.pdf
124,Introduction,['Generalization'],The second motivation for using ℓ1 constraints in machine learning problems is that in some cases it leads to improved generalization bounds.,Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions,http://icml2008.cs.helsinki.fi/papers/361.pdf
129,Introduction,"['Generalization', 'Efficiency']",Recent work on ℓ2 constrained optimization for machine learning indicates that gradient-related projection algorithms are more efﬁ- cient in approaching a solution of good generalization than second-order algorithms when the number of examples and the dimension are large.,Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions,http://icml2008.cs.helsinki.fi/papers/361.pdf
130,Introduction,"['Large scale', 'State-of-the-art']","For instance, Shalev-Shwartz et al. (2007) give recent state-of-the-art methods for solv- ing large scale support vector machines.",Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions,http://icml2008.cs.helsinki.fi/papers/361.pdf
133,Introduction,['Fast'],The main contribution of this paper is the derivation of gradient projections with ℓ1 domain con- straints that can be performed almost as fast as gradient projection with ℓ2 constraints.,Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions,http://icml2008.cs.helsinki.fi/papers/361.pdf
134,Introduction,['Efficiency'],Our starting point is an efﬁcient method for projection onto the probabilistic simplex.,Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions,http://icml2008.cs.helsinki.fi/papers/361.pdf
148,Introduction,['Theoretical guarantees'],We therefore shift gears and describe a more complex algorithm that employs red- black trees to obtain a linear dependence on the number of non-zero features in an example and only logarithmic dependence on the full dimension.,Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions,http://icml2008.cs.helsinki.fi/papers/361.pdf
152,Introduction,['Fast'],"Our projec- tion based methods outperform competing algorithms in terms of sparsity, and they exhibit faster convergence and lower regret than previous methods.",Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions,http://icml2008.cs.helsinki.fi/papers/361.pdf
158,Introduction,['State-of-the-art'],"Particularly worrisome is the phenomenon of adversarial examples [Big+13; Sze+14], imperceptibly perturbed natural inputs that induce erroneous predictions in state-of-the-art classifiers.","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
162,Introduction,"['Robustness', 'Accuracy']","From this point of view, it is natural to treat adversarial robustness as a goal that can be disentangled and pursued independently from maximizing accuracy [Mad+18; SHS19; Sug+19], either through improved standard regularization methods [TG16] or pre/post-processing of network inputs/outputs [Ues+18; CW17a; He+17].","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
172,Introduction,['Robustness'],"Since any two models are likely to learn similar non-robust features, perturbations that manipulate such features will apply to both.","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
176,Introduction,['Robustness'],"To corroborate our theory, we show that it is possible to disentangle robust from non-robust features in standard image classification datasets.","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
177,Introduction,['Robustness'],We are able to effectively remove non-robust features from a dataset.,"Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
178,Introduction,"['Robustness', 'Concreteness', 'Accuracy', 'Avoiding train/test discrepancy']","Concretely, we create a training set (semantically similar to the original) on which standard training yields good robust accuracy on the original, unmodified test set. T","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
180,Introduction,['Robustness'],"In fact, the inputs in the new training set are associated to their labels only through small adversarial perturbations (and hence utilize only nonrobust features).","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
181,Introduction,['Accuracy'],"Despite the lack of any predictive human-visible information, training on this dataset yields good accuracy on the original, unmodified test set.","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
183,Introduction,"['Robustness', 'Concreteness']","Finally, we present a concrete classification task where the connection between adversarial examples and non-robust features can be studied rigorously.","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
185,Introduction,"['Quantitative evidence (e.g. experiments)', 'Preciseness']","First, adversarial vulnerability in our setting can be precisely quantified as a difference between the intrinsic data geometry and that of the adversary’s perturbation set","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
186,Introduction,['Robustness'],"Second, robust training yields a classifier which utilizes a geometry corresponding to a combination of these two.","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
188,Discussion,['Robustness'],"Our analysis suggests that rather than offering quantitative classification benefits, a natural way to view the role of robust optimization is as enforcing a prior over the features learned by the classifier.","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
190,Conclusion,['Robustness'],"In this work, we cast the phenomenon of adversarial examples as a natural consequence of the presence of highly predictive but non-robust features in standard ML datasets.","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
191,Conclusion,['Robustness'],"We provide support for this hypothesis by explicitly disentangling robust and non-robust features in standard datasets, as well as showing that non-robust features alone are sufficient for good generalization.","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
192,Conclusion,['Robustness'],"Finally, we study these phenomena in more detail in a theoretical setting where we can rigorously study adversarial vulnerability, robust training, and gradient alignment.","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
194,Conclusion,['Robustness'],"In particular, we should not be surprised that classifiers exploit highly predictive features that happen to be non-robust under a human-selected notion of similarity, given such features exist in real-world datasets.","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
195,Conclusion,"['Robustness', 'Understanding (for researchers)']","In the same manner, from the perspective of interpretability, as long as models rely on these non-robust features, we cannot expect to have model explanations that are both human-meaningful and faithful to the models themselves.","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
196,Conclusion,"['Robustness', 'Understanding (for researchers)', 'Learning from humans']","Overall, attaining models that are robust and interpretable will require explicitly encoding human priors into the training process.","Adversarial Examples are not Bugs, they are Features",http://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf
197,Abstract,['Label efficiency (reduced need for labeled data)'],Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets.,MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
198,Abstract,['Novelty'],"In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that guesses low-entropy labels for data-augmented unlabeled examples and mixes labeled and unlabeled data using MixUp.",MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
199,Abstract,['State-of-the-art'],MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts.,MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
201,Abstract,"['Accuracy', 'Privacy']",We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy.,MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
202,Abstract,['Successful'],"Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success.",MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
203,Abstract,['Facilitating use (e.g. sharing code)'],We release all code used in our experiments,MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
204,Introduction,"['Large scale', 'Successful']","Much of the recent success in training large, deep neural networks is thanks in part to the existence of large labeled datasets.",MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
206,Introduction,['Label efficiency (reduced need for labeled data)'],This is perhaps best illustrated by medical tasks where measurements call for expensive machinery and labels are the fruit of a time-consuming analysis that draws from multiple human experts.,MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
208,Introduction,['Label efficiency (reduced need for labeled data)'],"In comparison, in many tasks it is much easier or cheaper to obtain unlabeled data.",MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
209,Introduction,['Label efficiency (reduced need for labeled data)'],Semi-supervised learning [6] (SSL) seeks to largely alleviate the need for labeled data by allowing a model to leverage unlabeled data.,MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
211,Introduction,"['Performance', 'Avoiding train/test discrepancy']","In much recent work, this loss term falls into one of three classes (discussed further in Section 2): entropy minimization [18, 28]—which encourages the model to output confident predictions on unlabeled data; consistency regularization—which encourages the model to produce the same output distribution when its inputs are perturbed; and generic regularization—which encourages the model to generalize well and avoid overfitting the training data.",MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
212,Introduction,['Unifying ideas or integrating components'],"In this paper, we introduce MixMatch, an SSL algorithm which introduces a single loss that gracefully unifies these dominant approaches to semi-supervised learning.",MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
213,Introduction,['Novelty'],"g. Unlike previous methods, MixMatch targets all the properties at once which we find leads to the following benefits:",MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
214,Introduction,"['Quantitative evidence (e.g. experiments)', 'State-of-the-art']","Experimentally, we show that MixMatch obtains state-of-the-art results on all standard image benchmarks (section 4.2), and reducing the error rate on CIFAR-10 by a factor of 4;",MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
216,Introduction,"['Accuracy', 'State-of-the-art', 'Privacy']","We demonstrate in section 4.3 that MixMatch is useful for differentially private learning, enabling students in the PATE framework [36] to obtain new state-of-the-art results that simultaneously strengthen both privacy guarantees and accuracy.",MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
218,Conclusion,['Unifying ideas or integrating components'],"We introduced MixMatch, a semi-supervised learning method which combines ideas and components from the current dominant paradigms for SSL.",MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
219,Conclusion,"['Quantitative evidence (e.g. experiments)', 'Performance', 'Privacy']","Through extensive experiments on semi-supervised and privacy-preserving learning, we found that MixMatch exhibited significantly improved performance compared to other methods in all settings we studied, often by a factor of two or more reduction in error rate.",MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
220,Conclusion,"['Effectiveness', 'Unifying ideas or integrating components']","In future work, we are interested in incorporating additional ideas from the semi-supervised learning literature into hybrid methods and continuing to explore which components result in effective algorithms.",MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
221,Conclusion,['Effectiveness'],"Separately, most modern work on semi-supervised learning algorithms is evaluated on image benchmarks; we are interested in exploring the effectiveness of MixMatch in other domains.",MixMatch: A Holistic Approach to Semi-Supervised Learning,http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf
222,Abstract,"['Novelty', 'Performance']","In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks.",SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
223,Abstract,['Performance'],"The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research.",SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
224,Abstract,"['Novelty', 'Building on recent work']","In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard.",SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
225,Abstract,['Facilitating use (e.g. sharing code)'],SuperGLUE is available at super.gluebenchmark.com.,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
227,Introduction,['Effectiveness'],The unifying theme of these methods is that they couple self-supervised learning from massive unlabelled text corpora with effective adapting of the resulting model to target tasks.,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
233,Introduction,['Progress'],The progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
234,Introduction,"['Quantitative evidence (e.g. experiments)', 'Performance', 'State-of-the-art']","While some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured in GLUE remain difficult, the current state of the art GLUE Score as of early July 2019 (88.4 from Yang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3 points, and in fact exceeds this human performance estimate on four tasks.",SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
235,Introduction,['Improvement'],"Consequently, while there remains substantial scope for improvement towards GLUE’s high-level goals, the original version of the benchmark is no longer a suitable metric for quantifying such progress.",SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
236,Introduction,['Novelty'],"In response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of language understanding",SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
237,Introduction,"['Simplicity', 'Building on recent work', 'Progress']","SuperGLUE has the same high-level motivation as GLUE: to provide a simple, hard-to-game measure of progress toward general-purpose language understanding technologies for English.",SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
238,Introduction,['Progress'],"We anticipate that significant progress on SuperGLUE should require substantive innovations in a number of core areas of machine learning, including sample-efficient, transfer, multitask, and unsupervised or self-supervised learning.",SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
239,Introduction,['Performance'],"We include human performance estimates for all benchmark tasks, which verify that substantial headroom exists between a strong BERT-based baseline and human performance.",SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
240,Conclusion,['Novelty'],"We present SuperGLUE, a new benchmark for evaluating general-purpose language understanding systems.",SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
244,Conclusion,"['Performance', 'Progress']","Given the difficulty of SuperGLUE for BERT, we expect that further progress in multi-task, transfer, and unsupervised/self-supervised learning techniques will be necessary to approach human-level performance on the benchmark.",SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
245,Conclusion,['Novelty'],"Overall, we argue that SuperGLUE offers a rich and challenging testbed for work developing new general-purpose machine learning methods for language understanding.",SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf
247,Abstract,"['Automatic', 'Useful']","Recently, deep architectures trained in an unsupervised manner have been proposed as an automatic method for extracting useful features.",Measuring Invariances in Deep Networks,http://papers.nips.cc/paper/3790-measuring-invariances-in-deep-networks.pdf
248,Abstract,['Quantitative evidence (e.g. experiments)'],"In this paper, we propose a number of empirical tests that directly measure the degree to which these learned features are invariant to different input transformations.",Measuring Invariances in Deep Networks,http://papers.nips.cc/paper/3790-measuring-invariances-in-deep-networks.pdf
250,Introduction,['Robustness'],"The concept of invariance implies a selectivity for complex, high level features of the input and yet a robustness to irrelevant input transformations.",Measuring Invariances in Deep Networks,http://papers.nips.cc/paper/3790-measuring-invariances-in-deep-networks.pdf
251,Introduction,['Robustness'],This tension between selectivity and robustness makes learning invariant features nontrivial.,Measuring Invariances in Deep Networks,http://papers.nips.cc/paper/3790-measuring-invariances-in-deep-networks.pdf
253,Introduction,['Useful'],Our work also seeks to address the question: why are deep learning algorithms useful?,Measuring Invariances in Deep Networks,http://papers.nips.cc/paper/3790-measuring-invariances-in-deep-networks.pdf
255,Introduction,['Useful'],"A second answer can also be found in such work as [2, 3, 4, 5], which shows that such architectures lead to useful representations for classification.",Measuring Invariances in Deep Networks,http://papers.nips.cc/paper/3790-measuring-invariances-in-deep-networks.pdf
257,Introduction,['Performance'],"In computer vision, one can view object recognition performance as a measure of the invariance of the underlying features.",Measuring Invariances in Deep Networks,http://papers.nips.cc/paper/3790-measuring-invariances-in-deep-networks.pdf
258,Introduction,['Performance'],"While such an end-to-end system performance measure has many benefits, it can also be expensive to compute and does not give much insight into how to directly improve representations in each layer of deep architectures.",Measuring Invariances in Deep Networks,http://papers.nips.cc/paper/3790-measuring-invariances-in-deep-networks.pdf
259,Introduction,['Robustness'],The test suite presented in this paper provides an alternative that can identify the robustness of deep architectures to specific types of variations.,Measuring Invariances in Deep Networks,http://papers.nips.cc/paper/3790-measuring-invariances-in-deep-networks.pdf
262,Introduction,['Useful'],These networks were shown by Larochelle et al. [3] to learn useful features for a range of vision tasks; this suggests that their learned features are significantly invariant to the transformations present in those tasks.,Measuring Invariances in Deep Networks,http://papers.nips.cc/paper/3790-measuring-invariances-in-deep-networks.pdf
274,Dicussion and Conclusion,['Generalization'],"However, the definition of our metric is sufficiently general that it could easily be used to test, for example, invariance of auditory features to rate of speech, or invariance of textual features to author identity.",Measuring Invariances in Deep Networks,http://papers.nips.cc/paper/3790-measuring-invariances-in-deep-networks.pdf
277,Dicussion and Conclusion,['Promising'],"For example, one promising approach that we are currently investigating is the idea of learning slow features [19] from temporal data.",Measuring Invariances in Deep Networks,http://papers.nips.cc/paper/3790-measuring-invariances-in-deep-networks.pdf
280,Dicussion and Conclusion,"['Automatic', 'Useful']","It is not obvious how to extend these explicit strategies to become invariant to more intricate transformations like large-angle out-of-plane rotations and complex illumination changes, and we expect that our metrics will be useful in guiding efforts to develop learning algorithms that automatically discover much more invariant features without relying on hard-wired strategies.",Measuring Invariances in Deep Networks,http://papers.nips.cc/paper/3790-measuring-invariances-in-deep-networks.pdf
281,Introduction,['Novelty'],"Many successes in deep learning (Krizhevsky et al., 2012; Goodfellow et al., 2014; Sutskever et al., 2014) have resulted from novel neural network architecture designs.",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
282,Introduction,['Novelty'],"This proliferation of choices has fueled research into neural architecture search (NAS), which casts the discovery of new architectures as an optimization problem",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
283,Introduction,"['Promising', 'State-of-the-art']","This has resulted in state of the art performance in the domain of image classification (Zoph et al., 2018; Real et al., 2018; Huang et al., 2018), and has shown promising results in other domains, such as sequence modeling (Zoph & Le, 2016; So et al., 2019).",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
284,Introduction,['Reproducibility'],"Unfortunately, NAS research is notoriously hard to reproduce (Li & Talwalkar, 2019; Sciuto et al., 2019).",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
285,Introduction,['Requires few resources'],"First, some methods require months of compute time (e.g., Zoph et al., 2018), making these methods inaccessible to most researchers",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
286,Introduction,['Efficiency'],"Second, while recent improvements (Liu et al., 2018a; Pham et al., 2018; Liu et al., 2018b) have yielded more efficient methods, different methods are not comparable to each other due to different training procedures and different search spaces, which make it difficult to attribute the success of each method to the search algorithm itself.",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
287,Introduction,['Novelty'],"To address the issues above, this paper introduces NASBench-101, the first architecture-dataset for NAS.",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
288,Introduction,"['Low cost', 'Efficiency']",This enables NAS experiments to be run via querying a table instead of performing the usual costly train and evaluate procedure.,NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
289,Introduction,"['Reproducibility', 'Facilitating use (e.g. sharing code)']","Moreover, the data, search space, and training code is fully public 1 , to foster reproducibility in the NAS community.",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
290,Introduction,['Novelty'],"Because NAS-Bench-101 exhaustively evaluates a search space, it permits, for the first time, a comprehensive analysis of a NAS search space as a whole.",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
292,Introduction,['Flexibility/Extensibility'],"Finally, we demonstrate its application to the analysis of algorithms by benchmarking a wide range of open source architecture/hyperparameter search methods, including evolutionary approaches, random search, and Bayesian optimization.",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
293,Introduction,"['Novelty', 'Facilitating use (e.g. sharing code)']","We introduce NAS-Bench-101, the first large-scale, open source architecture dataset for NAS (Section 2);",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
295,Introduction,"['Efficiency', 'Facilitating use (e.g. sharing code)']",We illustrate how to use the dataset to perform fast benchmarking of various open-source NAS optimization algorithms (Section 4).,NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
296,Abstract,"['Large scale', 'Reproducibility', 'Requires few resources']","Recent advances in neural architecture search (NAS) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers without access to large-scale computation",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
297,Abstract,['Novelty'],"We aim to ameliorate these problems by introducing NAS-Bench-101, the first public architecture dataset for NAS research.",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
300,Discussion,['Quantitative evidence (e.g. experiments)'],"To justify the approximation above, we performed a study on a different set of NAS-HPO-Bench (Klein & Hutter, 2019) datasets (described in detail in Supplement S5) T",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
301,Discussion,"['Exactness', 'Accuracy']","This let us compute the exact hyperparameteroptimized accuracy,",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
302,Discussion,['Accuracy'],"To do this, we chose a set of hyperparameters H† by optimizing the mean accuracy across all of the architectures for a given dataset.",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
303,Discussion,['Accuracy'],This allows us to map each architecture A to its approximate hyperparameteroptimized accuracy,NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
304,Discussion,['Requires few resources'],"The choice of search space, hyperparameters, and training techniques were designed to ensure that NAS-Bench-101 would be feasible to compute with our resources.",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
305,Discussion,"['Performance', 'State-of-the-art']","Unfortunately, this means that the models we evaluate do not reach current state-of-the-art performance on CIFAR-10.",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
306,Conclusion,['Novelty'],"We introduced NAS-Bench-101, a new tabular benchmark for neural architecture search that is inexpensive to evaluate but still preserves the original NAS optimization problem, enabling us to rigorously compare various algorithms quickly and without the enormous computational budgets often used by projects in the field",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
307,Conclusion,['Large scale'],"Based on the data we generated for this dataset, we were able to analyze the properties of an exhaustively evaluated set of convolutional neural architectures at unprecedented scale.",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
308,Conclusion,"['Reproducibility', 'Facilitating use (e.g. sharing code)']","In open-sourcing the NAS-Bench-101 data and generating code, we hope to make NAS research more accessible and reproducible.",NAS-Bench-101: Towards Reproducible Neural Architecture Search,https://arxiv.org/pdf/1902.09635.pdf
309,Abstract,['Performance'],He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training.,Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
310,Abstract,"['Robustness', 'Performance']","We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
311,Abstract,['Quantitative evidence (e.g. experiments)'],"Through extensive experiments on adversarial examples, label corruption, class imbalance, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
312,Abstract,"['Quantitative evidence (e.g. experiments)', 'State-of-the-art', 'Improvement']",We introduce adversarial pre-training and show approximately a 10% absolute improvement over the previous state-of-the-art in adversarial robustness.,Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
313,Abstract,['State-of-the-art'],"In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pretraining when evaluating future methods on robustness and uncertainty tasks.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
314,Introduction,['State-of-the-art'],"In research settings, pre-training is ubiquitously applied in state-of-the-art object detection and segmentation (He et al., 2017).",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
317,Introduction,['State-of-the-art'],This broadly applicable technique enables state-of-the-art model convergence.,Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
318,Introduction,['Reduced training time'],"However, He et al. (2018) argue that model convergence is merely faster with pre-training, so that the benefit on modern research datasets is only improved wall-clock time.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
319,Introduction,['Performance'],"Surprisingly, pre-training provides no performance benefit on various tasks and architectures over training from scratch, provided the model trains for long enough.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
320,Introduction,['Performance'],"Even models trained from scratch on only 10% of the COCO dataset (Lin et al., 2014) attain the same performance as pre-trained models.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
321,Introduction,['Understanding (for researchers)'],This casts doubt on our understanding of pre-training and raises the important question of whether there are any uses for pre-training beyond tuning for extremely small datasets.,Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
322,Introduction,['Performance'],"While He et al. (2018) are correct that models for traditional tasks such as classification perform well without pre-training, pre-training substantially improves the quality of various complementary model components",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
323,Introduction,"['Robustness', 'Accuracy']","For example, we show that while accuracy may not noticeably change with pre-training, what does tremendously improve with pre-training is the model’s adversarial robustness.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
324,Introduction,['Accuracy'],"And the claim that “pre-training does not necessarily help reduce overfitting” (He et al., 2018) is valid when measuring only model accuracy, but it becomes apparent that pre-training does reduce overfitting when also measuring model calibration.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
325,Introduction,"['Robustness', 'Accuracy']","We bring clarity to the doubts raised about pre-training by showing that pre-training can improve model robustness to label corruption (Sukhbaatar et al., 2014), class imbalance (Japkowicz, 2000), and adversarial attacks (Szegedy et al., 2014); it additionally improves uncertainty estimates for out-of-distribution detection (Hendrycks & Gimpel, 2017b) and calibration (Nguyen & O’Connor, 2015), though not necessarily traditional accuracy metrics.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
326,Introduction,"['Robustness', 'Performance', 'State-of-the-art', 'Improvement']",Pre-training yields improvements so significant that on many robustness and uncertainty tasks we surpass stateof-the-art performance.,Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
327,Introduction,"['Robustness', 'Performance', 'Applies to real world']",This is problematic since we find there are techniques which do not comport well with pre-training; thus some evaluations of robustness are less representative of real-world performance than previously thought.,Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
328,Introduction,"['Performance', 'Applies to real world']",Thus researchers would do well to adopt the “pre-train then tune” paradigm for increased performance and greater realism.,Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
329,Conclusion,['Performance'],"Although He et al. (2018) assert that pre-training does not improve performance on traditional tasks, for other tasks this is not so.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
330,Conclusion,"['Robustness', 'State-of-the-art']","On robustness and uncertainty tasks, pre-training results in models that surpass the previous state-of-the-art.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
331,Conclusion,['Improvement'],"For uncertainty tasks, we find pre-trained representations directly translate to improvements in predictive uncertainty estimates.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
332,Conclusion,['Accuracy'],"He et al. (2018) argue that both pre-training and training from scratch result in models of similar accuracy, but we show this only holds for unperturbed data.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
333,Conclusion,"['Accuracy', 'State-of-the-art']","In fact, pre-training with an untargeted adversary surpasses the long-standing state-of-the-art in adversarial accuracy by a significant margin.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
334,Conclusion,"['Robustness', 'Performance']","Robustness to label corruption is similarly improved by wide margins, such that pre-training alone outperforms certain task-specific methods, sometimes even after combining these methods with pre-training.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
335,Conclusion,['Robustness'],"This suggests future work on model robustness should evaluate proposed methods with pretraining in order to correctly gauge their utility, and some work could specialize pre-training for these downstream tasks.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
336,Conclusion,"['Robustness', 'Understanding (for researchers)']","In sum, the benefits of pre-training extend beyond merely quick convergence, as previously thought, since pre-training can improve model robustness and uncertainty.",Using Pre-Training Can Improve Model Robustness and Uncertainty,https://arxiv.org/abs/1901.09960
338,Abstract,"['Efficiency', 'Requires few resources']","We analyze and compare the CPU and network time complexity of each of these methods and present a theoretical analysis of conditional maxent models, including a study of the convergence of the mixture weight method, the most resource-efficient technique.",Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models,https://proceedings.neurips.cc/paper/2009/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf
339,Abstract,"['Performance', 'Requires few resources']","We also report the results of large-scale experiments comparing these three methods which demonstrate the benefits of the mixture weight method: this method consumes less resources, while achieving a performance comparable to that of standard approaches.",Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models,https://proceedings.neurips.cc/paper/2009/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf
343,Introduction,['Theoretical guarantees'],"While the theoretical foundation of conditional maxent models makes them attractive, the computational cost of their optimization problem is often prohibitive for data sets of several million points.",Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models,https://proceedings.neurips.cc/paper/2009/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf
345,Introduction,['Scales up'],This paper examines distributed methods for training conditional maxent models that can scale to very large samples of up to 1B instances.,Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models,https://proceedings.neurips.cc/paper/2009/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf
346,Introduction,['Parallelizability / distributed'],"Both batch algorithms and on-line training algorithms such as that of [5] or stochastic gradient descent [21] can benefit from parallelization, but we concentrate here on batch distributed methods.",Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models,https://proceedings.neurips.cc/paper/2009/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf
348,Introduction,"['Efficiency', 'Requires few resources']","We analyze and compare the CPU and network time complexity of each of these methods (Section 2) and present a theoretical analysis of conditional maxent models (Section 3), including a study of the convergence of the mixture weight method, the most resource-efficient technique",Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models,https://proceedings.neurips.cc/paper/2009/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf
349,Introduction,"['Performance', 'Requires few resources']","We also report the results of large-scale experiments comparing these three methods which demonstrate the benefits of the mixture weight method (Section 4): this method consumes less resources, while achieving a performance comparable to that of standard approaches such as the distributed gradient computation method",Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models,https://proceedings.neurips.cc/paper/2009/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf
351,Conclusion,"['Requires few resources', 'Fast']","Empirical results suggest that this method achieves similar or better accuracies while reducing network usage by about three orders of magnitude and modestly reducing the wall clock time, typically by about 15% or more.",Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models,https://proceedings.neurips.cc/paper/2009/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf
352,Conclusion,['Fast'],"In distributed environments without a high rate of connectivity, the decreased network usage of the mixture weight method should lead to substantial gains in wall clock as well.",Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models,https://proceedings.neurips.cc/paper/2009/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf
353,Abstract,['Large scale'],Sign-based algorithms (e.g. SIGNSGD) have been proposed as a biased gradient compression technique to alleviate the communication bottleneck in training large neural networks across multiple workers.,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
354,Abstract,"['Qualitative evidence (e.g. examples)', 'Building on recent work', 'Identifying limitations']",We show simple convex counter-examples where signSGD does not converge to the optimum.,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
355,Abstract,"['Generalization', 'Identifying limitations']","Further, even when it does converge, signSGD may generalize poorly when compared with SGD.",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
356,Abstract,['Understanding (for researchers)'],These issues arise because of the biased nature of the sign compression operator.,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
358,Abstract,"['Formal description/analysis', 'Theoretical guarantees']","We prove that our algorithm, EF-SGD, with arbitrary compression operator, achieves the same rate of convergence as SGD without any additional assumptions, indicating that we get gradient compression for free.",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
359,Abstract,['Quantitative evidence (e.g. experiments)'],. Our experiments thoroughly substantiate the theory showing the superiority of our algorithm.,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
360,Introduction,"['Large scale', 'Building on recent work', 'Progress', 'Parallelizability / distributed']","Stochastic optimization algorithms Bottou [2010] which are amenable to large-scale parallelization, taking advantage of massive computational resources Krizhevsky et al. [2012], Dean et al. [2012] have been at the core of significant recent progress in deep learning Schmidhuber [2015], LeCun et al. [2015].",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
361,Introduction,['Building on classic work'],"To minimize a continuous (possibly) non-convex function f : R d → R, the classic stochastic gradient algorithm (SGD) Robbins and Monro [1951] performs iterations of the form",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
362,Introduction,['Used in practice/Popular'],Methods performing updates only based on the sign of each coordinate of the gradient have recently gaining popularity for training deep learning models,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
363,Introduction,"['Approximation', 'Efficiency', 'Understanding (for researchers)']","Such sign-based algorithms are particularly interesting since they can be viewed through two lenses: as i) approximations of adaptive gradient methods such as ADAM Balles and Hennig [2018], and also a ii) communication efficient gradient compression schemes",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
364,Introduction,"['Identifying limitations', 'Understanding (for researchers)']","However, we show that a severe handicap of sign-based algorithms is that they do not converge in general.",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
365,Introduction,['Qualitative evidence (e.g. examples)'],". To substantiate this claim, we present in this work simple convex counter-examples where SIGNSGD cannot converge.",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
367,Introduction,['Theoretical guarantees'],"We present an elegant solution that provably fixes these problems of SIGNSGD, namely algorithms with error-feedback.",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
369,Introduction,['Generalization'],We show that naively using biased gradient compression schemes (such as e.g. SIGNSGD) can lead to algorithms which may not generalize or even converge.,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
370,Introduction,"['Simplicity', 'Theoretical guarantees', 'Quantitative evidence (e.g. experiments)']","We show both theoretically and experimentally that simply adding error-feedback solves such problems and recovers the performance of full SGD, thereby saving on communication costs.",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
372,Introduction,['Simplicity'],"We construct a simple convex non-smooth counterexample where SIGNSGD cannot converge, even with the full batch (sub)-gradient and and tuning the step-size.",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
373,Introduction,"['Formal description/analysis', 'Qualitative evidence (e.g. examples)']",Another counterexample for a wide class of smooth convex functions proves that SIGNSGD with stochastic gradients cannot converge with batch-size one.,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
374,Introduction,['Theoretical guarantees'],"We prove that by incorporating error-feedback, SIGNSGD—as well as any other gradient compression schemes—always converge.",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
375,Introduction,['Formal description/analysis'],"Further, our theoretical analysis for non-convex smooth functions recovers the same rate as SGD, i.e. we get gradient compression for free.",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
378,Introduction,"['Generalization', 'Theoretical guarantees']",This provides a theoretical justification for why EFSIGNSGD can be expected to generalize much better than SIGNSGD.,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
379,Introduction,"['Quantitative evidence (e.g. experiments)', 'Performance', 'Avoiding train/test discrepancy']","We show extensive experiments on CIFAR10 and CIFAR100 using Resnet and VGG architectures demonstrating that EF-SIGNSGD indeed significantly outperforms SIGNSGD, and matches SGD both on test as well as train datasets while reducing communication by a factor of ∼ 64×.",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
380,Conclusion,['Generalization'],We study the effect of biased compressors on the convergence and generalization of stochastic gradient algorithms for non-convex optimization.,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
381,Conclusion,"['Generalization', 'Formal description/analysis']","We have shown that biased compressors if naively used can lead to bad generalization, and even non-convergence.",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
382,Conclusion,['Formal description/analysis'],We then show that using error-feedback all such adverse effects can be mitigated.,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
383,Conclusion,"['Formal description/analysis', 'Quantitative evidence (e.g. experiments)']","Our theory and experiments indicate that using error-feedback, our compressed gradient algorithm EF-SGD enjoys the same rate of convergence as original SGD—thereby giving compression for free. ",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
384,Conclusion,['Large scale'],We believe this should have a large impact in the design of future compressed gradient schemes for distributed and decentralized learning.,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
385,Conclusion,"['Performance', 'Identifying limitations']","Further, given the relation between sign-based methods and Adam, we believe that our results will be relevant for better understanding the performance and limitations of adaptive methods.",Error Feedback Fixes SignSGD and other Gradient Compression Schemes,https://arxiv.org/pdf/1901.09847v1.pdf
388,Abstract,"['Controllability (of model owner)', 'Principled']","We first describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way, and account for spiking variability that may vary both across neurons and across time.",Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity,https://proceedings.neurips.cc/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf
389,Abstract,"['Novelty', 'Unifying ideas or integrating components']","We then present a novel method for extracting neural trajectories, Gaussian-process factor analysis (GPFA), which unifies the smoothing and dimensionality reduction operations in a common probabilistic framework.",Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity,https://proceedings.neurips.cc/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf
396,Introduction,['Building on recent work'],The approach adopted by recent studies is to consider each neuron being recorded as a noisy sensor reflecting the timeevolution of an underlying neural process,Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity,https://proceedings.neurips.cc/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf
398,Introduction,"['Identifying limitations', 'Improvement']","While this two-stage method of performing smoothing then dimensionality reduction has provided informative low-dimensional views of neural population activity, there are several aspects that can be improved.",Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity,https://proceedings.neurips.cc/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf
399,Introduction,"['Building on recent work', 'Unifying ideas or integrating components']",We first briefly describe relatively straightforward extensions of the two-stage methods that can help to address issues (i) and (iii) above.,Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity,https://proceedings.neurips.cc/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf
400,Introduction,['Principled'],This metric can be used to compare different smoothing kernels and allows for the degree of smoothness to be chosen in a principled way.,Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity,https://proceedings.neurips.cc/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf
402,Introduction,['Building on recent work'],Previous studies have typically assumed that the neural trajectories lie in a three-dimensional space for ease of visualization.,Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity,https://proceedings.neurips.cc/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf
404,Conclusion,"['Simplicity', 'Flexibility/Extensibility', 'Easy to implement']","GPFA offers a flexible and intuitive framework for extracting neural trajectories, whose learning algorithm is stable, approximation-free, and simple to implement.",Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity,https://proceedings.neurips.cc/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf
405,Conclusion,['Controllability (of model owner)'],"Compared with two-stage methods, the choice of GP covariance allows for more explicit specification of the smoothing properties of the low-dimensional trajectories.",Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity,https://proceedings.neurips.cc/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf
406,Conclusion,['Important'],This is important when investigating (possibly subtle) properties of the system dynamics.,Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity,https://proceedings.neurips.cc/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf
407,Conclusion,['Controllability (of model owner)'],"With GPFA, it is possible to select a triangular GP covariance that assumes smoothness in position, but not in velocity",Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity,https://proceedings.neurips.cc/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf
408,Conclusion,['Identifying limitations'],"In contrast, it is unclear how to choose the shape of the smoothing kernel to achieve this in the two-stage methods",Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity,https://proceedings.neurips.cc/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf
410,Abstract,['Building on recent work'],Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps.,Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
415,Abstract,['Performance'],"The proposed SAGAN performs better than prior work , boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to ´ 18.65 on the challenging ImageNet dataset.",Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
417,Introduction,['Important'],Image synthesis is an important problem in computer vision.,Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
418,Introduction,['Progress'],"There has been remarkable progress in this direction with the emergence of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014).",Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
419,Introduction,['Successful'],"GANs based on deep convolutional networks (Radford et al., 2016; Karras et al., 2018; Zhang et al.) have been especially successful.",Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
421,Introduction,"['State-of-the-art', 'Identifying limitations']","For example, while the state-of-the-art ImageNet GAN model (Miyato & Koyama, 2018) excels at synthesizing image classes with few structural constraints (e.g., ocean, sky and landscape classes, which are distinguished more by texture than by geometry), it fails to capture geometric or structural patterns that occur consistently in some classes (for example, dogs are often drawn with realistic fur texture but without clearly defined separate feet).",Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
425,Introduction,['Efficiency'],Increasing the size of the convolution kernels can increase the representational capacity of the network but doing so also loses the computational and statistical efficiency obtained by using local convolutional structure.,Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
426,Introduction,['Efficiency'],"Self-attention (Cheng et al., 2016; Parikh et al., 2016; Vaswani et al., 2017), on the other hand, exhibits a better balance between the ability to model long-range dependencies and the computational and statistical efficiency.",Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
427,Introduction,['Efficiency'],"The self-attention module calculates response at a position as a weighted sum of the features at all positions, where the weights – or attention vectors – are calculated with only a small computational cost.",Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
431,Introduction,['Accuracy'],"Moreover, the discriminator can also more accurately enforce complicated geometric constraints on the global image structure.",Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
432,Introduction,"['Building on recent work', 'Unifying ideas or integrating components']","In addition to self-attention, we also incorporate recent insights relating network conditioning to GAN performance.",Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
433,Introduction,['Performance'],"The work by (Odena et al., 2018) showed that well-conditioned generators tend to perform better",Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
435,Introduction,"['Quantitative evidence (e.g. experiments)', 'Effectiveness', 'Stable']",We have conducted extensive experiments on the ImageNet dataset to validate the effectiveness of the proposed selfattention mechanism and stabilization techniques.,Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
436,Introduction,['Performance'],SAGAN significantly outperforms prior work in image synthesis by boosting the best reported Inception score from 36.8 to 52.52 and reducing Frechet Inception distance ´ from 27.62 to 18.65. V,Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
437,Introduction,['Facilitating use (e.g. sharing code)'],Our code is available at https://github.com/ brain-research/self-attention-gan.,Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
439,Conclusion,['Effectiveness'],The selfattention module is effective in modeling long-range dependencies.,Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
440,Conclusion,"['Stable', 'Fast']","In addition, we show that spectral normalization applied to the generator stabilizes GAN training and that TTUR speeds up training of regularized discriminators. ",Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
441,Conclusion,"['Performance', 'State-of-the-art']",SAGAN achieves the state-of-the-art performance on class-conditional image generation on ImageNet.,Self-Attention Generative Adversarial Networks,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf
442,Abstract,"['Performance', 'Used in practice/Popular']","Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, the neural networks used in practice are going wider and deeper.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
445,Abstract,"['Simplicity', 'Theoretical guarantees']","In this work, we prove why simple algorithms such as stochastic gradient descent (SGD) can find global minima on the training objective of DNNs in polynomial time.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
448,Abstract,"['Concreteness', 'Accuracy', 'Reduced training time']","As concrete examples, on the training set and starting from randomly initialized weights, we show that SGD attains 100% accuracy in classification tasks, or minimizes regression loss in linear convergence speed ε ∝ e −Ω(T) , with a number of iterations that only scales polynomial in n and L.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
449,Abstract,['Used in practice/Popular'],"Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
451,Introduction,['Successful'],"Neural networks have demonstrated a great success in numerous machine-learning tasks [6, 25, 30, 33, 36, 46, 47].",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
452,Introduction,"['Performance', 'Avoiding train/test discrepancy']","One of the empirical findings is that neural networks, trained by first-order methods from random initialization, have a remarkable ability of fitting training data [57].",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
455,Introduction,"['Used in practice/Popular', 'Optimal']","Yet, from an optimization perspective, the fact that randomly initialized first-order methods can find such an optimal solution on the training data is quite non-trivial: neural networks used in practice are often equipped with the ReLU activation function, which makes the training objective not only non-convex, but even non-smooth.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
457,Introduction,['Used in practice/Popular'],"This is in direct contrast to practice, in which ReLU networks trained by stochastic gradient descent (SGD) from random initialization almost never face the problem of non-smoothness or non-convexity, and can converge to even a global minimal over the training set quite easily.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
458,Introduction,"['Successful', 'Building on recent work']","Recently, there are quite a few papers trying to understand the success of neural networks from optimization perspective.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
459,Introduction,['Accuracy'],"In Li and Liang [34], it was shown that for a two-layer network with ReLU activation, SGD finds nearly-global optimal (say, 99% classification accuracy) solutions on the training data, as long as the network is over-parameterized, meaning that when the number of neurons is polynomially large comparing to the input size.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
460,Introduction,['Avoiding train/test discrepancy'],"Moreover, if the data is sufficiently structured (say, coming from mixtures of separable distributions), this perfect accuracy can be extended to test data as well.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
462,Introduction,['Identifying limitations'],There are also results that go beyond two-layer neural networks but with limitations.,A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
468,Introduction,"['Accuracy', 'Efficiency']",Can DNN be trained close to zero training error efficiently under mild assumptions?,A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
469,Introduction,['Theoretical guarantees'],"If so, can the running time depend only polynomially in the number of layers?",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
472,Introduction,['Used in practice/Popular'],"In practice, we cannot make the network deeper by naively stacking layers together due to the so-called vanishing / exploding gradient issues.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
474,Introduction,['Practical'],"Compared to the practical neural networks that go much deeper, the existing theory has been mostly around two-layer (thus one-hidden-layer) networks even just for the training process alone.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
475,Introduction,['Formal description/analysis'],It is natural to ask if we can theoretically understand how the training process has worked for multi-layer neural networks.,A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
484,1.1 Our Result,['Accuracy'],"• Using the same network, if the task is multi-label classiﬁcation, then GD and SGD ﬁnd an 100% accuracy classiﬁer on the training set in T = poly(n, L, δ−1) iterations.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
486,1.1 Our Result,"['Generalization', 'Avoiding train/test discrepancy']",This paper does not cover the the generalization of over-parameterized neural networks to the test data.,A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
487,1.1 Our Result,['Generalization'],"We refer interested readers to some practical evidence [53, 56] that deeper (and wider) neural networks actually generalize better.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
492,1.1 Our Result,['Reduced training time'],"For fully-connected networks, their provable training time is exponential in the number of layers, leading to a claim of the form “ResNet has an advantage because ResNet is polynomial-time but fully-connected network is exponential-time.” As we prove in this paper, fully-connected networks do have polynomial training time, so the logic behind their claim is wrong.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
498,1.1 Our Result,"['Generalization', 'Flexibility/Extensibility']",Their result only applies to smooth activation functions and thus cannot apply to the state-of-the-art ReLU activation.,A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
505,1.2 Other Related Works,['Important'],Linear networks without activation functions are important subjects on its own.,A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
519,1.2 Other Related Works,['Simplicity'],"For experts in DNN theory, one may view this present paper as a deeply-simpliﬁed version of the recurrent neural network (RNN) paper [5] by the same set of authors.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
522,1.2 Other Related Works,['Simplicity'],"So, the over-parameterized convergence theory of DNN is much simpler than that of RNN.",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
523,1.2 Other Related Works,"['Simplicity', 'Flexibility/Extensibility', 'Important']","We write this DNN result as a separate paper because: (1) not all the readers can easily notice that DNN is easier to study than RNN; (2) we believe the convergence of DNN is important on its own; (3) the proof in this paper is much simpler (30 vs 80 pages) and could reach out to a wider audience; (4) the simplicity of this paper allows us to tighten parameters in some non-trivial ways; and (5) the simplicity of this paper allows us to also study convolutional networks, residual networks, as well as diﬀerent loss functions (all of them were missing from [5]).",A Convergence Theory for Deep Learning via Over-Parameterization,https://arxiv.org/pdf/1811.03962v3.pdf
529,Abstract,"['Formal description/analysis', 'Efficiency']","To amend the problem, this paper proposes conducting theoretical analysis of learning to rank algorithms through inves- tigations on the properties of the loss func- tions, including consistency, soundness, con- tinuity, diﬀerentiability, convexity, and eﬃ- ciency.",Listwise Approach to Learning to Rank - Theory and Algorithm,http://icml2008.cs.helsinki.fi/papers/167.pdf
533,Abstract,"['Novelty', 'Quantitative evidence (e.g. experiments)']","The use of the likelihood loss leads to the development of a new listwise method called ListMLE, whose loss function oﬀers better properties, and also leads to better experimental results.",Listwise Approach to Learning to Rank - Theory and Algorithm,http://icml2008.cs.helsinki.fi/papers/167.pdf
534,Introduction,"['Used in practice/Popular', 'Applies to real world']","Ranking, which is to sort objects based on certain fac- tors, is the central problem of applications such as in- formation retrieval (IR) and information ﬁltering.",Listwise Approach to Learning to Rank - Theory and Algorithm,http://icml2008.cs.helsinki.fi/papers/167.pdf
535,Introduction,['Successful'],"Re- cently machine learning technologies called ‘learning to rank’ have been successfully applied to ranking, and several approaches have been proposed, including the pointwise, pairwise, and listwise approaches.",Listwise Approach to Learning to Rank - Theory and Algorithm,http://icml2008.cs.helsinki.fi/papers/167.pdf
541,Introduction,['Novelty'],"Existing work on the listwise approach mainly fo- cused on the development of new algorithms, such as RankCosine and ListNet.",Listwise Approach to Learning to Rank - Theory and Algorithm,http://icml2008.cs.helsinki.fi/papers/167.pdf
544,Introduction,['Understanding (for researchers)'],"This largely prevented us from deeply un- derstanding the approach, more critically, from devis- ing more advanced algorithms.",Listwise Approach to Learning to Rank - Theory and Algorithm,http://icml2008.cs.helsinki.fi/papers/167.pdf
551,Introduction,['Efficiency'],"Second, we evaluate a surrogate loss function from four aspects: (a) consistency, (b) soundness, (c) mathemat- ical properties of continuity, diﬀerentiability, and con- vexity, and (d) computational eﬃciency in learning.",Listwise Approach to Learning to Rank - Theory and Algorithm,http://icml2008.cs.helsinki.fi/papers/167.pdf
553,Introduction,['Novelty'],"The ﬁrst one is newly proposed in this paper, and the last two were used in RankCosine and ListNet, respectively.",Listwise Approach to Learning to Rank - Theory and Algorithm,http://icml2008.cs.helsinki.fi/papers/167.pdf
554,Introduction,['Novelty'],"Third, we propose a novel method for the listwise ap- proach, which we call ListMLE.",Listwise Approach to Learning to Rank - Theory and Algorithm,http://icml2008.cs.helsinki.fi/papers/167.pdf
556,Introduction,['Effectiveness'],"Due to the nice properties of the loss function, ListMLE stands to be more eﬀective than RankCosine and ListNet.",Listwise Approach to Learning to Rank - Theory and Algorithm,http://icml2008.cs.helsinki.fi/papers/167.pdf
560,Conclusion,"['Formal description/analysis', 'Effectiveness']","We have pointed out that to understand the eﬀective- ness of a learning to rank algorithm, it is necessary to conduct theoretical analysis on its loss function.",Listwise Approach to Learning to Rank - Theory and Algorithm,http://icml2008.cs.helsinki.fi/papers/167.pdf
561,Conclusion,['Efficiency'],"We propose investigating a loss function from the view- points of (a) consistency, (b) soundness, (c) continu- ity, diﬀerentiability, convexity, and (d) eﬃciency.",Listwise Approach to Learning to Rank - Theory and Algorithm,http://icml2008.cs.helsinki.fi/papers/167.pdf
565,Conclusion,"['Quantitative evidence (e.g. experiments)', 'Effectiveness']","We have then de- veloped a new learning algorithm using the likelihood loss, called ListMLE and demonstrated its eﬀective- ness through experiments.",Listwise Approach to Learning to Rank - Theory and Algorithm,http://icml2008.cs.helsinki.fi/papers/167.pdf
572,abstract,['Parallelizability / distributed'],The particular online algorithms we study here maintain a Gaussian distribution over parameter vectors and update the mean and covariance of the distribution with each instance. ,Confidence-weighted linear classification,http://icml2008.cs.helsinki.fi/papers/322.pdf
573,abstract,"['Quantitative evidence (e.g. experiments)', 'State-of-the-art', 'Improvement', 'Parallelizability / distributed', 'Fast']","Empirical evaluation on a range of NLP tasks show that our algorithm improves over other state of the art online and batch methods, learns faster in the online setting, and lends itself to better classifier combination after parallel training.",Confidence-weighted linear classification,http://icml2008.cs.helsinki.fi/papers/322.pdf
574,introduction,"['Simplicity', 'Performance', 'Practical', 'Fast', 'Valid assumptions']","Online learning algorithms operate on a single instance at a time, allowing for updates that are fast, simple, make few assumptions about the data, and perform well in wide range of practical settings. ",Confidence-weighted linear classification,http://icml2008.cs.helsinki.fi/papers/322.pdf
575,introduction,['Used in practice/Popular'],"Online learning algorithms have become especially popular in natural language processing for tasks including classification, tagging, and parsing. ",Confidence-weighted linear classification,http://icml2008.cs.helsinki.fi/papers/322.pdf
579,introduction,['Improvement'],"Therefore, it is worth investigating whether online learning algorithms for linear classifiers could be improved to take advantage of these particularities of natural language data. ",Confidence-weighted linear classification,http://icml2008.cs.helsinki.fi/papers/322.pdf
580,introduction,['Novelty'],"We introduce confidence-weighted (CW) learning, a new class of online learning methods that maintain a probabilistic measure of confidence in each parameter. ",Confidence-weighted linear classification,http://icml2008.cs.helsinki.fi/papers/322.pdf
582,introduction,['Formal description/analysis'],"Parameter confidence is formalized with a Gaussian distribution over parameter vectors, which is updated for each new training instance so that the probability of correct classification for that instance under the updated distribution meets a specified confidence. ",Confidence-weighted linear classification,http://icml2008.cs.helsinki.fi/papers/322.pdf
583,introduction,"['Novelty', 'Accuracy', 'State-of-the-art', 'Parallelizability / distributed', 'Fast']","We show superior classification accuracy over state-of-the-art online and batch baselines, faster learning, and new classifier combination methods after parallel training. ",Confidence-weighted linear classification,http://icml2008.cs.helsinki.fi/papers/322.pdf
584,introduction,"['Quantitative evidence (e.g. experiments)', 'Beneficence']",We begin with a discussion of the motivating particularities of natural language data. We then derive our algorithm and discuss variants. A series of experiments shows CW learning’s empirical benefits. We conclude with a discussion of related work.,Confidence-weighted linear classification,http://icml2008.cs.helsinki.fi/papers/322.pdf
585,conclusion,['Novelty'],"We have presented confidenceweighted linear classifiers, a new learning method designed for NLP problems based on the notion of parameter confidence. ",Confidence-weighted linear classification,http://icml2008.cs.helsinki.fi/papers/322.pdf
586,conclusion,"['Improvement', 'Parallelizability / distributed']",The algorithm maintains a distribution over parameter vectors; online updates both improve the parameter estimates and reduce the distribution’s variance.,Confidence-weighted linear classification,http://icml2008.cs.helsinki.fi/papers/322.pdf
587,conclusion,"['Improvement', 'Fast']",Our method improves over both online and batch methods and learns faster on a dozen NLP datasets. ,Confidence-weighted linear classification,http://icml2008.cs.helsinki.fi/papers/322.pdf
588,conclusion,"['Performance', 'Improvement', 'Parallelizability / distributed']","Additionally, our new algorithms allow more intelligent classifier combination techniques, yielding improved performance after parallel learning. ",Confidence-weighted linear classification,http://icml2008.cs.helsinki.fi/papers/322.pdf
594,abstract,"['Performance', 'Optimal']","Moreover, one of our algorithms asymptotically achieves optimal worst-case performance even if users’ interests change.",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
596,introduction,['Label efficiency (reduced need for labeled data)'],The conventional approach to this learningto-rank problem has been to assume the availability of manually labeled training data. ,Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
597,introduction,['Label efficiency (reduced need for labeled data)'],"Usually, this data consists of a set of documents judged as relevant or not to specific queries, or of pairwise judgments comparing the relative relevance of pairs of documents. ",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
598,introduction,['Optimal'],"These judgments are used to optimize a ranking function offline, to a standard information retrieval metric, then deploying the learned function in a live search engine.",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
599,introduction,"['Novelty', 'Formal description/analysis']",We propose a new learning to rank problem formulation that differs in three fundamental ways. ,Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
600,introduction,['Label efficiency (reduced need for labeled data)'],"First, unlike most previous methods, we learn from usage data rather than manually labeled relevance judgments. ",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
601,introduction,"['Low cost', 'Large scale']",Usage data is available in much larger quantities and at much lower cost.,Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
602,introduction,"['Realistic output', 'Data efficiency']","Moreover, unlike manual judgments, which need to be constantly updated to stay relevant, usage data naturally reflects current users’ needs and the documents currently available. ",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
603,introduction,"['User influence', 'Optimal']","Although some researchers have transformed usage data into relevance judgments, or used it to generate features (e.g. Joachims, 2002; Radlinski & Joachims, 2005; Agichtein et al., 2006), we go one step further by directly optimizing a usage-based metric. ",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
604,introduction,['Data efficiency'],"Second, we propose an online learning approach for learning from usage data. ",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
605,introduction,['Data efficiency'],"As training data is being collected, it immediately impacts the rankings shown.
",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
607,introduction,['Data efficiency'],"In particular, in this setting there is a natural tradeoff between exploration and exploitation: It may be valuable in the long run to present some rankings with unknown documents, to allow training data about these documents to be collected. ",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
608,introduction,"['Building on classic work', 'Optimal']","In contrast, in the short run exploitation is typically optimal. With only few exceptions (e.g. Radlinski & Joachims, 2007), previous work does not consider such an online approach.",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
609,introduction,['Building on recent work'],"Third and most importantly, except for (Chen & Karger, 2006), previous algorithms for learning to rank have considered the relevance of each document independently of other documents. ",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
610,introduction,"['Preciseness', 'Performance', 'Optimal']","This is reflected in the performance measures typically optimized, such as Precision, Recall, Mean Average Precision (MAP) (Baeza-Yates & Ribeiro-Neto, 1999) and Normalized Discounted Cumulative Gain (NDCG) (Burges et al., 2006). ",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
611,introduction,['Building on recent work'],"In fact, recent work has shown that these measures do not necessarily correlate with user satisfaction (Turpin & Scholer, 2006). ",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
613,introduction,['Diverse output'],"Moreover, web queries often have different meanings for different users (a canonical example is the query jaguar ) suggesting that a ranking with diverse documents may be preferable.",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
614,Conclusions and Extensions,"['Novelty', 'Formal description/analysis', 'Diverse output']",We have presented a new formulation of the learning to rank problem that explicitly takes into account the relevance of different documents being interdependent. ,Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
616,Conclusions and Extensions,"['Theoretical guarantees', 'Performance', 'Used in practice/Popular']","We have shown that the learning problem can be solved in a theoretically sound manner, and that our algorithms can be expected to perform reasonably in practice. ",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
617,Conclusions and Extensions,"['Quantitative evidence (e.g. experiments)', 'Performance', 'Used in practice/Popular', 'User influence']","We plan to extend this work by addressing the nonbinary document relevance settings, and perform empirical evaluations using real users and real documents. ",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
618,Conclusions and Extensions,"['Improvement', 'Fast']","Furthermore, we plan to investigate how prior knowledge can be incorporated into the algorithms to improve speed of convergence. ",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
619,Conclusions and Extensions,['Improvement'],"Finally, we plan to investigate if the bandits at different ranks can be coupled to improve the rate at which RBA converges.",Learning diverse rankings with multi-armed bandits,http://icml2008.cs.helsinki.fi/papers/264.pdf
620,abstract,['Optimal '],"Many applications require optimizing an unknown, noisy function that is expensive to evaluate. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
621,abstract,['Formal description/analysis'],"We formalize this task as a multiarmed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
622,abstract,"['Novelty', 'Optimal ']","We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
623,abstract,"['Novelty', 'Quantitative evidence (e.g. experiments)', 'Optimal ']","We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
626,abstract,"['Quantitative evidence (e.g. experiments)', 'Optimal ']","In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
627,introduction,"['Low cost', 'Optimal ']","In most stochastic optimization settings, evaluating the unknown function is expensive, and sampling is to be minimized. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
628,introduction,"['Qualitative evidence (e.g. examples)', 'Applies to real world', 'Optimal ']","Examples include choosing advertisements in sponsored search to maximize profit in a click-through model (Pandey & Olston, 2007) or learning optimal control strategies for robots (Lizotte et al., 2007). ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
629,introduction,"['Quantitative evidence (e.g. experiments)', 'Qualitative evidence (e.g. examples)', 'Optimal ']","Predominant approaches to this problem include the multi-armed bandit paradigm (Robbins, 1952), where the goal is to maximize cumulative reward by optimally balancing exploration and exploitation, and experimental design (Chaloner & Verdinelli, 1995), where the function is to be explored globally with as few evaluations as possible, for example by maximizing information gain. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
630,introduction,"['Approximation', 'Optimal ']","The challenge in both approaches is twofold: we have to estimate an unknown function f from noisy samples, and we must optimize our estimate over some high-dimensional input space. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
631,introduction,"['Flexibility/Extensibility', 'Building on classic work']","For the former, much progress has been made in machine learning through kernel methods and Gaussian process (GP) models (Rasmussen & Williams, 2006), where smoothness assumptions about f are encoded through the choice of kernel in a flexible nonparametric fashion.",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
632,introduction,['Formal description/analysis'],"Beyond Euclidean spaces, kernels can be defined on diverse domains such as spaces of graphs, sets, or lists.",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
633,introduction,['Optimal '],"We are concerned with GP optimization in the multiarmed bandit setting, where f is sampled from a GP distribution or has low “complexity” measured in terms of its RKHS norm under some kernel. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
634,introduction,['Optimal '],"We provide the first sublinear regret bounds in this nonparametric setting, which imply convergence rates for GP optimization. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
635,introduction,['Simplicity'],"In particular, we analyze the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, a simple and intuitive Bayesian method (Auer et al., 2002; Auer, 2002; Dani et al., 2008). ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
636,introduction,"['Formal description/analysis', 'Quantitative evidence (e.g. experiments)', 'Fast']","While objectives are different in the multi-armed bandit and experimental design paradigm, our results draw a close technical connection between them: our regret bounds come in terms of an information gain quantity, measuring how fast f can be learned in an information theoretic sense. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
638,related work,"['Generalization', 'Optimal ']","Our work generalizes stochastic linear optimization in a bandit setting, where the unknown function comes from a finite-dimensional linear space. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
640,related work,['Building on classic work'],"For the standard linear setting, Dani et al. (2008) provide a near-complete characterization explicitly dependent on the dimensionality. In the GP setting, the challenge is to characterize complexity in a different manner, through properties of the kernel function. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
642,related work,['Building on classic work'],"Kleinberg et al. (2008) provide regret bounds under weaker and less configurable assumptions (only Lipschitz-continuity w.r.t. a metric is assumed; Bubeck et al. 2008 consider arbitrary topological spaces), which however degrade rapidly with the dimensionality of the problem (Ω(T d+1 d+2 )).",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
644,related work,['Qualitative evidence (e.g. examples)'],"Adopting GP assumptions, we can model levels of smoothness in a fine-grained way. For example, our rates for the frequently used Squared Exponential kernel, enforcing a high degree of smoothness, have weak dependence on the dimensionality: O( p T(log T) d+1) (see Fig. 1).",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
645,related work,"['Successful', 'Building on classic work', 'Improvement', 'Applies to real world', 'Optimal ']","There is a large literature on GP (response surface) optimization. Several heuristics for trading off exploration and exploitation in GP optimization have been proposed (such as Expected Improvement, Mockus et al. 1978, and Most Probable Improvement, Mockus 1989) and successfully applied in practice (c.f., Lizotte et al. 2007). ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
646,related work,"['Efficiency', 'Building on classic work', 'Optimal ']",Brochu et al. (2009) provide a comprehensive review of and motivation for Bayesian optimization using GPs. The Efficient Global Optimization (EGO) algorithm for optimizing expensive black-box functions is proposed by Jones et al. (1998) and extended to GPs by Huang et al. (2006). ,Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
647,related work,"['Theoretical guarantees', 'Performance', 'Building on classic work', 'Optimal ']","Little is known about theoretical performance of GP optimization. While convergence of EGO is established by Vazquez & Bect (2007), convergence rates have remained elusive. Gr¨unew¨alder et al. (2010) consider the pure exploration problem for GPs, where the goal is to find the optimal decision over T rounds, rather than maximize cumulative reward (with no exploration/exploitation dilemma). ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
648,related work,"['Theoretical guarantees', 'Performance', 'Optimal ']",They provide sharp bounds for this exploration problem. Note that this methodology would not lead to bounds for minimizing the cumulative regret. Our cumulative regret bounds translate to the first performance guarantees (rates) for GP optimization.,Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
649,related work,['Optimal '],"Our main contributions are: • We analyze GP-UCB, an intuitive algorithm for GP optimization, when the function is either sampled from a known GP, or has low RKHS norm.",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
650,related work,"['Novelty', 'Quantitative evidence (e.g. experiments)', 'Unifying ideas or integrating components', 'Optimal ']","We bound the cumulative regret for GP-UCB in terms of the information gain due to sampling, establishing a novel connection between experimental design and GP optimization. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
651,related work,"['Novelty', 'Optimal ']","• By bounding the information gain for popular classes of kernels, we establish sublinear regret bounds for GP optimization for the first time. Our bounds depend on kernel choice and parameters in a fine-grained fashion. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
652,related work,['Optimal '],"• We evaluate GP-UCB on sensor network data, demonstrating that it compares favorably to existing algorithms for GP optimization.",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
653,conclusion,['Optimal '],"We prove the first sublinear regret bounds for GP optimization with commonly used kernels (see Figure 1), both for f sampled from a known GP and f of low RKHS norm. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
654,conclusion,"['Novelty', 'Quantitative evidence (e.g. experiments)', 'Optimal ']","We analyze GP-UCB, an intuitive, Bayesian upper confidence bound based sampling rule. Our regret bounds crucially depend on the information gain due to sampling, establishing a novel connection between bandit optimization and experimental design. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
655,conclusion,['Scientific methodology'],"We bound the information gain in terms of the kernel spectrum, providing a general methodology for obtaining regret bounds with kernels of interest. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
656,conclusion,"['Quantitative evidence (e.g. experiments)', 'Performance', 'Optimal ']","Our experiments on real sensor network data indicate that GPUCB performs at least on par with competing criteria for GP optimization, for which no regret bounds are known at present. ",Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
657,conclusion,['Understanding (for researchers)'],Our results provide an interesting step towards understanding exploration–exploitation tradeoffs with complex utility functions.,Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,https://icml.cc/Conferences/2010/papers/422.pdf
659,abstract,"['Formal description/analysis', 'Human-like mechanism']","Here, we formalize such training strategies in the context of machine learning, and call them “curriculum learning”. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
660,abstract,['Building on recent work'],"In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
661,abstract,"['Generalization', 'Quantitative evidence (e.g. experiments)', 'Improvement']",The experiments show that significant improvements in generalization can be achieved. ,Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
662,abstract,"['Generality', 'Effectiveness']","We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
666,introduction,"['Building on classic work', 'Learning from humans']","Previous research (Elman, 1993; Rohde & Plaut, 1999; Krueger & Dayan, 2009) at the intersection of cognitive science and machine learning has raised the following question: can machine learning algorithms benefit from a similar training strategy? ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
667,introduction,['Building on classic work'],"The idea of training a learning machine with a curriculum can be traced back at least to Elman (1993). The basic idea is to start small, learn easier aspects of the task or easier subtasks, and then gradually increase the difficulty level. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
668,introduction,"['Simplicity', 'Quantitative evidence (e.g. experiments)', 'Successful']","The experimental results, based on learning a simple grammar with a recurrent network (Elman, 1993), suggested that successful learning of grammatical structure depends, not on innate knowledge of grammar, but on starting with a limited architecture that is at first quite restricted in complexity, but then expands its resources gradually as it learns. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
669,introduction,"['Simplicity', 'Learning from humans']","Such conclusions are important for developmental psychology, because they illustrate the adaptive value of starting, as human infants do, with a simpler initial state, and then building on that to develop more and more sophisticated representations of structure. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
670,introduction,['Learning from humans'],Elman (1993) makes the statement that this strategy could make it possible for humans to learn what might otherwise prove to be unlearnable. ,Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
671,introduction,"['Simplicity', 'Identifying limitations']","However, these conclusions have been seriously questioned in Rohde and Plaut (1999). The question of guiding learning of a recurrent neural network for learning a simple language and increasing its capacity along the way was recently revisited from the cognitive perspective (Krueger & Dayan, 2009), providing evidence for faster convergence using a shaping procedure. Similar ideas were also explored in robotics (Sanger, 1994), by gradually making the learning task more difficult.",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
672,introduction,['Beneficence'],"We want to clarify when and why a curriculum or “starting small” strategy can benefit machine learning algorithms. 
",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
673,introduction,"['Simplicity', 'Generalization', 'Improvement']",We contribute to this question by showing several cases - involving vision and language tasks - in which very simple multi-stage curriculum strategies give rise to improved generalization and faster convergence. ,Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
675,introduction,['Scientific methodology'],"This hypothesis is essentially that a well chosen curriculum strategy can act as a continuation method (Allgower & Georg, 1980), i.e., can help to find better local minima of a non-convex training criterion. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
676,introduction,"['Quantitative evidence (e.g. experiments)', 'Beneficence']","In addition, the experiments reported here suggest that (like other strategies recently proposed to train deep deterministic or stochastic neural networks) the curriculum strategies appear on the surface to operate like a regularizer, i.e., their beneficial effect is most pronounced on the test set. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
677,introduction,['Quantitative evidence (e.g. experiments)'],"Furthermore, experiments on convex criteria also show that a curriculum strategy can speed the convergence of training towards the global minimum.",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
678,discussion and future work ,"['Quantitative evidence (e.g. experiments)', 'Building on classic work', 'Beneficence']","We started with the following question left from previous cognitive science research (Elman, 1993; Rohde & Plaut, 1999): can machine learning algorithms benefit from a curriculum strategy? Our experimental results in many different settings bring evidence towards a positive answer to that question. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
680,discussion and future work ,['Deferral to humans'],"After all, the art of teaching is difficult and humans do not agree among themselves about the order in which concepts should be introduced to pupils.",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
681,discussion and future work ,['Successful'],"From the machine learning point of view, once the success of some curriculum strategies has been established, the important questions are: why? and how?",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
683,discussion and future work ,['Learning from humans'],"• faster training in the online setting (i.e. faster both from an optimization and statistical point of view) because the learner wastes less time with noisy or harder to predict examples (when it is not ready to incorporate them),",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
684,discussion and future work ,['Generalization'],"• guiding training towards better regions in parameter space, i.e. into basins of attraction (local minima) of the descent procedure associated with better generalization: a curriculum can be seen as a particular continuation method.",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
685,discussion and future work ,['Quantitative evidence (e.g. experiments)'],"Faster convergence with a curriculum was already observed in (Krueger & Dayan, 2009). However, unlike in our experiments where capacity is fixed throughout the curriculum, they found that compared to using no curriculum, worse results were obtained with fixed neural resources. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
686,discussion and future work ,['Generalization'],"The reasons for these differences remain to be clarified. In both cases, though, an appropriate curriculum strategy acts to help the training process (faster convergence to better solutions), and we even find that it regularizes, giving rise to lower generalization error for the same training error. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
687,discussion and future work ,"['Generalization', 'Improvement']","This is like in the case of unsupervised pre-training (Erhan et al., 2009), and again it remains to be clarified why one would expect improved generalization, for both curriculum and unsupervised pre-training procedures. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
688,discussion and future work ,"['Generality', 'Understanding (for researchers)']","The way we have defined curriculum strategies leaves a lot to be defined by the teacher. It would be nice to understand general principles that make some curriculum strategies work better than others, and this clearly should be the subject of future work on curriculum learning. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
689,discussion and future work ,['Learning from humans'],"In particular, to reap the advantages of a curriculum strategy while minimizing the amount of human (teacher) effort involved, it is natural to consider a form of active selection of examples similar to what humans (and in particular children) do. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
690,discussion and future work ,['Improvement'],"At any point during the “education” of a learner, some examples can be considered “too easy” (not helping much to improve the current model), while some examples can be considered “too difficult” (no small change in the model would allow to capture these examples). ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
692,discussion and future work ,"['Quantitative evidence (e.g. experiments)', 'Human-like mechanism', 'Automatic', 'Useful']","Such an approach could be used to at least automate the pace at which a learner would move along a predefined curriculum. In the experiments we performed, that pace was fixed arbitrarily. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
697,discussion and future work ,['Performance'],"Curriculum strategies are also connected to transfer (or multi-task) learning and lifelong learning (Thrun, 1996). Curriculum learning strategies can be seen as a special form of transfer learning where the initial tasks are used to guide the learner so that it will perform better on the final task. ",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
698,discussion and future work ,"['Generalization', 'Improvement']","Whereas the traditional motivation for multitask learning is to improve generalization by sharing across tasks, curriculum learning adds the notion of guiding the optimization process, either to converge faster, or more importantly, to guide the learner towards better local minima.",Curriculum learning,https://dl.acm.org/doi/abs/10.1145/1553374.1553380
700,abstract,['Effectiveness'],"This paper focuses on learning the basis set, also called dictionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. ",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
701,abstract,"['Approximation', 'Scales up']","This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples. ",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
702,abstract,"['Quantitative evidence (e.g. experiments)', 'Performance']","A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
703,introduction,['State-of-the-art'],"The linear decomposition of a signal using a few atoms of a learned dictionary instead of a predefined one—based on wavelets (Mallat, 1999) for example—has recently led to state-of-the-art results for numerous low-level image processing tasks such as denoising (Elad & Aharon, 2006) as well as higher-level tasks such as classification (Raina et al., 2007; Mairal et al., 2009), showing that sparse learned models are well adapted to natural signals. ",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
704,introduction,['Flexibility/Extensibility'],"Unlike decompositions based on principal component analysis and its variants, these models do not impose that the basis vectors be orthogonal, allowing more flexibility to adapt the representation to the data. ",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
705,introduction,"['State-of-the-art', 'Effectiveness', 'Improvement']","While learning the dictionary has proven to be critical to achieve (or improve upon) state-of-the-art results, effectively solving the corresponding optimization problem is a significant computational challenge, particularly in the context of the largescale datasets involved in image processing tasks, that may include millions of training samples. Addressing this challenge is the topic of this paper.",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
706,introduction,"['Approximation', 'Concreteness']","Concretely, consider a signal x in R m. We say that it admits a sparse approximation over a dictionary D in R m×k, with k columns referred to as atoms, when one can find a linear combination of a “few” atoms from D that is “close” to the signal x. ",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
707,introduction,"['Quantitative evidence (e.g. experiments)', 'Effectiveness']","Experiments have shown that modelling a signal with such a sparse decomposition (sparse coding) is very effective in many signal processing applications (Chen et al., 1999). ",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
709,introduction,['Improvement'],"However, learning the dictionary instead of using off-the-shelf bases has been shown to dramatically improve signal reconstruction (Elad & Aharon, 2006). ",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
712,introduction,"['Quantitative evidence (e.g. experiments)', 'Data efficiency', 'Effectiveness']","Although they have shown experimentally to be much faster than first-order gradient descent methods (Lee et al., 2007), they cannot effectively handle very large training sets (Bottou & Bousquet, 2008), or dynamic training data changing over time, such as video sequences. ",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
715,introduction,['Approximation'],"In this setting, online techniques based on stochastic approximations are an attractive alternative to batch methods (Bottou, 1998). ",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
716,introduction,['Qualitative evidence (e.g. examples)'],"For example, first-order stochastic gradient descent with projections on the constraint set is sometimes used for dictionary learning (see Aharon and Elad (2008) for instance). ",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
717,introduction,"['Memory efficiency', 'Label efficiency (reduced need for labeled data)', 'Energy efficiency']","We show in this paper that it is possible to go further and exploit the specific structure of sparse coding in the design of an optimization procedure dedicated to the problem of dictionary learning, with low memory consumption and lower computational cost than classical second-order batch algorithms and without the need of explicit learning rate tuning. ",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
718,introduction,"['Quantitative evidence (e.g. experiments)', 'Data efficiency', 'Scales up']","As demonstrated by our experiments, the algorithm scales up gracefully to large datasets with millions of training samples, and it is usually faster than more standard methods.",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
719,discussion,['Novelty'],"We have introduced in this paper a new stochastic online algorithm for learning dictionaries adapted to sparse coding tasks, and proven its convergence. ",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
720,discussion,['Quantitative evidence (e.g. experiments)'],"Preliminary experiments demonstrate that it is significantly faster than batch alternatives on large datasets that may contain millions of training examples, yet it does not require learning rate tuning like regular stochastic gradient descent methods. ",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
721,discussion,"['Quantitative evidence (e.g. experiments)', 'Promising']","More experiments are of course needed to better assess the promise of this approach in image restoration tasks such as denoising, deblurring, and inpainting. ",Online dictionary learning for sparse coding,https://dl.acm.org/doi/abs/10.1145/1553374.1553463
724,abstract,['Large scale'],"Scaling such models to full-sized, high-dimensional images remains a di_x000E_cult problem.",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
725,abstract,"['Realistic output', 'Large scale']","To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. ",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
726,abstract,['Flexibility/Extensibility'],This model is translation-invariant and supports e_x000E_cient bottom-up and top-down probabilistic inference. ,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
727,abstract,['Novelty'],"Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. ",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
728,abstract,"['Quantitative evidence (e.g. experiments)', 'Label efficiency (reduced need for labeled data)', 'Useful']","Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. ",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
729,abstract,['Performance'],We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
732,introduction,['Qualitative evidence (e.g. examples)'],"For instance, lower layers could support object detection by spotting low-level features indicative of object parts. ",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
734,introduction,['Simplicity'],"Deep architectures consist of feature detector units arranged in layers. Lower layers detect simple features and feed into higher layers, which in turn detect more complex features. ",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
736,introduction,"['Approximation', 'Data efficiency']","In particular, the deep belief network (DBN) (Hinton et al., 2006) is a multilayer generative model where each layer encodes statistical dependencies among the units in the layer below it; it is trained to (approximately) maximize the likelihood of its training data.",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
737,introduction,['Successful'],"DBNs have been successfully used to learn high-level structure in a wide variety of domains, including handwritten digits (Hinton et al., 2006) and human motion capture data (Taylor et al., 2007). ",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
738,introduction,['Understanding (for researchers)'],We build upon the DBN in this paper because we are interested in learning a generative model of images which can be trained in a purely unsupervised manner.,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
739,introduction,"['Realistic output', 'Large scale', 'Successful']","While DBNs have been successful in controlled domains, scaling them to realistic-sized (e.g., 200x200 pixel) images remains challenging for two reasons. ",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
740,introduction,['Large scale'],"First, images are high-dimensional, so the algorithms must scale gracefully and be computationally tractable even when applied to large images. ",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
742,introduction,['Useful'],"We address these issues by incorporating translation invariance. Like LeCun et al. (1989) and Grosse et al. (2007), we learn feature detectors which are shared among all locations in an image, because features which capture useful information in one part of an image can pick up the same information elsewhere. ",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
744,introduction,['Scales up'],"This paper presents the convolutional deep belief network, a hierarchical generative model that scales to full-sized images. ",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
745,introduction,['Novelty'],"Another key to our approach is probabilistic max-pooling, a novel technique that allows higher-layer units to cover larger areas of the input in a probabilistically sound way. ",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
746,introduction,"['Realistic output', 'Scales up']","To the best of our knowledge, ours is the _x000C_rst translation invariant hierarchical generative model which supports both top-down and bottom-up probabilistic inference and scales to realistic image sizes. ",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
748,introduction,['Performance'],"We show that these representations achieve excellent performance on several visual recognition tasks and allow \hidden"" object parts to be inferred from high-level object information.",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
749,conclusion,"['Performance', 'Scales up']","We presented the convolutional deep belief network, a scalable generative model for learning hierarchical representations from unlabeled images, and showed that our model performs well in a variety of visual recognition tasks. ",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
750,conclusion,"['Promising', 'Scales up']","We believe our approach holds promise as a scalable algorithm for learning hierarchical representations from high-dimensional, complex data.",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/abs/10.1145/1553374.1553453
752,abstract,"['Human-like mechanism', 'Performance']","Less surprisingly, image classifiers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. ",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
753,abstract,"['Theoretical guarantees', 'Quantitative evidence (e.g. experiments)']",In this paper we provide both empirical and theoretical evidence that these are two manifestations of the same underlying phenomenon. ,Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
754,abstract,['Robustness'],"We establish close connections between the adversarial robustness and corruption robustness research programs, with the strongest connection in the case of additive Gaussian noise. ",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
755,abstract,"['Robustness', 'Realistic output', 'Generality', 'Performance', 'Improvement']",This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. ,Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
756,abstract,['Robustness'],Based on our results we recommend that future adversarial defenses consider evaluating the robustness of their methods to distributional shift with benchmarks such as ImageNet-C.,Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
757,introduction,"['Performance', 'State-of-the-art']",State-of-the-art computer vision models can achieve impressive performance on many image classification tasks. ,Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
758,introduction,"['Robustness', 'Human-like mechanism']","Despite this, these same models still lack the robustness of the human visual system to various forms of image corruptions.",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
759,introduction,"['Robustness', 'Deferral to humans']","For example, they are distinctly subhuman when classifying images distorted with additive Gaussian noise (Dodge & Karam, 2017), they lack robustness to different types of blur, pixelation, and changes in brightness (Hendrycks & Dietterich, 2019), lack robustness to random translations of the input (Azulay & Weiss, 2018), and even make errors when foreign objects are inserted into the field of view (Rosenfeld et al., 2018).",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
762,introduction,"['Robustness', 'Improvement', 'Parallelizability / distributed']",The machine learning community has researchers working on each of these two types of errors: adversarial example researchers seek to measure and improve robustness to small-worst case perturbations of the input while corruption robustness researchers seek to measure and improve model robustness to distributional shift. ,Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
763,introduction,"['Robustness', 'Unifying ideas or integrating components', 'Parallelizability / distributed']","In this work we analyze the connection between these two research directions, and we see that adversarial robustness is closely related to robustness to certain kinds of distributional shift. ",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
765,introduction,"['Novelty', 'Unifying ideas or integrating components']","We make this connection in several ways. First, in Section 4, we provide a novel analysis of the error set of an image classifier. ",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
767,introduction,"['Robustness', 'Improvement']","In Section 5, we show that improving an alternate notion of adversarial robustness requires that error rates under large additive noise be reduced to essentially zero.",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
768,introduction,"['Robustness', 'Improvement']","Finally, this suggests that methods which increase the distance to the decision boundary should also improve robustness to Gaussian noise, and vice versa. ",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
770,introduction,"['Robustness', 'Effectiveness', 'Successful']",We also show that measuring corruption robustness can effectively distinguish successful adversarial defense methods from ones that merely cause vanishing gradients.,Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
771,introduction,['Robustness'],"We hope that this work will encourage both the adversarial and corruption robustness communities to work more closely together, since their goals seem to be so closely related. ",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
772,introduction,['Robustness'],"In particular, it is not common for adversarial defense methods to measure corruption robustness. ",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
773,introduction,"['Robustness', 'Successful', 'Improvement']","Given that successful adversarial defense methods should also improve some types of corruption robustness we recommend that future researchers consider evaluating corruption robustness in addition to adversarial robustness.
",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
776,conclusion,['Robustness'],1. The nearby errors we can find show up at the same distance scales we would expect from a linear model with the same corruption robustness.,Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
778,conclusion,"['Robustness', 'Improvement']","3. Finally, training procedures designed to improve adversarial robustness also improve many types of corruption robustness, and training on Gaussian noise moderately improves adversarial robustness.",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
779,conclusion,"['Generalization', 'Robustness', 'Beneficence']","In light of this, we believe it would be beneficial for the adversarial defense literature to start reporting generalization to distributional shift, such as the common corruption benchmark introduced in Hendrycks & Dietterich (2019), in addition to empirical estimates of adversarial robustness.",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
781,conclusion,"['Robustness', 'Performance', 'Improvement']","For example, we found that adversarial training significantly degraded performance on the fog and contrast corruptions despite improving small perturbation robustness.",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
782,conclusion,['Performance'],"In particular, performance on constrast-5 dropped to 55.3% accuracy vs 85.7% for the vanilla model (see Appendix B for more details).",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
783,conclusion,['Robustness'],"Second, measuring corruption robustness is significantly easier than measuring adversarial robustness — computing adversarial robustness perfectly requires solving an NPhard problem for every point in the test set (Katz et al., 2017). ",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
785,conclusion,['Robustness'],"To our knowledge, only one (Madry et al., 2017) has reported robustness numbers which were confirmed by a third party. ",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
786,conclusion,['Robustness'],We believe the difficulty of measuring robustness under the usual definition has contributed to this unproductive situation,Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
787,conclusion,"['Performance', 'Improvement']","Third, all of the failed defense strategies we examined also failed to improve performance in Gaussian noise. ",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
788,conclusion,"['Robustness', 'Scientific methodology', 'Improvement']","For this reason, we should be highly skeptical of defense strategies that only claim improved lp robustness but are unable to demonstrate robustness to distributional shift.",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
789,conclusion,['Improvement'],"Finally, if the goal is improving the security of our models in adversarial settings, errors on corrupted images already imply that our models are not secure. ",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
790,conclusion,['Robustness'],"Until our models are perfectly robust in the presence of average-case corruptions, they will not be robust in worst-case settings.",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
791,conclusion,"['Robustness', 'Unifying ideas or integrating components']",The communities of researchers studying adversarial and corruption robustness seem to be attacking essentially the same problem in two different ways. ,Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
792,conclusion,"['Robustness', 'Unifying ideas or integrating components']","We believe that the corruption robustness problem is also interesting independently of its connection to adversarial examples, and we hope that the results presented here will encourage more collaboration between these two communities.",Adversarial Examples Are a Natural Consequence of Test Error in Noise,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf
795,abstract,['Simplicity'],"In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers.",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
796,abstract,['Theoretical guarantees'],We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. ,Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
797,abstract,"['Simplicity', 'Quantitative evidence (e.g. experiments)', 'Accuracy']","Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. ",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
798,abstract,"['Performance', 'Scales up', 'Interpretable (to users)']","Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
799,introduction,['Efficiency'],"Graph Convolutional Networks (GCNs) (Kipf & Welling, 2017) are an efficient variant of Convolutional Neural Networks (CNNs) on graphs. GCNs stack layers of learned first-order spectral filters followed by a nonlinear activation function to learn graph representations. ",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
800,introduction,"['State-of-the-art', 'Applies to real world']","Recently, GCNs and subsequent variants have achieved state-of-the-art results in various application areas, including but not limited to citation networks (Kipf & Welling, 2017), social networks (Chen et al., 2018), applied chemistry (Liao et al., 2019), natural language processing (Yao et al., 2019; Han et al., 2012; Zhang et al., 2018c), and computer vision (Wang et al., 2018; Kampffmeyer et al., 2018).",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
801,introduction,['Simplicity'],"Historically, the development of machine learning algorithms has followed a clear trend from initial simplicity to need-driven complexity. ",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
803,introduction,['Simplicity'],"Similarly, simple pre-defined linear image filters (Sobel & Feldman, 1968; Harris & Stephens, 1988) eventually gave rise to nonlinear CNNs with learned convolutional kernels (Waibel et al., 1989; LeCun et al., 1989). ",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
804,introduction,"['Simplicity', 'Understanding (for researchers)']","As additional algorithmic complexity tends to complicate theoretical analysis and obfuscates understanding, it is typically only introduced for applications where simpler methods are insufficient. ",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
807,introduction,['Simplicity'],"GCNs are built upon multi-layer neural networks, and were never an extension of a simpler (insufficient) linear counterpart",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
809,introduction,"['Simplicity', 'Building on recent work']","Motivated by the glaring historic omission of a simpler predecessor, we aim to derive the simplest linear model that “could have” preceded the GCN, had a more “traditional” path been taken. ",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
810,introduction,['Simplicity'],We reduce the excess complexity of GCNs by repeatedly removing the nonlinearities between GCN layers and collapsing the resulting function into a single linear transformation. ,Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
811,introduction,"['Scientific methodology', 'Performance', 'Efficiency']",We empirically show that the final linear model exhibits comparable or even superior performance to GCNs on a variety of tasks while being computationally more efficient and fitting significantly fewer parameters. ,Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
812,introduction,['Simplicity'],We refer to this simplified linear model as Simple Graph Convolution (SGC).,Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
813,introduction,['Simplicity'],"In contrast to its nonlinear counterparts, the SGC is intuitively interpretable and we provide a theoretical analysis from the graph convolution perspective. ",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
815,introduction,"['Accuracy', 'Effectiveness', 'Improvement']","Kipf & Welling (2017) empirically observe that the “renormalization trick”, i.e. adding self-loops to the graph, improves accuracy, and we demon strate that this method effectively shrinks the graph spectral domain, resulting in a low-pass-type filter when applied to SGC. ",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
817,introduction,"['Performance', 'State-of-the-art']","Through an empirical assessment on node classification benchmark datasets for citation and social networks, we show that the SGC achieves comparable performance to GCN and other state-of-the-art graph neural networks. ",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
818,introduction,['Performance'],"However, it is significantly faster, and even outperforms FastGCN (Chen et al., 2018) by up to two orders of magnitude on the largest dataset (Reddit) in our evaluation. ",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
819,introduction,"['Effectiveness', 'Facilitating use (e.g. sharing code)']","Finally, we demonstrate that SGC extrapolates its effectiveness to a wide-range of downstream tasks. In particular, SGC rivals, if not surpasses, GCN-based approaches on text classification, user geolocation, relation extraction, and zero-shot image classification tasks. The code is available on Github1
",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
820,conclusion,"['Simplicity', 'Understanding (for researchers)']","In order to better understand and explain the mechanisms of GCNs, we explore the simplest possible formulation of a graph convolutional model, SGC. ",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
821,conclusion,['Simplicity'],"The algorithm is almost trivial, a graph based pre-processing step followed by standard multi-class logistic regression.",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
822,conclusion,"['Performance', 'State-of-the-art']","However, the performance of SGC rivals — if not surpasses — the performance of GCNs and state-of-the-art graph neural network models across a wide range of graph learning tasks. ",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
823,conclusion,"['Reduced training time', 'Data efficiency']","Moreover by precomputing the fixed feature extractor SK, training time is reduced to a record low. For example on the Reddit dataset, SGC can be trained up to two orders of magnitude faster than sampling-based GCN variants.
",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
827,conclusion,['Performance'],"Ultimately, the strong performance of SGC sheds light onto GCNs. It is likely that the expressive power of GCNs originates primarily from the repeated graph propagation (which SGC preserves) rather than the nonlinear feature extraction (which it doesn’t.)",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
828,conclusion,"['Simplicity', 'Performance', 'Efficiency', 'Interpretable (to users)', 'Beneficence']","Given its empirical performance, efficiency, and interpretability, we argue that the SGC should be highly beneficial to the community in at least three ways: (1) as a first model to try, especially for node classification tasks; (2) as a simple baseline for comparison with future graph learning models; (3) as a starting point for future research in graph learning — returning to the historic machine learning practice to develop complex from simple models",Simplifying Graph Convolutional Networks,https://arxiv.org/abs/1902.07153
829,abstract,"['Successful', 'Understanding (for researchers)']","Pre-training and fine-tuning, e.g., BERT (Devlin et al., 2018), have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. ",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
830,abstract,['Successful'],"Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for encoder-decoder based language generation.",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
833,abstract,"['Reduced training time', 'Improvement']","By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over baselines without pre-training or with other pretraining methods. ",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
834,abstract,"['Accuracy', 'State-of-the-art']","Specially, we achieve state-ofthe-art accuracy (37.5 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model (Bahdanau et al., 2015b)1.",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
836,introduction,['Large scale'],"For example, in computer vision, models are usually pre-trained on the large scale ImageNet dataset and then finetuned on downstream tasks like object detection (Szegedy et al., 2015; Ouyang et al., 2015) or image segmentation (Girshick et al., 2014). ",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
837,introduction,"['Accuracy', 'State-of-the-art', 'Understanding (for researchers)']","Recently, pre-training methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2018) have attracted a lot of attention in natural language processing, and achieved state-of-the-art accuracy in multiple language understanding tasks such as sentiment classification (Socher et al., 2013), natural language inference (Bowman et al., 2015), named entity recognition (Tjong Kim Sang & De Meulder, 2003) and SQuAD question answering (Rajpurkar et al., 2016), which usually have limited supervised data. ",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
838,introduction,['Large scale'],"Among the pre-training methods mentioned above, BERT is the most prominent one by pre-training the bidirectional encoder representations on a large monolingual corpus through masked language modeling and next sentence prediction.",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
839,introduction,['Applies to real world'],"Different from language understanding, language generation aims to generate natural language sentences conditioned on some inputs, including tasks like neural machine translation (NMT) (Cho et al., 2014; Bahdanau et al., 2015a; Vaswani et al., 2017), text summarization (Ayana et al., 2016; Suzuki & Nagata, 2017; Gehring et al., 2017) and conversational response generation (Shang et al., 2015; Vinyals & Le, 2015).",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
840,introduction,['Requires few resources'],"Language generation tasks are usually data-hungry, and many of them are low-resource or even zero-source in terms of training data. ",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
843,introduction,"['Novelty', 'Building on classic work']","In this paper, inspired by BERT, we propose a novel objective for pre-training: MAsked Sequence to Sequence learning (MASS) for language generation.",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
848,introduction,['Avoiding train/test discrepancy'],"MASS just needs to pre-train one model and then fine-tune on a variety of downstream tasks. We use transformer as the basic sequence to sequence model and pre-train on theWMT monolingual corpus2, and then fine-tune on three different language generation tasks including NMT, text summarization and conversational response generation. ",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
849,introduction,['Requires few resources'],"Considering the downstream tasks cover cross-lingual task like NMT, we pre-train one model on multiple languages. We explore the low-resource setting for all the three tasks, and also consider unsupervised NMT which is a purely zero-resource setting.",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
850,introduction,['Quantitative evidence (e.g. experiments)'],"For NMT, the experiments are conducted on WMT14 English-French, WMT16 English-German and WMT16 English-Romanian datasets.",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
852,introduction,"['Quantitative evidence (e.g. experiments)', 'Requires few resources']","For low-resource NMT, we finetune our model on limited bilingual data. For the other two tasks, we conduct experiments on: 1) the Gigaword corpus for abstractive text summarization; 2) the Cornell Movie Dialog corpus for conversational response generation. ",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
853,introduction,"['Effectiveness', 'Improvement', 'Requires few resources']","Our method achieves improvements on all these tasks as well as both the zero- and low-resource settings, demonstrating our method is effective and applicable to a wide range of sequence generation tasks.",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
854,introduction,"['Effectiveness', 'Improvement']","The contributions of this work are listed as follows: 1) We propose MASS, a masked sequence to sequence pre-training method for language generation; 2) We apply MASS on a variety of language generation tasks including NMT, text summarization and conversational response generation, and achieve significant improvements, demonstrating the effectiveness of our proposed method. ",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
855,introduction,"['Performance', 'State-of-the-art']","Specially, we achieve a state-of-the art BLEU score for unsupervised NMT on two language pairs: English-French and English-German, and outperform the previous unsupervised NMT method (Lample & Conneau, 2019) by more than 4 points on English-French and 1 point on French-English in terms of BLEU score, and even beating the early attention-based supervised model (Bahdanau et al., 2015b).",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
858,conclusion,"['Quantitative evidence (e.g. experiments)', 'Reduced training time', 'Improvement']","Through experiments on the three above tasks and total eight datasets, MASS achieved significant improvements over the baseline without pre-training or with other pre-training methods. ",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
859,conclusion,"['Performance', 'State-of-the-art']","More specifically, MASS achieved the state-of-the-art BLEU scores for unsupervised NMT on three language pairs, outperforming the previous state-ofthe-art by more than 4 BLEU points on English-French. ",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
860,conclusion,['Flexibility/Extensibility'],"For future work, we will apply MASS to more language generation tasks such as sentence paraphrasing, text style transfer and post editing, as well as other sequence generation tasks (Ren et al., 2019). ",MASS: Masked Sequence to Sequence Pre-training for Language Generation,https://arxiv.org/abs/1905.02450
862,abstract,['Generality'],This paper studies the general problem of learning kernels based on a polynomial combination of base kernels. ,Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
864,abstract,['Simplicity'],"We examine the corresponding learning kernel optimization problem, show how that minimax problem can be reduced to a simpler minimization problem, and prove that the global solution of this problem always lies on the boundary. ",Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
866,abstract,"['Quantitative evidence (e.g. experiments)', 'Effectiveness']","Finally, we report the results of extensive experiments with this algorithm using several publicly available datasets demonstrating the effectiveness of our technique.",Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
867,introduction,['Successful'],"Learning algorithms based on kernels have been used with much success in a variety of tasks [17,19].",Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
868,introduction,"['Generality', 'Beneficence']","Classification algorithms such as support vector machines (SVMs) [6, 10], regression algorithms, e.g., kernel ridge regression and support vector regression (SVR) [16,22], and general dimensionality reduction algorithms such as kernel PCA (KPCA) [18] all benefit from kernel methods.",Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
871,introduction,"['Performance', 'Improvement']","However, in the typical use of these kernel method algorithms, the choice of the PDS kernel, which is crucial to improved performance, is left to the user. ",Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
873,introduction,"['Efficiency', 'Building on recent work']","There is a large recent body of literature addressing various aspects of this problem, including deriving efficient solutions to the optimization problems it generates and providing a better theoretical analysis of the problem both in classification and regression [1, 8, 9, 11, 13, 15, 21]. ",Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
874,introduction,['Generality'],"With the exception of a few publications considering infinite-dimensional kernel families such as hyperkernels [14] or general convex classes of kernels [2], the great majority of analyses and algorithmic results focus on learning finite linear combinations of base kernels as originally considered by [12]. ",Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
875,introduction,"['Efficiency', 'Understanding (for researchers)', 'Improvement', 'Progress']","However, despite the substantial progress made in the theoretical understanding and the design of efficient algorithms for the problem of learning such linear combinations of kernels, no method seems to reliably give improvements over baseline methods. ",Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
876,introduction,['Performance'],"For example, the learned linear combination does not consistently outperform either the uniform combination of base kernels or simply the best single base kernel (see, for example, UCI dataset experiments in [9, 12], see also NIPS 2008 workshop).",Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
877,introduction,"['Performance', 'Improvement']",This suggests exploring other non-linear families of kernels to obtain consistent and significant performance improvements.,Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
878,introduction,"['Quantitative evidence (e.g. experiments)', 'Generality', 'Performance', 'Improvement']","Non-linear combinations of kernels have been recently considered by [23]. However, here too, experimental results have not demonstrated a consistent performance improvement for the general learning task. ",Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
879,introduction,['Efficiency'],"Another method, hierarchical multiple learning [3], considers learning a linear combination of an exponential number of linear kernels, which can be efficiently represented as a product of sums. ",Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
882,introduction,"['Performance', 'Efficiency', 'Effectiveness', 'Improvement']",For this approach the authors provide an effective and efficient algorithm and some performance improvement is actually observed for regression problems in very high dimensions.,Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
862,introduction,['Generality'],This paper studies the general problem of learning kernels based on a polynomial combination of base kernels. ,Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
884,introduction,"['Simplicity', 'Scientific methodology']",We show how to simplify its optimization problem from a minimax problem to a simpler minimization problem and prove that the global solution of the optimization problem always lies on the boundary. ,Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
866,introduction,"['Quantitative evidence (e.g. experiments)', 'Effectiveness']","Finally, we report the results of extensive experiments with this algorithm using several publicly available datasets demonstrating the effectiveness of our technique.",Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
889,introduction,['Performance'],"In Section 4, we study the performance of our algorithm for learning nonlinear combinations of kernels in regression (NKRR) on several publicly available datasets.",Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
891,conclusion,['Performance'],This extends learning kernel ideas and helps explore kernel combinations leading to better performance. ,Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
892,conclusion,['Generalization'],We proved that the global solution of the optimization problem always lies on the boundary and gave a simple projection-based gradient descent algorithm shown empirically to converge in few iterations. ,Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
893,conclusion,['Scientific methodology'],We also gave a necessary and sufficient condition for that algorithm to converge to a global optimum. ,Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
894,conclusion,"['Quantitative evidence (e.g. experiments)', 'Beneficence']","Finally, we reported the results of several experiments on publicly available datasets demonstrating the benefits of learning polynomial combinations of kernels. ",Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
896,conclusion,"['Performance', 'Improvement']",We hope that the performance improvements reported will further motivate such analyses.,Learning Non-Linear Combinations of Kernels,http://papers.nips.cc/paper/3692-learning-non-linear-combinations-of-kernels.pdf
899,abstract,['Parallelizability / distributed'],"In this paper we prove that online learning with delayed updates converges well, thereby facilitating parallel online learning.",Slow Learners are Fast,http://papers.nips.cc/paper/3888-slow-learners-are-fast
900,introduction,['Large scale'],Online learning has become the paradigm of choice for tackling very large scale estimation problems. ,Slow Learners are Fast,http://papers.nips.cc/paper/3888-slow-learners-are-fast
906,introduction,['Parallelizability / distributed'],"In other words, the algorithms are entirely sequential in their nature.",Slow Learners are Fast,http://papers.nips.cc/paper/3888-slow-learners-are-fast
908,introduction,['Identifying limitations'],It is therefore very wasteful if only one of these cores is actually used for estimation.,Slow Learners are Fast,http://papers.nips.cc/paper/3888-slow-learners-are-fast
909,introduction,['Identifying limitations'],A second problem arises from the fact that network and disk I/O have not been able to keep up with the increase in processor speed. ,Slow Learners are Fast,http://papers.nips.cc/paper/3888-slow-learners-are-fast
911,introduction,['Memory efficiency'],"This means that current algorithms reach their limit at problems of size 1TB whenever the algorithm is I/O bound (this amounts to a training time of 3 hours), or even smaller problems whenever the model parametrization makes the algorithm CPU bound.",Slow Learners are Fast,http://papers.nips.cc/paper/3888-slow-learners-are-fast
912,introduction,['Parallelizability / distributed'],"Finally, distributed and cloud computing are unsuitable for today’s online learning algorithms. ",Slow Learners are Fast,http://papers.nips.cc/paper/3888-slow-learners-are-fast
914,introduction,"['Novelty', 'Theoretical guarantees', 'Quantitative evidence (e.g. experiments)']","We propose two variants. To our knowledge, this is the first paper which provides theoretical guarantees combined with empirical evidence for such an algorithm. ",Slow Learners are Fast,http://papers.nips.cc/paper/3888-slow-learners-are-fast
915,introduction,"['Theoretical guarantees', 'Quantitative evidence (e.g. experiments)']","Previous work, e.g. by [6] proved rather inconclusive in terms of theoretical and empirical guarantees. ",Slow Learners are Fast,http://papers.nips.cc/paper/3888-slow-learners-are-fast
916,introduction,['Performance'],"In a nutshell, we propose the following two variants: several processing cores perform stochastic gradient descent independently of each other while sharing a common parameter vector which is updated asynchronously. ",Slow Learners are Fast,http://papers.nips.cc/paper/3888-slow-learners-are-fast
919,introduction,['Unifying ideas or integrating components'],Subsequently the results are combined and the combination is then used for a descent step.,Slow Learners are Fast,http://papers.nips.cc/paper/3888-slow-learners-are-fast
920,introduction,['Unifying ideas or integrating components'],"A common feature of both algorithms is that the update occurs with some delay: in the first case other cores may have updated the parameter vector in the meantime, in the second case, other cores may have already computed parts of the function for the subsequent examples before an update.",Slow Learners are Fast,http://papers.nips.cc/paper/3888-slow-learners-are-fast
922,summary and discussion,"['Novelty', 'Theoretical guarantees']","In this paper, we have shown theoretically how independence between examples can make the actual effect much smaller. ",Slow Learners are Fast,http://papers.nips.cc/paper/3888-slow-learners-are-fast
923,summary and discussion,"['Quantitative evidence (e.g. experiments)', 'Large scale', 'Efficiency']","The experimental results showed three important aspects: first of all, small simulated delayed updates do not hurt much, and in harder problems they hurt less; secondly, in practice it is hard to speed up “easy” problems with a small amount of computation, such as e-mails with linear features; finally, when examples are larger or harder, the speedups can be quite dramatic.",Slow Learners are Fast,http://papers.nips.cc/paper/3888-slow-learners-are-fast
925,abstract,['Automatic'],"We use this to motivate the β-TCVAE (Total Correlation Variational Autoencoder) algorithm, a refinement and plug-in replacement of the β-VAE for learning disentangled representations, requiring no additional hyperparameters during training. ",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
926,abstract,['Principled'],We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG).,Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
927,abstract,['Performance'],"We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the model is trained using our framework.",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
929,introduction,"['Generality', 'Interpretable (to users)']",Disentangled variables are generally considered to contain interpretable semantic information and reflect separate factors of variation in the data. ,Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
931,introduction,['Useful'],"Such representations distill information into a compact form which is oftentimes semantically meaningful and useful for a variety of tasks [2, 4]. ",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
932,introduction,"['Generalization', 'Robustness']","For instance, it is found that such representations are more generalizable and robust against adversarial attacks [5].",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
933,introduction,['State-of-the-art'],Many state-of-the-art methods for learning disentangled representations are based on re-weighting parts of an existing objective.,Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
934,introduction,['Interpretable (to users)'],"For instance, it is claimed that mutual information between latent variables and the observed data can encourage the latents into becoming more interpretable [6]. ",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
936,introduction,['Scientific methodology'],"However, there is no strong evidence linking factorial representations to disentanglement.",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
937,introduction,['Qualitative evidence (e.g. examples)'],"In part, this can be attributed to weak qualitative evaluation procedures. ",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
938,introduction,"['Quantitative evidence (e.g. experiments)', 'Qualitative evidence (e.g. examples)']","While traversals in the latent representation can qualitatively illustrate disentanglement, quantitative measures of disentanglement are in their infancy.",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
939,introduction,['Successful'],"In this paper, we: • show a decomposition of the variational lower bound that can be used to explain the success of the β-VAE [7] in learning disentangled representations.",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
940,introduction,['Simplicity'],• propose a simple method based on weighted minibatches to stochastically train with arbitrary weights on the terms of our decomposition without any additional hyperparameters.,Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
941,introduction,"['Robustness', 'Scientific methodology', 'Interpretable (to users)']","• introduce the β-TCVAE, which can be used as a plug-in replacement for the β-VAE with no extra hyperparameters. Empirical evaluations suggest that the β-TCVAE discovers more interpretable representations than existing methods, while also being fairly robust to random initialization.",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
942,introduction,"['Novelty', 'Generalization', 'Parallelizability / distributed']","• propose a new information-theoretic disentanglement metric, which is classifier-free and generalizable to arbitrarily-distributed and non-scalar latent variables.",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
943,introduction,['Building on recent work'],"While Kim & Mnih [8] have independently proposed augmenting VAEs with an equivalent total correlation penalty to the β-TCVAE, their proposed training method differs from ours and requires an auxiliary discriminator network.",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
944,conclusion,"['Automatic', 'Explicability']",We present a decomposition of the ELBO with the goal of explaining why β-VAE works. ,Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
946,conclusion,['Automatic'],"We then designate a special case as β-TCVAE, which can be trained stochastically using minibatch estimator with no additional hyperparameters compared to the β-VAE.",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
947,conclusion,"['Simplicity', 'Unifying ideas or integrating components', 'Easy to implement']",The simplicity of our method allows easy integration into different frameworks [44].,Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
948,conclusion,['Qualitative evidence (e.g. examples)'],"To quantitatively evaluate our approach, we propose a classifier-free disentanglement metric called MIG. ",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
949,conclusion,"['Efficiency', 'Beneficence']",This metric benefits from advances in efficient computation of mutual information [23] and enforces compactness in addition to disentanglement. ,Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
950,conclusion,['Scientific methodology'],"Unsupervised learning of disentangled representations is inherently a difficult problem due to the lack of a prior for semantic awareness, but we show some evidence in simple datasets with uniform factors that independence between latent variables can be strongly related to disentanglement.",Isolating Sources of Disentanglement in VAEs,http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders 
951,abstract,['Novelty'],We introduce a new family of deep neural network models.,Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
954,abstract,"['Preciseness', 'Memory efficiency']","These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. ",Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
957,abstract,"['Large scale', 'Scales up']","For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.",Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
960,introduction,['Formal description/analysis'],"What happens as we add more layers and take smaller steps? In the limit, we parameterize the continuous dynamics of hidden units using an ordinary differential equation (ODE) specified by a neural network: dh(t) dt = f(h(t), t, θ)",Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
961,introduction,['Formal description/analysis'],"Starting from the input layer h(0), we can define the output layer h(T) to be the solution to this ODE initial value problem at some time T. ",Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
962,introduction,['Accuracy'],"This value can be computed by a black-box differential equation solver, which evaluates the hidden unit dynamics f wherever necessary to determine the solution with the desired accuracy. ",Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
964,introduction,['Memory efficiency'],"Memory efficiency In Section 2, we show how to compute gradients of a scalar-valued loss with respect to all inputs of any ODE solver, without backpropagating through the operations of the solver.",Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
966,introduction,"['Simplicity', 'Building on classic work']",Adaptive computation Euler’s method is perhaps the simplest method for solving ODEs. ,Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
967,introduction,"['Accuracy', 'Efficiency', 'Building on classic work']","There have since been more than 120 years of development of efficient and accurate ODE solvers (Runge, 1895; Kutta, 1901; Hairer et al., 1987). ",Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
968,introduction,"['Flexibility/Extensibility', 'Approximation', 'Accuracy']","Modern ODE solvers provide guarantees about the growth of approximation error, monitor the level of error, and adapt their evaluation strategy on the fly to achieve the requested level of accuracy. ",Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
969,introduction,"['Realistic output', 'Accuracy', 'Scales up']","This allows the cost of evaluating a model to scale with problem complexity. After training, accuracy can be reduced for real-time or low-power applications.",Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
970,introduction,"['Automatic', 'Efficiency']","Parameter efficiency When the hidden unit dynamics are parameterized as a continuous function of time, the parameters of nearby “layers” are automatically tied together. ",Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
972,introduction,['Scales up'],Scalable and invertible normalizing flows An unexpected side-benefit of continuous transformations is that the change of variables formula becomes easier to compute. ,Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
976,conclusion,"['Controllability (of model owner)', 'Accuracy']","These models are evaluated adaptively, and allow explicit control of the tradeoff between computation speed and accuracy.",Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
977,conclusion,['Scales up'],"Finally, we derived an instantaneous version of the change of variables formula, and developed continuous-time normalizing flows, which can scale to large layer sizes.",Neural Ordinary Differential Equations,http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations
978,abstract,['Parallelizability / distributed'],"Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. ",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
979,abstract,"['Simplicity', 'Quantitative evidence (e.g. experiments)']","In this paper we propose Glow, a simple type of generative flow using an invertible 1 × 1 convolution. ",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
980,abstract,['Improvement'],Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks.,Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
981,abstract,"['Large scale', 'Efficiency', 'Optimal ']","Perhaps most strikingly, we demonstrate that a flow-based generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. ",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
983,introduction,"['Generalization', 'Robustness', 'Data efficiency', 'Learning from humans']","Two major unsolved problems in the field of machine learning are (1) data-efficiency: the ability to learn from few datapoints, like humans; and (2) generalization: robustness to changes of the task or its context. ",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
984,introduction,"['Generalization', 'Qualitative evidence (e.g. examples)']","AI systems, for example, often do not work at all when given inputs that are different from their training distribution. ",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
985,introduction,"['Generalization', 'Realistic output', 'Promising', 'Label efficiency (reduced need for labeled data)']","A promise of generative models, a major branch of machine learning, is to overcome these limitations by: (1) learning realistic world models, potentially allowing agents to plan in a world model before actual interaction with the world, and (2) learning meaningful features of the input while requiring little or no human supervision or labeling. ",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
986,introduction,"['Robustness', 'Large scale', 'Data efficiency']","Since such features can be learned from large unlabeled datasets and are not necessarily task-specific, downstream solutions based on those features could potentially be more robust and more data efficient.",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
987,introduction,"['State-of-the-art', 'Improvement']","In this paper we work towards this ultimate vision, in addition to intermediate applications, by aiming to improve upon the state-of-the-art of generative models.",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
988,introduction,['Generality'],"Generative modeling is generally concerned with the extremely challenging task of modeling all dependencies within very high-dimensional input data, usually specified in the form of a full joint probability distribution. ",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
989,introduction,['Accuracy'],"Since such joint models potentially capture all patterns that are present in the data, the applications of accurate generative models are near endless. ",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
992,introduction,"['Simplicity', 'Large scale', 'Parallelizability / distributed']","1. Autoregressive models (Hochreiter and Schmidhuber, 1997; Graves, 2013; van den Oord et al., 2016a,b; Van Den Oord et al., 2016). Those have the advantage of simplicity, but have as disadvantage that synthesis has limited parallelizability, since the computational length of synthesis is proportional to the dimensionality of the data; this is especially troublesome for large images or video.",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
993,introduction,['Optimal '],"2. Variational autoencoders (VAEs) (Kingma and Welling, 2013, 2018), which optimize a lower bound on the log-likelihood of the data. ",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
994,introduction,"['Parallelizability / distributed', 'Optimal ']","Variational autoencoders have the advantage of parallelizability of training and synthesis, but can be comparatively challenging to optimize (Kingma et al., 2016).
",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
995,introduction,['Building on recent work'],"3. Flow-based generative models, first described in NICE (Dinh et al., 2014) and extended in RealNVP (Dinh et al., 2016). We explain the key ideas behind this class of model in the following sections.
",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
997,introduction,"['Approximation', 'Exactness']","Exact latent-variable inference and log-likelihood evaluation. In VAEs, one is able to infer only approximately the value of the latent variables that correspond to a datapoint. ",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
998,introduction,"['Approximation', 'Exactness']","GAN’s have no encoder at all to infer the latents. In reversible generative models, this can be done exactly without approximation. ",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
999,introduction,"['Exactness', 'Accuracy', 'Optimal ']","Not only does this lead to accurate inference, it also enables optimization of the exact log-likelihood of the data, instead of a lower bound of it.",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
1000,introduction,"['Efficiency', 'Parallelizability / distributed']","Efficient inference and efficient synthesis. Autoregressive models, such as the PixelCNN (van den Oord et al., 2016b), are also reversible, however synthesis from such models is difficult to parallelize, and typically inefficient on parallel hardware. ",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
1001,introduction,"['Efficiency', 'Parallelizability / distributed']",Flow-based generative models like Glow (and RealNVP) are efficient to parallelize for both inference and synthesis.,Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
1002,introduction,"['Quantitative evidence (e.g. experiments)', 'Performance', 'Useful']","Useful latent space for downstream tasks. The hidden layers of autoregressive models have unknown marginal distributions, making it much more difficult to perform valid manipulation of data. ",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
1005,introduction,['Memory efficiency'],"Significant potential for memory savings. Computing gradients in reversible neural networks requires an amount of memory that is constant instead of linear in their depth, as explained in the RevNet paper (Gomez et al., 2017).",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
1006,introduction,['Novelty'],"In this paper we propose a new a generative flow coined Glow, with various new elements as described in Section 3. ",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
1007,introduction,"['Quantitative evidence (e.g. experiments)', 'Qualitative evidence (e.g. examples)']","In Section 5, we compare our model quantitatively with previous flows, and in Section 6, we study the qualitative aspects of our model on high-resolution datasets.",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
1008,conclusion,"['Novelty', 'Performance', 'Improvement']",We propose a new type of generative flow and demonstrate improved quantitative performance in terms of log-likelihood on standard image modeling benchmarks. ,Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
1009,conclusion,"['Generalization', 'Flexibility/Extensibility', 'Realistic output']","In addition, we demonstrate that when trained on high-resolution faces, our model is able to synthesize realistic images.",Glow: Generative Flow with Invertible 1×1 Convolutions,http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions
1010,abstract,['Progress'],Recent progress in natural language generation has raised dual-use concerns. ,Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1013,abstract,"['Robustness', 'Controllability (of model owner)']","Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover.",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1015,abstract,['Robustness'],Developing robust verification techniques against generators like Grover is critical.,Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1016,abstract,['Accuracy'],"We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1017,abstract,['Accuracy'],"Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1023,introduction,"['Large scale', 'Progress']","To the best of our knowledge, most disinformation online today is manually written (Vargo et al., 2018). However, as progress continues in natural language generation, malicious actors will increasingly be able to controllably generate realistic-looking propaganda at scale. ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1024,introduction,['Progress'],"Thus, while we are excited about recent progress in text generation (Józefowicz et al., 2016; Radford et al., 2018; 2019), we are also concerned with the inevitability of AI-generated ‘neural’ fake news.",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1025,introduction,"['Large scale', 'Understanding (for researchers)']","With this paper, we seek to understand and respond to neural fake news before it manifests at scale.",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1026,introduction,['Robustness'],"We draw on the field of computer security, which relies on threat modeling: analyzing the space of potential threats and vulnerabilities in a system to develop robust defenses. ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1027,introduction,['Scientific methodology'],"To scientifically study the risks of neural disinformation, we present a new generative model called Grover.",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1028,introduction,['Efficiency'],"Our model allows for controllable yet effcient generation of an entire news article – not just the body, but also the title, news source, publication date, and author lis",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1029,introduction,"['Controllability (of model owner)', 'Human-like mechanism']","This lets us study an adversary with controllable generations (e.g. Figure 1, an example anti-vaccine article written in the style of the New York Times).",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1030,introduction,['Robustness'],"Humans rate the disinformation generated by Grover as trustworthy, even more so than humanwritten disinformation. Thus, developing robust verification techniques against generators such as Grover is an important research area. ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1032,introduction,['Accuracy'],"In this setting, the best existing fake news discriminators are, themselves, deep pretrained language models (73% accuracy) (Peters et al., 2018; Radford et al., 2018; 2019; Devlin et al., 2018). However, we find that Grover, when used in a discriminative setting, performs even better at 92% accuracy. ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1036,introduction,['Understanding (for researchers)'],"We conclude with a sketch of the ethical territory that must be mapped out in order to understand our responsibilities as researchers when studying fake news, and the potential negative implications of releasing models (Hecht et al., 2018; Zellers, 2019; Solaiman et al., 2019). ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1038,introduction,['Concreteness'],We believe our proposed framework and accompanying models provide a concrete initial proposal for an evolving conversation about ML-based disinformation threats and how they can be countered,Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1040,conclusion,['Applies to real world'],Our sketch of what these threats might look like – a controllable language model named Grover – suggests that these threats are real and dangerous. ,Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1041,conclusion,['Deferral to humans'],"Grover can rewrite propaganda articles, with humans rating the rewritten versions as more trustworthy. At the same time, there are defenses to these models – notably, in the form of Grover itself. ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1043,conclusion,['Easy to implement'],"The Era of Neural Disinformation. Though training Grover was challenging, it is easily achievable by real-world adversaries today. ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1044,conclusion,['Parallelizability / distributed'],Obtaining the data required through Common Crawl cost $10k in AWS credits and can be massively parallelized over many CPUs. ,Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1045,conclusion,['Low cost'],"Training Grover-Mega is relatively inexpensive: at a cost of $0.30 per TPU v3 core-hour and two weeks of training, the total cost is $25k. ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1047,conclusion,['Effectiveness'],"Release of generators is critical. At first, it would seem like keeping models like Grover private would make us safer. However, Grover serves as an e↵ective detector of neural fake news, even when the generator is much larger (Section 5). ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1049,conclusion,['Progress'],"Future of progress in generation. Models like BERT are strong discriminators for many NLP tasks, but they are not as good at detecting Grover’s generations as left-to-right models like Grover, even after domain adaptation. ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1050,conclusion,['Progress'],"One hypothesis is that the artifacts shown in Section 6 are most visible to a left-to-right discriminator. This also suggests that recent progress on generating text in any order (Gu et al., 2019; Stern et al., 2019; Ghazvininejad et al., 2019) may lead to models that evade a Grover discriminator. ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1051,conclusion,['Performance'],"Likewise, models that are trained conditioned on their own predictions might avoid exposure bias, however, these objectives often lead to low performance on language tasks (Caccia et al., 2018). One additional possibility is the use of Adversarial Filtering (Zellers et al., 2018; 2019b) to oversample and then select a subset of generations. ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1052,conclusion,['Identifying limitations'],"However, we found this didn’t work well for very long sequences (up to 1024 BPE tokens), possibly as these are far from the ‘Goldilocks Zone’ wherein discrimination is hard for machines.",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1058,conclusion,"['Effectiveness', 'Parallelizability / distributed']","Future of progress in discrimination. Our discriminators are e↵ective, but they primarily leverage distributional features rather than evidence. ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1059,conclusion,['Deferral to humans'],"In contrast, humans assess whether an article is truthful by relying on a model of the world, assessing whether the evidence in the article matches that model. ",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1060,conclusion,"['Progress', 'Scales up']","Future work should investigate integrating knowledge into the discriminator (e.g. for claim verification in FEVER; Thorne et al., 2018). An open question is to scale progress in this task towards entire news articles, and without paired evidence (similar to open-domain QA; Chen et al., 2017).",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1063,conclusion,['Deferral to humans'],"However, humans must still be in the loop due to dangers of flagging real news as machine-generated, and possible unwanted social biases of these models.",Defending Against Neural Fake News,http://papers.nips.cc/paper/9106-defending-against-neural-fake-news
1064,abstract,['Building on classic work'],"We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1065,abstract,['Generalization'],"By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1066,abstract,['Accuracy'],We evaluate a broad range of models and find accuracy drops of 3% – 15% on CIFAR-10 and 11% – 14% on ImageNet,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1067,abstract,['Accuracy'],"However, accuracy gains on the original test sets translate to larger gains on the new test sets.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1068,abstract,"['Generalization', 'Accuracy']","Our results suggest that the accuracy drops are not caused by adaptivity, but by the models’ inability to generalize to slightly “harder” images than those found in the original test sets. ",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1069,introduction,['Generalization'],The overarching goal of machine learning is to produce models that generalize.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1070,introduction,"['Generalization', 'Performance']",We usually quantify generalization by measuring the performance of a model on a held-out test set.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1071,introduction,['Performance'],What does good performance on the test set then imply?,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1072,introduction,['Performance'],"At the very least, one would hope that the model also performs well on a new test set assembled from the same data source by following the same data cleaning protocol.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1073,introduction,['Reproducibility'],"In this paper, we realize this thought experiment by replicating the dataset creation process for two prominent benchmarks, CIFAR-10 and ImageNet",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1074,introduction,['Accuracy'],"In contrast to the ideal outcome, we find that a wide range of classification models fail to reach their original accuracy scores.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1075,introduction,['Accuracy'],The accuracy drops range from 3% to 15% on CIFAR-10 and 11% to 14% on ImageNet.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1076,introduction,"['Approximation', 'Accuracy']","On ImageNet, the accuracy loss amounts to approximately five years of progress in a highly active period of machine learning research.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1078,introduction,['Quantitative evidence (e.g. experiments)'],"However, our experiments show that the relative order of models is almost exactly preserved on our new test sets:",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1079,introduction,['Accuracy'],the models with highest accuracy on the original test sets are still the models with highest accuracy on the new test sets.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1080,introduction,['Accuracy'],"Moreover, there are no diminishing returns in accuracy",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1081,introduction,"['Accuracy', 'Improvement']","In fact, every percentage point of accuracy improvement on the original test set",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1082,introduction,['Improvement'],test set translates to a larger improvement on our new test sets.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1083,introduction,['Accuracy'],"So although later models could have been adapted more to the test set, they see smaller drops in accuracy.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1084,introduction,"['Quantitative evidence (e.g. experiments)', 'Effectiveness', 'Improvement']",These results provide evidence that exhaustive test set evaluations are an effective way to improve image classification models.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1085,introduction,['Accuracy'],Adaptivity is therefore an unlikely explanation for the accuracy drops.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1087,introduction,"['Approximation', 'Exactness', 'Accuracy']",We demonstrate that it is possible to recover the original ImageNet accuracies almost exactly if we only include the easiest images from our candidate pool.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1088,introduction,['Accuracy'],This suggests that the accuracy scores of even the best image classifiers are still highly sensitive to minutiae of the data cleaning process. ,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1089,introduction,"['Generalization', 'Quantitative evidence (e.g. experiments)', 'Reproducibility']","This brittleness puts claims about human-level performance into context [20, 31, 48]. It also shows that current classifiers still do not generalize reliably even in the benign environment of a carefully controlled reproducibility experiment.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1090,introduction,['Quantitative evidence (e.g. experiments)'],"Figure 1 shows the main result of our experiment. Before we describe our methodology in Section 3, the next section provides relevant background. To enable future research, we release both our new test sets and the corresponding code.1
",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1091,introduction,['Reproducibility'],Ideal reproducibility,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1092,introduction,['Accuracy'],Model accuracy,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1093,introduction,['Accuracy'],Figure 1: Model accuracy on the original test sets vs. our new test sets. Each data point corresponds to one model in our testbed (shown with 95% Clopper-Pearson confidence intervals). The plots reveal two main phenomena:,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1094,introduction,['Accuracy'],(i) There is a significant drop in accuracy from the original to the new test sets,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1095,introduction,['Accuracy'],(ii) The model accuracies closely follow a linear function with slope greater than 1 (1:7 for CIFAR-10 and 1:1 for ImageNet,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1096,introduction,['Progress'],This means that every percentage point of progress on the original test set translates into more than one percentage point on the new test set.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1100,conclusion,['Performance'],"In order to use machine learning in these areas responsibly, it is important that we can both train models with sufficient generalization abilities, and also reliably measure their performance.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1102,conclusion,"['Quantitative evidence (e.g. experiments)', 'Promising']","Our experiments are only one step in addressing this reliability challenge. There are multiple promising avenues for future work:
",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1103,conclusion,"['Quantitative evidence (e.g. experiments)', 'Understanding (for researchers)']","Understanding Adaptive Overfitting. In contrast to conventional wisdom, our experiments show that there are no diminishing returns associated with test set re-use on CIFAR-10 and ImageNet.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1104,conclusion,['Understanding (for researchers)'],A more nuanced understanding of this phenomenon will require studying whether other machine learning problems are also resilient to adaptive overfitting.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1105,conclusion,"['Quantitative evidence (e.g. experiments)', 'Reproducibility']","For instance, one direction would be to conduct similar reproducibility experiments on tasks in natural language processing, or to analyze data from competition platforms such as Kaggle and CodaLab.7
",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1106,conclusion,"['Quantitative evidence (e.g. experiments)', 'Performance', 'Learning from humans']",Characterizing the Distribution Gap. Why do the classification models in our testbed perform worse on the new test sets? The selection frequency experiments in Section 4 suggest that images selected less frequently by the MTurk workers are harder for the models.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1108,conclusion,['Robustness'],Learning More Robust Models.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1109,conclusion,['Robustness'],An overarching goal is to make classification models more robust to small variations in the data.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1110,conclusion,['Accuracy'],"If the change from the original to our new test sets can be characterized accurately, ",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1111,conclusion,"['Robustness', 'Accuracy']",techniques such as data augmentation or robust optimization may be able to close some of the accuracy gap.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1113,conclusion,['Accuracy'],Measuring Human Accuracy. One interesting question is whether our new test sets are also harder for humans,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1114,conclusion,"['Quantitative evidence (e.g. experiments)', 'Accuracy']","As a first step in this direction, our human accuracy experiment on CIFAR10 (see Appendix B.2.5) shows that average human performance is not affected significantly by the distribution shift between the original and new images that are most difficult for the models.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1116,conclusion,"['Quantitative evidence (e.g. experiments)', 'Accuracy', 'Building on classic work', 'Understanding (for researchers)']",But a more comprehensive understanding of the human baseline will require additional human accuracy experiments on both CIFAR-10 and ImageNet. ,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1117,conclusion,['Performance'],Building Further Test Sets. The dominant paradigm in machine learning is to evaluate the performance of a classification model on a single test set per benchmark,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1118,conclusion,['Quantitative evidence (e.g. experiments)'],Our results suggest that this is not comprehensive enough to characterize the reliability of current models.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1119,conclusion,"['Generalization', 'Accuracy']","To understand their generalization abilities more accurately, new test data from various sources may be needed.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1120,conclusion,['Accuracy'],One intriguing question here is whether accuracy on other test sets will also follow,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1121,conclusion,['Accuracy'],linear function of the original test accuracy.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1123,conclusion,['Reproducibility'],Code release. It is hard to fully document the dataset creation process in a paper because it involves a long list of design choices. Hence it would be beneficial for reproducibility efforts if future dataset papers released not only the data but also all code used to create the datasets.,Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1126,conclusion,"['Qualitative evidence (e.g. examples)', 'Accuracy']","“Super hold-out”. Having access to data from the original CIFAR-10 and ImageNet data collection could have clarified the cause of the accuracy drops in our experiments. By keeping an additional test set hidden for multiple years, future benchmarks could explicitly test for adaptive overfitting after a certain time period.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1129,conclusion,['Performance'],"Moreover, the large number of classes causes difficulties when characterizing human performance.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1134,conclusion,"['Accuracy', 'State-of-the-art']","Both datasets were assembled in the late 2000s for an accuracy regime that is very different from the state-of-the-art now. Over the past decade, especially ImageNet has successfully guided the field to increasingly better models, thereby clearly demonstrating the immense value of
this dataset. ",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1135,conclusion,['Accuracy'],"But as models have increased in accuracy and our reliability expectations have grown accordingly, it is now time to revisit how we create and utilize datasets in machine learning.",Do ImageNet Classifiers Generalize to ImageNet?,http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf
1136,abstract,"['Large scale', 'Promising', 'Data efficiency']","The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1137,abstract,"['Building on recent work', 'Used in practice/Popular']","We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton & Salakhutdinov, 2006; Raina et al., 2007). ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1138,abstract,['Large scale'],"Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1139,abstract,"['Large scale', 'Parallelizability / distributed']","In this paper, we suggest massively parallel methods to help resolve these problems. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1141,abstract,"['Large scale', 'Generality', 'Parallelizability / distributed']",We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. ,Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1142,abstract,"['Successful', 'Scales up']",We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. ,Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1143,abstract,"['Large scale', 'Fast']",Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. ,Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1144,abstract,"['Qualitative evidence (e.g. examples)', 'Reduced training time', 'Fast']","For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1145,abstract,"['Simplicity', 'Parallelizability / distributed', 'Fast']","For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1146,introduction,['Building on classic work'],"We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that can learn hierarchical representations of their input (Olshausen & Field, 1996; Hinton & Salakhutdinov, 2006). ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1147,introduction,"['Efficiency', 'Applies to real world']","With the invention of increasingly efficient learning algorithms over the past decade, these models have been applied to a number of machine learning applications, including computer vision, text modeling and collaborative filtering, among others. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1149,introduction,"['Large scale', 'Label efficiency (reduced need for labeled data)']","When applied to images, these models can easily have tens of millions of free parameters, and ideally, we would want to use millions of unlabeled training examples to richly cover the input space. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1151,introduction,['Generality'],"Partly due to such daunting computational requirements, typical applications of DBNs and sparse coding considered in the literature generally contain many fewer free parameters (e.g., see Table 1), or are trained on a fraction of the available input examples.",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1152,introduction,"['Large scale', 'Identifying limitations']","In our view, if the goal is to deploy better machine learning applications, the difficulty of learning large models is a severe limitation. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1153,introduction,"['Simplicity', 'Qualitative evidence (e.g. examples)', 'Large scale', 'Performance', 'Building on classic work']","To take a specific case study, for two widely-studied statistical learning tasks in natural language processing—language modeling and spelling correction—it has been shown that simple, classical models can outperform newer, more complex models, just because the simple models can be tractably learnt using orders of magnitude more input data (Banko & Brill, 2001; Brants et al., 2007).",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1154,introduction,"['Performance', 'Data efficiency', 'Scales up']","Analogously, in our view, scaling up existing DBN and sparse coding models to use more parameters, or more training data, might produce very significant performance benefits. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1155,introduction,['Large scale'],"For example, it has been shown that sparse coding exhibits a qualitatively different and highly selective behavior called “end-stopping” when the model is large, but not otherwise (Lee et al., 2006). ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1156,introduction,"['Unifying ideas or integrating components', 'Scales up']","There has been a lot of recent work on scaling up DBN and sparse coding algorithms, sometimes with entire research papers devoted to ingenious methods devised specifically for each of these models (Hinton et al., 2006; Bengio et al., 2006; Murray & Kreutz-Delgado, 2006; Lee et al., 2006; Kavukcuoglu et al., 2008).",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1157,introduction,['Fast'],"Meanwhile, the raw clock speed of single CPUs has begun to hit a hardware power limit, and most of the growth in processing power is increasingly obtained by throwing together multiple CPU cores, instead of speeding up a single core (Gelsinger, 2001; Frank, 2002). ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1158,introduction,"['Performance', 'Building on recent work', 'Used in practice/Popular', 'Easy to implement', 'Parallelizability / distributed']","Recent work has shown that several popular learning algorithms such as logistic regression, linear SVMs and others can be easily implemented in parallel on multicore architectures, by having each core perform the required computations for a subset of input examples, and then combining the results centrally (Dean & Ghemawat, 2004; Chu et al., 2006). ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1160,introduction,"['Large scale', 'Parallelizability / distributed']","This makes the updates hard to massively parallelize at a coarse, dataparallel level (e.g., by computing the updates in parallel and summing them together centrally) without losing the critical stochastic nature of the updates. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1161,introduction,"['Successful', 'Parallelizability / distributed']",It appears that fine-grained parallelism might be needed to successfully parallelize these tasks.,Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1162,introduction,['Large scale'],"In this paper, we exploit the power of modern graphics processors (GPUs) to tractably learn large DBN and sparse coding models. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1163,introduction,['Memory efficiency'],"The typical graphics card shipped with current desktops contains over a hundred processing cores, and has a peak memory bandwidth several times higher than modern CPUs. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1165,introduction,"['Generality', 'Parallelizability / distributed']",Such finegrained parallelism makes GPUs increasingly attractive for general-purpose computation that is hard to parallelize on other distributed architectures. ,Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1166,introduction,['Parallelizability / distributed'],"There is of course a tradeoff—this parallelism is obtained by devoting many more transistors to data processing, rather than to caching and control flow, as in a regular CPU core. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1167,introduction,['Efficiency'],This puts constraints on the types of instructions and memory accesses that can be efficiently implemented. ,Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1168,introduction,['Successful'],"Thus, the main challenge in successfully applying GPUs to a machine learning task is to redesign the learning algorithms to meet these constraints as far as possible. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1169,introduction,['Successful'],"While a thorough introduction to graphics processor architecture is beyond the scope of this paper, we now review the basic ideas behind successful computation with GPUs.",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1170,discussion,['Parallelizability / distributed'],Graphics processors are able to exploit finer-grained parallelism than current multicore architectures or distributed clusters. ,Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1171,discussion,['Large scale'],"They are designed to maintain thousands of active threads at any time, and to schedule the threads on hundreds of cores with very low scheduling overhead. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1172,discussion,"['Successful', 'Building on classic work', 'Parallelizability / distributed']","The map-reduce framework (Dean & Ghemawat, 2004) has been successfully applied to parallelize a class of machine learning algorithms (Chu et al., 2006). ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1173,discussion,['Parallelizability / distributed'],"However, that method relies exclusively on data parallelism—each core might work independently on a different set of input examples—with no further subdivision of work.",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1174,discussion,"['Simplicity', 'Parallelizability / distributed', 'Powerful']","In contrast, the two-level parallelism offered by GPUs is much more powerful: the top-level GPU blocks can already exploit data parallelism, and GPU threads can further subdivide the work in each block, often working with just a single element of an input example. ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1175,discussion,['Applies to real world'],"GPUs have been applied to certain problems in machine learning, including SVMs (Catanzaro et al., 2008), and supervised learning in convolutional networks (Chellapilla et al., 2006). ",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1176,discussion,['Facilitating use (e.g. sharing code)'],"To continue this line of work, and to encourage further applications of deep belief networks and sparse coding, we will make our source code publicly available.",Large-scale deep unsupervised learning using graphics processors,https://dl.acm.org/doi/abs/10.1145/1553374.1553486
1178,abstract,['Identifying limitations'],"However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1179,abstract,['Applies to real world'],"Both of these challenges severely limit the applicability of such methods to complex, real-world domains. ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1182,abstract,['Successful'],"That is, to succeed at the task while acting as randomly as possible. ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1183,abstract,['Formal description/analysis'],Prior deep RL methods based on this framework have been formulated as Q-learning methods. ,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1184,abstract,"['Formal description/analysis', 'Performance', 'State-of-the-art', 'Unifying ideas or integrating components', 'Stable']","By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1185,abstract,"['Performance', 'Stable']","Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1186,introduction,['Applies to real world'],"Model-free deep reinforcement learning (RL) algorithms have been applied in a range of challenging domains, from games (Mnih et al., 2013; Silver et al., 2016) to robotic control (Schulman et al., 2015). ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1187,introduction,"['Approximation', 'Promising', 'Automatic', 'Applies to real world']","The combination of RL and high-capacity function approximators such as neural networks holds the promise of automating a wide range of decision making and control tasks, but widespread adoption of these methods in real-world domains has been hampered by two major challenges. ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1191,introduction,['Identifying limitations'],Both of these challenges severely limit the applicability of model-free deep RL to real-world tasks.,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1192,introduction,['Efficiency'],"One cause for the poor sample efficiency of deep RL methods is on-policy learning: some of the most commonly used deep RL algorithms, such as TRPO (Schulman et al., 2015), PPO (Schulman et al., 2017b) or A3C (Mnih et al., 2016), require new samples to be collected for each gradient step.",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1193,introduction,['Effectiveness'],"This quickly becomes extravagantly expensive, as the number of gradient steps and samples per step needed to learn an effective policy increases with task complexity. ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1195,introduction,['Formal description/analysis'],"This is not directly feasible with conventional policy gradient formulations, but is relatively straightforward for Q-learning based methods (Mnih et al., 2015). ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1196,introduction,"['Approximation', 'Stable']","Unfortunately, the combination of off-policy learning and high-dimensional, nonlinear function approximation with neural networks presents a major challenge for stability and convergence (Bhatnagar et al., 2009). ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1197,introduction,['Performance'],"This challenge is further exacerbated in continuous state and action spaces, where a separate actor network is often used to perform the maximization in Q-learning. ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1198,introduction,"['Efficiency', 'Identifying limitations']","A commonly used algorithm in such settings, deep deterministic policy gradient (DDPG) (Lillicrap et al., 2015), provides for sample-efficient learning but is notoriously challenging to use due to its extreme brittleness and hyperparameter sensitivity (Duan et al., 2016; Henderson et al., 2017).",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1199,introduction,"['Efficiency', 'Stable']",We explore how to design an efficient and stable modelfree deep RL algorithm for continuous state and action spaces. ,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1200,introduction,['Unifying ideas or integrating components'],"To that end, we draw on the maximum entropy framework, which augments the standard maximum reward reinforcement learning objective with an entropy maximization term (Ziebart et al., 2008; Toussaint, 2009; Rawlik et al., 2012; Fox et al., 2016; Haarnoja et al., 2017). ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1202,introduction,"['Robustness', 'Formal description/analysis', 'Building on classic work', 'Improvement']","More importantly, the maximum entropy formulation provides a substantial improvement in exploration and robustness: as discussed by Ziebart (2010), maximum entropy policies are robust in the face of model and estimation errors, and as demonstrated by (Haarnoja et al., 2017), they improve exploration by acquiring diverse behaviors. ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1203,introduction,"['Performance', 'Building on recent work']","Prior work has proposed model-free deep RL algorithms that perform on-policy learning with entropy maximization (O’Donoghue et al., 2016), as well as off-policy methods based on soft Q-learning and its variants (Schulman et al., 2017a; Nachum et al., 2017a; Haarnoja et al., 2017). ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1204,introduction,['Approximation'],"However, the on-policy variants suffer from poor sample complexity for the reasons discussed above, while the off-policy variants require complex approximate inference procedures in continuous action spaces.",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1205,introduction,"['Efficiency', 'Stable']","In this paper, we demonstrate that we can devise an offpolicy maximum entropy actor-critic algorithm, which we call soft actor-critic (SAC), which provides for both sampleefficient learning and stability. ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1207,introduction,"['Approximation', 'Stable']","SAC also avoids the complexity and potential instability associated with approximate inference in prior off-policy maximum entropy algorithms based on soft Q-learning (Haarnoja et al., 2017). ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1208,introduction,"['Novelty', 'Approximation', 'Scientific methodology']","We present a convergence proof for policy iteration in the maximum entropy framework, and then introduce a new algorithm based on an approximation to this procedure that can be practically implemented with deep neural networks, which we call soft actor-critic.",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1209,introduction,"['Quantitative evidence (e.g. experiments)', 'Performance', 'Efficiency', 'Improvement']",We present empirical results that show that soft actor-critic attains a substantial improvement in both performance and sample efficiency over both off-policy and on-policy prior methods. ,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1210,introduction,['Improvement'],"We also compare to twin delayed deep deterministic (TD3) policy gradient algorithm (Fujimoto et al., 2018), which is a concurrent work that proposes a deterministic algorithm that substantially improves on DDPG.",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1211,conclusion,"['Efficiency', 'Stable']","We present soft actor-critic (SAC), an off-policy maximum entropy deep reinforcement learning algorithm that provides sample-efficient learning while retaining the benefits of entropy maximization and stability. ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1212,conclusion,"['Theoretical guarantees', 'Optimal']","Our theoretical results derive soft policy iteration, which we show to converge to the optimal policy. ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1213,conclusion,"['Formal description/analysis', 'Performance', 'State-of-the-art']","From this result, we can formulate a soft actor-critic algorithm, and we empirically show that it outperforms state-of-the-art model-free deep RL methods, including the off-policy DDPG algorithm and the on-policy PPO algorithm. ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1214,conclusion,['Efficiency'],"In fact, the sample efficiency of this approach actually exceeds that of DDPG by a substantial margin. ",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1215,conclusion,"['Robustness', 'Promising', 'Improvement', 'Stable']","Our results suggest that stochastic, entropy maximizing reinforcement learning algorithms can provide a promising avenue for improved robustness and stability, and further exploration of maximum entropy methods, including methods that incorporate second order information (e.g., trust regions (Schulman et al., 2015)) or more expressive policy classes is an exciting avenue for future work.",Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,http://export.arxiv.org/pdf/1801.01290
1219,abstract,['Performance'],"We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. ",Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1220,abstract,"['Large scale', 'Performance', 'State-of-the-art', 'Efficiency', 'Data efficiency']",This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efficiently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.,Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1221,introduction,['Building on classic work'],"In the last two decades, kernel methods have been a prolific theoretical and algorithmic machine learning framework. ",Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1222,introduction,['Large scale'],"By using appropriate regularization by Hilbertian norms, representer theorems enable to consider large and potentially infinite-dimensional feature spaces while working within an implicit feature space no larger than the number of observations. ",Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1224,introduction,['Building on recent work'],"Regularization by sparsity-inducing norms, such as the ℓ 1 -norm has also attracted a lot of interest in recent years. ",Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1225,introduction,"['Performance', 'Reduced training time', 'Building on classic work', 'Building on recent work']","While early work has focused on efficient algorithms to solve the convex optimization problems, recent research has looked at the model selection properties and predictive performance of such methods, in the linear case [3] or within the multiple kernel learning framework (see, e.g., [4]). ",Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1226,introduction,['Unifying ideas or integrating components'],"In this paper, we aim to bridge the gap between these two lines of research by trying to use ℓ 1-norms inside the feature space. ",Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1228,introduction,['Performance'],This leads to two natural questions that we try to answer in this paper: (1) Is it feasible to perform optimization in this very large feature space with cost which is polynomial in the size of the input space? ,Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1229,introduction,['Performance'],(2) Does it lead to better predictive performance and feature selection?,Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1230,introduction,['Preciseness'],"More precisely, we consider a positive definite kernel that can be expressed as a large sum of positive definite basis or local kernels. ",Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1231,introduction,['Exactness'],"This exactly corresponds to the situation where a large feature space is the concatenation of smaller feature spaces, and we aim to do selection among these many kernels, which may be done through multiple kernel learning. ",Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1233,introduction,"['Performance', 'Efficiency']","In order to peform selection efficiently, we make the extra assumption that these small kernels can be embedded in a directed acyclic graph (DAG). ",Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1234,introduction,['Optimal'],"Following [5], we consider in Section 2 a specific combination of ℓ2 -norms that is adapted to the DAG, and will restrict the authorized sparsity patterns; in our specific kernel framework, we are able to use the DAG to design an optimization algorithm which has polynomial complexity in the number of selected kernels (Section 3). ",Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1235,introduction,['Performance'],"In simulations (Section 5), we focus on directed grids, where our framework allows to perform non-linear variable selection. ",Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1236,introduction,"['Novelty', 'Quantitative evidence (e.g. experiments)', 'Qualitative evidence (e.g. examples)', 'Performance']","We provide extensive experimental validation of our novel regularization framework; in particular, we compare it to the regular ℓ2 -regularization and shows that it is always competitive and often leads to better performance, both on synthetic examples, and standard regression and classification datasets from the UCI repository.",Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1237,introduction,['Scientific methodology'],"Finally, we extend in Section 4 some of the known consistency results of the Lasso and multiple kernel learning [3, 4], and give a partial answer to the model selection capabilities of our regularization framework by giving necessary and sufficient conditions for model consistency. ",Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1239,introduction,['Efficiency'],"Hence, by restricting the statistical power of our method, we gain computational efficiency. ",Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1240,conclusion,['Performance'],We have shown how to perform hierarchical multiple kernel learning (HKL) in polynomial time in the number of selected kernels. ,Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1241,conclusion,['Flexibility/Extensibility'],This framework may be applied to many positive definite kernels and we have focused on polynomial and Gaussian kernels used for nonlinear variable selection. ,Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning
1243,abstract,"['Novelty', 'Flexibility/Extensibility']","This paper presents a new UNIfied pre-trained Language Model (UNILM) that
can be fine-tuned for both natural language understanding and generation tasks.","Unified Language Model Pre-training for
Natural Language Understanding and Generation",https://arxiv.org/pdf/1905.03197.pdf
1246,abstract,['Performance'],"UNILM
compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0
and CoQA question answering tasks. ","Unified Language Model Pre-training for
Natural Language Understanding and Generation",https://arxiv.org/pdf/1905.03197.pdf
1247,abstract,"['Quantitative evidence (e.g. experiments)', 'State-of-the-art']","Moreover, UNILM achieves new state-ofthe-art results on five natural language generation datasets, including improving
the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute
improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86
absolute improvement), the CoQA generative question answering F1 score to 82.5
(37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12
(3.75 absolute improvement), and the DSTC7 document-grounded dialog response
generation NIST-4 to 2.67 (human performance is 2.65).","Unified Language Model Pre-training for
Natural Language Understanding and Generation",https://arxiv.org/pdf/1905.03197.pdf
1248,abstract,['Facilitating use (e.g. sharing code)'],"The code and pre-trained
models are available at https://github.com/microsoft/unilm.","Unified Language Model Pre-training for
Natural Language Understanding and Generation",https://arxiv.org/pdf/1905.03197.pdf
1249,introduction,['State-of-the-art'],"Language model (LM) pre-training has substantially advanced the state of the art across a variety
of natural language processing tasks [8, 29, 19, 31, 9, 1].","Unified Language Model Pre-training for
Natural Language Understanding and Generation",https://arxiv.org/pdf/1905.03197.pdf
1255,introduction,"['Performance', 'State-of-the-art', 'Practical']","Although BERT significantly improves the performance of a wide range of natural
language understanding tasks [9], its bidirectionality nature makes it difficult to be applied to natural
language generation tasks [44].","Unified Language Model Pre-training for
Natural Language Understanding and Generation",https://arxiv.org/pdf/1905.03197.pdf
1256,introduction,['Flexibility/Extensibility'],"In this work we propose a new UNIfied pre-trained Language Model (UNILM) that can be applied to
both natural language understanding (NLU) and natural language generation (NLG) tasks.","Unified Language Model Pre-training for
Natural Language Understanding and Generation",https://arxiv.org/pdf/1905.03197.pdf
1264,introduction,['Label efficiency (reduced need for labeled data)'],"Similar to BERT, the pre-trained UNILM can be fine-tuned (with additional task-specific layers
if necessary) to adapt to various downstream tasks.","Unified Language Model Pre-training for
Natural Language Understanding and Generation",https://arxiv.org/pdf/1905.03197.pdf
1265,introduction,['Flexibility/Extensibility'],"But unlike BERT which is used mainly for
NLU tasks, UNILM can be configured, using different self-attention masks (Section 2), to aggregate
context for different types of language models, and thus can be used for both NLU and NLG tasks.","Unified Language Model Pre-training for
Natural Language Understanding and Generation",https://arxiv.org/pdf/1905.03197.pdf
1267,introduction,['Reduced training time'],"First, the unified pre-training procedure leads to a
single Transformer LM that uses the shared parameters and architecture for different types of LMs,
alleviating the need of separately training and hosting multiple LMs.","Unified Language Model Pre-training for
Natural Language Understanding and Generation",https://arxiv.org/pdf/1905.03197.pdf
1269,introduction,['Applies to real world'],"Third, in addition to its application to NLU tasks, the use of UNILM as a
sequence-to-sequence LM (Section 2.3), makes it a natural choice for NLG, such as abstractive
summarization and question generation.","Unified Language Model Pre-training for
Natural Language Understanding and Generation",https://arxiv.org/pdf/1905.03197.pdf
1271,introduction,"['Qualitative evidence (e.g. examples)', 'Effectiveness']","In addition, we demonstrate the effectiveness of UNILM on five NLG datasets, where it
is used as a sequence-to-sequence model, creating new state-of-the-art results on CNN/DailyMail
and Gigaword abstractive summarization, SQuAD question generation, CoQA generative question
answering, and DSTC7 dialog response generation.","Unified Language Model Pre-training for
Natural Language Understanding and Generation",https://arxiv.org/pdf/1905.03197.pdf
1272,Abstract,"['Robustness', 'Secure']","Adversarial training, in which a network is trained on adversarial examples, is one
of the few defenses against adversarial attacks that withstands strong attacks.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1273,Abstract,"['Efficiency', 'Scales up']","Unfortunately, the high cost of generating strong adversarial examples makes standard
adversarial training impractical on large-scale problems like ImageNet.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1274,Abstract,['Efficiency'],"We present an algorithm that eliminates the overhead cost of generating adversarial examples
by recycling the gradient information computed when updating model parameters.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1275,Abstract,"['Robustness', 'Efficiency']","Our “free” adversarial training algorithm achieves comparable robustness to PGD
adversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other
strong adversarial training methods.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1276,Abstract,"['Robustness', 'Efficiency', 'Requires few resources', 'Secure']"," Using a single workstation with 4 P100 GPUs
and 2 days of runtime, we can train a robust model for the large-scale ImageNet
classification task that maintains 40% accuracy against PGD attacks.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1277,Introduction,"['Generalization', 'Performance']",Deep learning has been widely applied to various computer vision tasks with excellent performance.,Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1279,Introduction,"['Robustness', 'Secure']","However, in
security-critical applications, robustness to adversarial attacks has emerged as a critical factor.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1280,Introduction,['Robustness'],A robust classifier is one that correctly labels adversarially perturbed images.,Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1281,Introduction,['Robustness'],"Alternatively, robustness may be achieved by detecting and rejecting adversarial examples [Ma et al., 2018, Meng and Chen,
2017, Xu et al., 2017].",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1282,Introduction,"['Robustness', 'Building on recent work', 'Secure']","Recently, Athalye et al. [2018] broke a complete suite of allegedly robust
defenses, leaving adversarial training, in which the defender augments each minibatch of training
data with adversarial examples [Madry et al., 2017], among the few that remain resistant to attacks.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1283,Introduction,['Efficiency'],"Adversarial training is time-consuming—in addition to the gradient computation needed to update
the network parameters, each stochastic gradient descent (SGD) iteration requires multiple gradient
computations to produce adversarial images.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1284,Introduction,['Robustness'],"In fact, it takes 3-30 times longer to form a robust
network with adversarial training than forming a non-robust equivalent.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1286,Introduction,['Efficiency'],The high cost of adversarial training has motivated a number of alternatives.,Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1288,Introduction,"['Flexibility/Extensibility', 'Efficiency', 'Scales up']","This approach is slower than standard training, and problematic on complex datasets, such as ImageNet, for which it is hard to
produce highly expressive GANs that cover the entire image space.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1289,Introduction,['Secure'],"Another popular defense strategy
is to regularize the training loss using label smoothing, logit squeezing, or a Jacobian regularization
[Shafahi et al., 2019a, Mosbach et al., 2018, Ross and Doshi-Velez, 2018, Hein and Andriushchenko,
2017, Jakubovitz and Giryes, 2018, Yu et al., 2018].",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1290,Introduction,"['Parallelizability / distributed', 'Scales up']","These methods have not been applied to
large-scale problems, such as ImageNet, and can be applied in parallel to adversarial training.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1291,Introduction,['Secure'],"Recently, there has been a surge of certified defenses [Wong and Kolter, 2017, Wong et al., 2018,
Raghunathan et al., 2018a,b, Wang et al., 2018].",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1292,Introduction,"['Generalization', 'Flexibility/Extensibility', 'Scales up']","These methods were mostly demonstrated for
small networks, low-res datasets, and relatively small perturbation budgets (_x000F_).",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1295,Introduction,['Secure'],"Their study was the first certifiable defense for the ImageNet dataset [Deng
et al., 2009].",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1296,Introduction,['Robustness'],"They claim to achieve 12% robustness against non-targeted attacks that are within an
`2 radius of 3 (for images with pixels in [0, 1]).",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1298,Introduction,"['Scales up', 'Secure']","Adversarial training remains among the most trusted defenses, but it is nearly intractable on largescale problems. ",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1299,Introduction,"['Generalization', 'Scales up']","Adversarial training on high-resolution datasets, including ImageNet, has only been
within reach for research labs having hundreds of GPUs1.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1300,Introduction,['Scales up'],"Even on reasonably-sized datasets, such as CIFAR-10 and CIFAR-100, adversarial training is time consuming and can take multiple days.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1301,Introduction,"['Robustness', 'Scales up']","We propose a fast adversarial training algorithm that produces robust models with almost no extra
cost relative to natural training.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1302,Introduction,['Parallelizability / distributed'],"The key idea is to update both the model parameters and image
perturbations using one simultaneous backward pass, rather than using separate gradient computations
for each update step. ",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1303,Introduction,['Efficiency'],"Our proposed method has the same computational cost as conventional natural training, and can be 3-30 times faster than previous adversarial training methods [Madry et al.,
2017, Xie et al., 2019].",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1304,Introduction,"['Robustness', 'Accuracy', 'Secure']","Our robust models trained on CIFAR-10 and CIFAR-100 achieve accuracies
comparable and even slightly exceeding models trained with conventional adversarial training when
defending against strong PGD attacks.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1305,Introduction,"['Robustness', 'Requires few resources', 'Scales up']","We can apply our algorithm to the large-scale ImageNet classification task on a single workstation
with four P100 GPUs in about two days, achieving 40% accuracy against non-targeted PGD attacks.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1306,Introduction,"['Simplicity', 'Robustness']","To the best of our knowledge, our method is the first to successfully train a robust model for ImageNet
based on the non-targeted formulation and achieves results competitive with previous (significantly
more complex) methods [Kannan et al., 2018, Xie et al., 2019].",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1307,Conclusion,"['Robustness', 'Interpretable (to users)']","Adversarial training is a well-studied method that boosts the robustness and interpretability of neural
networks.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1308,Conclusion,"['Efficiency', 'Secure']","While it remains one of the few effective ways to harden a network to attacks, few can
afford to adopt it because of its high computation cost.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1309,Conclusion,['Efficiency'],"We present a “free” version of adversarial
training with cost nearly equal to natural training.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1310,Conclusion,"['Robustness', 'Efficiency', 'Secure']","Free training can be further combined with
other defenses to produce robust models without a slowdown.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1311,Conclusion,"['Requires few resources', 'Secure']","We hope that this approach can put
adversarial training within reach for organizations with modest compute resources.",Adversarial Training for Free!,http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf
1312,Abstract,"['Formal description/analysis', 'Theoretical guarantees', 'Applies to real world', 'Practical']","Neural networks have many successful applications, while much less theoretical
understanding has been gained.",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1315,Abstract,"['Quantitative evidence (e.g. experiments)', 'Understanding (for researchers)']","Furthermore,
the analysis provides interesting insights into several aspects of learning neural
networks and can be verified based on empirical studies on synthetic data and on
the MNIST dataset.",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1317,Introduction,['Quantitative evidence (e.g. experiments)'],"For example, it is empirically observed that
learning with stochastic gradient descent (SGD) in the overparameterized setting (i.e., learning a large
network with number of parameters larger than the number of training data points) does not lead to
overfitting [24, 31]. ",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1318,Introduction,['Generalization'],"Some recent studies use the low complexity of the learned solution to explain the
generalization, but usually do not explain how the SGD or its variants favors low complexity solutions
(i.e., the inductive bias or implicit regularization) [3, 23].",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1320,Introduction,['Valid assumptions'],"Moreover, most of the existing
works trying to explain these phenomenons in general rely on unrealistic assumptions about the data
distribution, such as Gaussian-ness and/or linear separability [32, 25, 10, 17, 7].",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1321,Introduction,['Applies to real world'],"This paper thus proposes to study the problem of learning a two-layer overparameterized neural
network using SGD for classification, on data with a more realistic structure.",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1323,Introduction,['Practical'],"This is motivated
by practical data",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1324,Introduction,['Building on classic work']," For example, on the dataset MNIST [15], each class corresponds to a digit and can
have several components corresponding to different writing styles of the digit, and an image in it is
a small perturbation of one of the components.",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1326,Introduction,"['Generalization', 'Formal description/analysis']","Analysis in this setting can then
help understand how the structure of the practical data affects the optimization and generalization.",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1327,Introduction,"['Generalization', 'Theoretical guarantees']","In this setting, we prove that when the network is sufficiently overparameterized, SGD provably
learns a network close to the random initialization and with a small generalization error.",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1328,Introduction,['Generalization'],"This result
shows that in the overparameterized setting and when the data is well structured, though in principle the network can overfit, SGD with random initialization introduces a strong inductive bias and leads
to good generalization.",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1330,Introduction,['Formal description/analysis'],"More
importantly, the analysis to obtain the result also provides some interesting theoretical insights for
various aspects of learning neural networks.",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1333,Introduction,['Generalization'],"This coupling, together with the structure of the data, allows SGD to find a
solution that has a low generalization error, while still remains in the aforementioned neighborhood of
the initialization. ",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1334,Introduction,['Generalization'],"Our work makes a step towrads explaining how overparameterization and random
initialization help optimization, and how the inductive bias and good generalization arise from the
SGD dynamics on structured data.",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1335,Introduction,['Formal description/analysis'],"Some other more technical implications of our analysis will be
discussed in later sections, such as the existence of a good solution close to the initialization, and the
low-rankness of the weights learned.",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1336,Introduction,['Quantitative evidence (e.g. experiments)'],"Complementary empirical studies on synthetic data and on the
benchmark dataset MNIST provide positive support for the analysis and insights.",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1337,Conclusion,"['Applies to real world', 'Practical']","This work studied the problem of learning a two-layer overparameterized ReLU neural network
via stochastic gradient descent (SGD) from random initialization, on data with structure inspired
by practical datasets.",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1339,Conclusion,['Applies to real world'],"In particular, the real data could be separable
with respect to different metric than `2, or even a non-convex distance given by some manifold. We view this an important open direction.",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,https://proceedings.neurips.cc/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf
1340,abstract,"['Novelty', 'Unifying ideas or integrating components']","We introduce a new family of positive-definite kernel functions that mimic the
computation in large, multilayer neural nets.",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1343,abstract,"['Performance', 'State-of-the-art']","On several problems, we obtain
better results than previous, leading benchmarks from both SVMs with Gaussian
kernels as well as deep belief nets.",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1344,introduction,['Building on recent work'],"Recent work in machine learning has highlighted the circumstances that appear to favor deep architectures, such as multilayer neural nets, over shallow architectures, such as support vector machines
(SVMs) [1].",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1346,introduction,['Flexibility/Extensibility'],"Researchers have advanced several motivations for deep
architectures: the wide range of functions that can be parameterized by composing weakly nonlinear transformations, the appeal of hierarchical distributed representations, and the potential for
combining unsupervised and supervised methods.",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1347,introduction,"['Quantitative evidence (e.g. experiments)', 'Used in practice/Popular']","Experiments have also shown the benefits of
deep learning in several interesting applications [3, 4, 5].",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1349,introduction,['Easy to implement'],Deep architectures are generally more difficult to train than shallow ones.,Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1350,introduction,['Easy to implement'],"They involve difficult nonlinear
optimizations and many heuristics.",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1354,introduction,['Simplicity'],"Like many, we are intrigued by the successes of deep architectures yet drawn to the elegance of kernel methods.",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1355,introduction,['Building on recent work']," In this paper, we explore the possibility of deep learning in kernel machines. Though
we share a similar motivation as previous authors [7], our approach is very different.",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1357,introduction,"['Novelty', 'Simplicity']","First, we develop a new family of kernel functions that mimic the
computation in large neural nets.",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1358,introduction,['Unifying ideas or integrating components'],"Second, using these kernel functions, we show how to train multilayer kernel machines (MKMs) that benefit from many advantages of deep learning.",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1360,introduction,"['Novelty', 'Quantitative evidence (e.g. experiments)']"," In section 2, we describe a new family of kernel
functions and experiment with their use in SVMs.",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1363,introduction,['Simplicity'],"The different layers are trained using a simple combination of
supervised and unsupervised methods.",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1364,introduction,['Identifying limitations'],"Finally, we conclude in section 4 by evaluating the strengths
and weaknesses of our approach.",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1365,discussion,"['Novelty', 'Unifying ideas or integrating components']","In this paper, we have developed a new family of kernel functions that mimic the computation in
large, multilayer neural nets.",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1366,discussion,"['Quantitative evidence (e.g. experiments)', 'Performance']","On challenging data sets, we have obtained results that outperform previous SVMs and compare favorably to deep belief nets.",Kernel Methods for Deep Learning,http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf
1377,Abstract,"['Generalization', 'Building on recent work']","We pursue
a similar approach with a richer kind of latent variable—latent features—using a
Bayesian nonparametric approach to simultaneously infer the number of features
at the same time we learn which entities have each feature.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1379,Abstract,['Quantitative evidence (e.g. experiments)'],We demonstrate that the greater expressiveness of this approach allows us to improve performance on three datasets.,Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1380,Introduction,"['Scales up', 'Applies to real world']","Statistical analysis of social networks and other relational data has been an active area of research for
over seventy years and is becoming an increasingly important problem as the scope and availability
of social network datasets increase [1].",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1384,Introduction,['Building on recent work'],"Our goal is to improve the expressiveness and performance of generative models based on extracting
latent structure representing the properties of individual entities from the observed data, so we will
focus on these kinds of models.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1392,Introduction,['Flexibility/Extensibility'],"The Infinite Relational Model (IRM) [6] used methods from nonparametric Bayesian statistics to
tackle this problem, allowing the number of classes to be determined at inference time.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1393,Introduction,['Flexibility/Extensibility'],"The Infinite
Hidden Relational Model [7] further elaborated on this model and the Mixed Membership Stochastic
Blockmodel (MMSB) [8] extended it to allow entities to have mixed memberships.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1397,Introduction,['Data efficiency'],"In a similar vein, with a limited
amount of data, it might be reasonable to combine these into a single class “male high school students,” but with more data we would want to split this group into athletes and musicians.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1404,Introduction,['Data efficiency'],"Continuing our earlier example, if
we had a limited amount of data, we might not pick up on a feature like “athlete.” ",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1405,Introduction,['Data efficiency'],"However, as we
observe more interactions, this could emerge as a clear feature.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1408,Introduction,['Novelty'],"In this paper, we present the nonparametric latent feature relational model, a Bayesian nonparametric model in which each entity has binary-valued latent features that influences its relations.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1409,Introduction,['Novelty']," In
addition, the relations depend on a set of known covariates.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1410,Introduction,['Novelty'],"This model allows us to simultaneously
infer how many latent features there are while at the same time inferring what features each entity
has and how those features influence the observations.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1411,Introduction,"['Novelty', 'Building on recent work']","This model is strictly more expressive than
the stochastic blockmodel. ",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1414,Introduction,['Quantitative evidence (e.g. experiments)'],"In Section 4, we illustrate the
properties of our model using synthetic data and then show that the greater expressiveness of the
latent feature representation results in improved link prediction on three real datasets.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1416,Conclusion,"['Novelty', 'Flexibility/Extensibility']","We have introduced the nonparametric latent feature relational model, an expressive nonparametric
model for inferring latent binary features in relational entities.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1417,Conclusion,"['Flexibility/Extensibility', 'Building on recent work']","This model combines approaches
from the statistical network analysis community, which have emphasized feature-based methods for
analyzing network data, with ideas from Bayesian nonparametrics in order to simultaneously infer
the number of latent binary features at the same time we infer the features of each entity and how
those features interact.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1418,Conclusion,['Building on recent work'],"Existing class-based approaches infer latent structure that is a special case
of what can be inferred by this model.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1419,Conclusion,['Building on recent work'],"As a consequence, our model is strictly more expressive
than these approaches, and can use the solutions produced by these approaches for initialization.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1420,Conclusion,"['Quantitative evidence (e.g. experiments)', 'Building on recent work']","We showed empirically that the nonparametric latent feature model performs well at link prediction
on several different datasets, including datasets that were originally used to argue for class-based
approaches.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1421,Conclusion,"['Novelty', 'Successful']","The success of this model can be traced to its richer representations, which make it able
to capture subtle patterns of interaction much better than class-based models.",Nonparametric Latent Feature Models for Link Prediction,http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf
1422,Abstract,"['Novelty', 'Quantitative evidence (e.g. experiments)']",We introduce a new type of top-level model for Deep Belief Nets and evaluate it on a 3D object recognition task.,3D Object Recognition with Deep Belief Nets,https://proceedings.neurips.cc/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf
1423,Abstract,['Unifying ideas or integrating components'],"The top-level model is a third-order
Boltzmann machine, trained using a hybrid algorithm that combines both
generative and discriminative gradients.",3D Object Recognition with Deep Belief Nets,https://proceedings.neurips.cc/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf
1424,Abstract,"['Quantitative evidence (e.g. experiments)', 'Performance']","Performance is evaluated on the
NORB database (normalized-uniform version), which contains stereo-pair
images of objects under different lighting conditions and viewpoints.",3D Object Recognition with Deep Belief Nets,https://proceedings.neurips.cc/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf
1426,Abstract,['Performance'],"It substantially outperforms
shallow models such as SVMs (11.6%). ",3D Object Recognition with Deep Belief Nets,https://proceedings.neurips.cc/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf
1427,Abstract,['Label efficiency (reduced need for labeled data)'],"DBNs are especially suited for
semi-supervised learning, and to demonstrate this we consider a modified
version of the NORB recognition task in which additional unlabeled images
are created by applying small translations to the images in the database.",3D Object Recognition with Deep Belief Nets,https://proceedings.neurips.cc/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf
1428,Abstract,['Performance'],"With the extra unlabeled data (and the same amount of labeled data as
before), our model achieves 5.2% error.",3D Object Recognition with Deep Belief Nets,https://proceedings.neurips.cc/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf
1429,Introduction,"['Label efficiency (reduced need for labeled data)', 'Building on recent work', 'Useful']","Recent work on deep belief nets (DBNs) [10], [13] has shown that it is possible to learn
multiple layers of non-linear features that are useful for object classification without requiring labeled data.",3D Object Recognition with Deep Belief Nets,https://proceedings.neurips.cc/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf
1448,Introduction,['Performance'],"Our model significantly outperforms SVM’s, and it also outperforms convolutional neural
nets when given additional unlabeled data produced by small translations of the training
images.",3D Object Recognition with Deep Belief Nets,https://proceedings.neurips.cc/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf
1452,Conclusion,"['Accuracy', 'Label efficiency (reduced need for labeled data)']","The main two points are: 1) Unsupervised, greedy, generative learning can extract an
image representation that supports more accurate object recognition than the raw pixel
representation. ",3D Object Recognition with Deep Belief Nets,https://proceedings.neurips.cc/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf
1453,Conclusion,['Accuracy'],"Including P(v|l) in the objective function for training the top-level model
results in better classification accuracy than using P(l|v) alone. ",3D Object Recognition with Deep Belief Nets,https://proceedings.neurips.cc/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf
1454,Conclusion,"['Generalization', 'Flexibility/Extensibility']","In future work we plan to
factorize the third-order Boltzmann machine as described in [18] so that some of the top-level
features can be shared across classes.",3D Object Recognition with Deep Belief Nets,https://proceedings.neurips.cc/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf
1455,Abstract,['Accuracy'],"Convolutional Neural Networks (ConvNets) are
commonly developed at a fixed resource budget,
and then scaled up for better accuracy if more
resources are available.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1456,Abstract,['Performance'],"In this paper, we systematically study model scaling and identify that
carefully balancing network depth, width, and resolution can lead to better performance.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1457,Abstract,['Simplicity'],"Based
on this observation, we propose a new scaling
method that uniformly scales all dimensions of
depth/width/resolution using a simple yet highly
effective compound coefficient.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1459,Abstract,"['Accuracy', 'Efficiency']","To go even further, we use neural architecture search to design a new baseline network
and scale it up to obtain a family of models,
called EfficientNets, which achieve much
better accuracy and efficiency than previous
ConvNets.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1460,Abstract,"['State-of-the-art', 'Efficiency']","In particular, our EfficientNet-B7
achieves state-of-the-art 84.3% top-1 accuracy
on ImageNet, while being 8.4x smaller and
6.1x faster on inference than the best existing
ConvNet.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1461,Abstract,"['Generalization', 'State-of-the-art', 'Efficiency']","Our EfficientNets also transfer well and
achieve state-of-the-art accuracy on CIFAR-100
(91.7%), Flowers (98.8%), and 3 other transfer
learning datasets, with an order of magnitude
fewer parameters.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1462,Abstract,['Facilitating use (e.g. sharing code)'],"Source code is at https:
//github.com/tensorflow/tpu/tree/
master/models/official/efficientnet.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1463,Introduction,['Accuracy'],Scaling up ConvNets is widely used to achieve better accuracy.,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1464,Introduction,"['Accuracy', 'Scales up']","For example, ResNet (He et al., 2016) can be scaled up from ResNet-18 to ResNet-200 by using more layers; Recently, GPipe (Huang et al., 2018) achieved 84.3% ImageNet top-1 accuracy by scaling up a baseline model four time larger.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1465,Introduction,['Understanding (for researchers)'],"However, the process of scaling up ConvNets
has never been well understood and there are currently many
ways to do it.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1469,Introduction,"['Accuracy', 'Efficiency', 'Easy to implement']","Though it is possible to scale two or three dimensions
arbitrarily, arbitrary scaling requires tedious manual tuning
and still often yields sub-optimal accuracy and efficiency.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1471,Introduction,"['Principled', 'Accuracy', 'Efficiency']"," In particular, we investigate the
central question: is there a principled method to scale up
ConvNets that can achieve better accuracy and efficiency?",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1473,Introduction,"['Simplicity', 'Effectiveness']","Based on this observation, we
propose a simple yet effective compound scaling method",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1475,Introduction,['Simplicity'],"For
example, if we want to use 2
N times more computational
resources, then we can simply increase the network depth by
α
N , width by β
N , and image size by γ
N , where α, β, γ are
constant coefficients determined by a small grid search on
the original small model.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1478,Introduction,"['Formal description/analysis', 'Quantitative evidence (e.g. experiments)']","In
fact, previous theoretical (Raghu et al., 2017; Lu et al., 2018)
and empirical results (Zagoruyko & Komodakis, 2016) both
show that there exists certain relationship between network
width and depth, but to our best knowledge, we are the
first to empirically quantify the relationship among all three
dimensions of network width, depth, and resolution.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1480,Introduction,['Effectiveness'],"Notably, the effectiveness of
model scaling heavily depends on the baseline network; to
go even further, we use neural architecture search (Zoph
& Le, 2017; Tan et al., 2019) to develop a new baseline
network, and scale it up to obtain a family of models, called
EfficientNets.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1481,Introduction,['Performance'],"Figure 1 summarizes the ImageNet performance, where our EfficientNets significantly outperform other ConvNets.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1482,Introduction,"['Accuracy', 'Efficiency']","In particular, our EfficientNet-B7 surpasses
the best existing GPipe accuracy (Huang et al., 2018), but
using 8.4x fewer parameters and running 6.1x faster on inference.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1483,Introduction,"['Accuracy', 'Efficiency']","Compared to the widely used ResNet-50 (He et al.,
2016), our EfficientNet-B4 improves the top-1 accuracy
from 76.3% to 83.0% (+6.7%) with similar FLOPS.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1484,Introduction,"['Generalization', 'Accuracy', 'State-of-the-art', 'Efficiency']","Besides ImageNet, EfficientNets also transfer well and achieve stateof-the-art accuracy on 5 out of 8 widely used datasets, while
reducing parameters by up to 21x than existing ConvNets.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1485,Conclusion,"['Accuracy', 'Efficiency']","In this paper, we systematically study ConvNet scaling and
identify that carefully balancing network width, depth, and
resolution is an important but missing piece, preventing us
from better accuracy and efficiency.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1486,Conclusion,"['Simplicity', 'Principled', 'Efficiency']","To address this issue,
we propose a simple and highly effective compound scaling
method, which enables us to easily scale up a baseline ConvNet to any target resource constraints in a more principled
way, while maintaining model efficiency.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1487,Conclusion,"['State-of-the-art', 'Efficiency', 'Scales up']","Powered by this
compound scaling method, we demonstrate that a mobilesize EfficientNet model can be scaled up very effectively,
surpassing state-of-the-art accuracy with an order of magnitude fewer parameters and FLOPS, on both ImageNet and
five commonly used transfer learning datasets.",EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf
1488,Abstract,['Generalization'],"Recent works have cast some light on the mystery of why deep nets fit any data and generalize
despite being very overparametrized.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1489,Abstract,['Generalization'],"This paper analyzes training and generalization for a simple
2-layer ReLU net with random initialization, and
provides the following improvements over recent
works:",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1490,Abstract,"['Efficiency', 'Understanding (for researchers)']","Using a tighter characterization of training
speed than recent papers, an explanation for
why training a neural net with random labels leads to slower training, as originally
observed in [Zhang et al. ICLR’17].",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1491,Abstract,['Generalization'],"Generalization bound independent of network size, using a data-dependent complexity measure.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1493,Abstract,['Applies to real world'],"Moreover, recent papers require sample complexity to increase (slowly)
with the size, while our sample complexity is
completely independent of the network size.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1494,Abstract,['Theoretical guarantees'],Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent.,Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1495,Abstract,['Generalization'],"The key idea is to track dynamics of training and
generalization via properties of a related kernel",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1499,Introduction,['Generalization'],"See Figure 2 replicating some of
these results.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1500,Introduction,['Understanding (for researchers)'],"Recent papers have begun to provide explanations, showing
that gradient descent can allow an overparametrized multilayer net to attain arbitrarily low training error on fairly
generic datasets (Du et al., 2018a;c; Li & Liang, 2018; AllenZhu et al., 2018b; Zou et al., 2018), provided the amount
of overparametrization is a high polynomial of the relevant
parameters (i.e. vastly more than the overparametrization in
(Zhang et al., 2017)).",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1501,Introduction,['Generalization'],"Under further assumptions it can also
be shown that the trained net generalizes (Allen-Zhu et al.,
2018a). ",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1502,Introduction,['Building on recent work'],"But some issues were not addressed in these papers,
and the goal of the current paper is to address them.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1503,Introduction,['Accuracy'],"First, the experiments in (Zhang et al., 2017) show that
though the nets attain zero training error on even random
data, the convergence rate is much slower.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1504,Introduction,['Accuracy'],See Figure 1.,Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1505,Introduction,"['Efficiency', 'Understanding (for researchers)']","Question 1. Why do true labels give faster convergence
rate than random labels for gradient descent?",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1507,Introduction,['Generalization'],"The next issue is about generalization: clearly, some property of properly labeled data controls generalization, but
what?",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1508,Introduction,"['Generalization', 'Building on classic work']","Classical measures used in generalization theory
such as VC-dimension and Rademacher complexity are
much too pessimistic. ",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1512,Introduction,['Generalization']," There is no property
of data alone that determine upfront whether the trained net
will generalize.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1513,Introduction,['Accuracy'],"A recent paper (Allen-Zhu et al., 2018a)
assumed that there exists an underlying (unknown) neural
network that achieves low error on the data distribution, and
the amount of data available is quite a bit more than the min-imum number of samples needed to learn this underlying
neural net.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1514,Introduction,['Generalization'],"Under this condition, the overparametrized net
(which has way more parameters) can learn in a way that
generalizes.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1515,Introduction,['Flexibility/Extensibility'],"However, it is hard to verify from data whether
this assumption is satisfied, even after the larger net has
finished training.1 ",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1517,Introduction,['Simplicity'],"Question 2. Is there an easily verifiable complexity measure
that can differentiate true labels and random labels?",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1520,Introduction,['Flexibility/Extensibility'],"However, how
to extend these results to non-linear neural networks remains unclear (Wei et al., 2018).",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1521,Introduction,"['Generalization', 'Flexibility/Extensibility', 'Formal description/analysis']","Another line of algorithmdependent analysis of generalization (Hardt et al., 2015;
Mou et al., 2017; Chen et al., 2018) used stability of specific
optimization algorithms that satisfy certain generic properties like convexity, smoothness, etc.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1522,Introduction,['Scales up'],"However, as the number
of epochs becomes large, these generalization bounds are
vacuous.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1523,Introduction,"['Flexibility/Extensibility', 'Formal description/analysis']","We give a new analysis that provides answers
to Questions 1 and 2 for overparameterized two-layer neural
networks with ReLU activation trained by gradient descent
(GD), when the number of neurons in the hidden layer is
sufficiently large.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1527,Introduction,['Accuracy'],"In Section 4, using the trajectory of the network predictions on the training data during optimization, we
accurately estimate the magnitude of training loss in
each iteration.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1528,Introduction,['Accuracy'],"Our key finding is that the number of
iterations needed to achieve a target accuracy depends
on the projections of data labels on the eigenvectors
of a certain Gram matrix to be defined in Equation (3).",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1530,Introduction,"['Generalization', 'Theoretical guarantees', 'Accuracy']","• In Section 5, we give a generalization bound for the
solution found by GD, based on accurate estimates of
how much the network parameters can move during
optimization (in suitable norms).",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1531,Introduction,"['Generalization', 'Flexibility/Extensibility']"," Our generalization bound depends on a data-dependent complexity measure (c.f. Equation (10)), and notably, is completely independent of the number of hidden units in the network.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1533,Introduction,"['Generalization', 'Theoretical guarantees', 'Accuracy']","Notice that because zero training error is achieved by
the solution found by GD, a generalization bound is an
upper bound on the error on the data distribution (test
error). ",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1534,Introduction,"['Generalization', 'Flexibility/Extensibility']","We also remark that our generalization bound is
valid for any data labels – it does not require the existence of a small ground-truth network as in (Allen-Zhu
et al., 2018a).",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1535,Introduction,['Efficiency'],"Moreover, our bound can be efficiently
computed for any data labels.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1536,Introduction,"['Flexibility/Extensibility', 'Theoretical guarantees']","In Section 6, we further study what kind of functions
can be provably learned by two-layer ReLU networks
trained by GD. ",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1537,Introduction,"['Generalization', 'Flexibility/Extensibility']","Combining the optimization and generalization results, we uncover a broad class of learnable
functions, including linear functions, two-layer neural
networks with polynomial activation φ(z) = z
2l or cosine activation, etc.",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1538,Introduction,['Flexibility/Extensibility'],"Our requirement on the smoothness
of learnable functions is weaker than that in (Allen-Zhu
et al., 2018a).",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1539,Introduction,['Flexibility/Extensibility'],"Finally, we note that the intriguing generalization phenomena in deep learning were observed in kernel methods as
well (Belkin et al., 2018).",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1540,Introduction,['Building on recent work'],"The analysis in the current paper is also related to a kernel from the ReLU activation
(c.f. Equation (3)).",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1541,Conclusion,"['Flexibility/Extensibility', 'Formal description/analysis']","This paper shows how to give a fine-grained analysis of
the optimization trajectory and the generalization ability
of overparameterized two-layer neural networks trained by
gradient descent",Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,http://proceedings.mlr.press/v97/arora19a/arora19a-supp.pdf
1543,Abstract,"['Flexibility/Extensibility', 'Robustness']",We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the `2 norm.,Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1545,Abstract,['Theoretical guarantees'],"We prove a tight robustness guarantee in
`2 norm for smoothing with Gaussian noise.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1546,Abstract,['Accuracy'],"We
use randomized smoothing to obtain an ImageNet
classifier with e.g. a certified top-1 accuracy of
49% under adversarial perturbations with `2 norm
less than 0.5 (=127/255).",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1547,Abstract,['Security'],No certified defense has been shown feasible on ImageNet except for smoothing.,Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1548,Abstract,"['Robustness', 'Accuracy']","On smaller-scale datasets where competing approaches to certified `2 robustness are
viable, smoothing delivers higher certified accuracies. ",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1549,Abstract,"['Robustness', 'Quantitative evidence (e.g. experiments)']","Our strong empirical results suggest that
randomized smoothing is a promising direction
for future research into adversarially robust classification. ",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1550,Abstract,['Facilitating use (e.g. sharing code)'],Code and models are available at http: //github.com/locuslab/smoothing,Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1551,Introduction,"['Robustness', 'Accuracy', 'Security']","Modern image classifiers achieve high accuracy on i.i.d.
test sets but are not robust to small, adversarially-chosen
perturbations of their inputs (Szegedy et al., 2014; Biggio
et al., 2013).",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1553,Introduction,['Robustness'],"Many works have proposed heuristic methods for training classifiers intended to be robust to
adversarial perturbations.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1555,Introduction,['Theoretical guarantees'],"In response, a line of work on certifiable robustness studies classifiers whose prediction at
any point x is verifiably constant within some set around x
(Wong & Kolter, 2018; Raghunathan et al., 2018a, e.g.).",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1557,Introduction,"['Theoretical guarantees', 'Scales up']","Unfortunately, all existing approaches for
certifying the robustness of neural networks have trouble
scaling to networks that are large and expressive enough to
solve problems like ImageNet.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1559,Introduction,['Formal description/analysis'],"Recently, two papers (Lecuyer et al., 2019;
Li et al., 2018) showed that an operation we call randomized
smoothing1
can transform any arbitrary base classifier f into
a new “smoothed classifier” g that is certifiably robust in
`2 norm.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1564,Introduction,['Theoretical guarantees'],"But the smoothed classifier g will also possess
a desirable property that the base classifier may lack: one
can verify that g’s prediction is constant within an `2 ball
around any input x, simply by estimating the probabilities
with which f classifies N (x, σ2
I) as each class",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1566,Introduction,"['Theoretical guarantees', 'Security']","Lecuyer et al. (2019) proposed randomized smoothing as
a provable adversarial defense, and used it to train the first
certifiably robust classifier for ImageNet.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1568,Introduction,['Theoretical guarantees'],"However, both of these guarantees are loose, in the sense that
the smoothed classifier g is provably always more robust
than the guarantee indicates.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1569,Introduction,"['Robustness', 'Theoretical guarantees']","In this paper, we prove the
first tight robustness guarantee for randomized smoothing.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1570,Introduction,"['Robustness', 'Theoretical guarantees']","Our analysis reveals that smoothing with Gaussian noise
naturally induces certifiable robustness under the `2 norm.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1571,Introduction,['Robustness'],"We suspect that other, as-yet-unknown noise distributions
might induce robustness to other perturbation sets such as
general `p norm balls",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1574,Introduction,['Robustness'],"Therefore, it is not possible to exactly evaluate g’s
prediction at any input x, or to exactly compute the radius
in which this prediction is certifiably robust.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1575,Introduction,['Flexibility/Extensibility']," Instead, we
present Monte Carlo algorithms for both tasks that are guaranteed to succeed with arbitrarily high probability",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1576,Introduction,"['Simplicity', 'Flexibility/Extensibility', 'Scales up']","Despite this drawback, randomized smoothing enjoys several compelling advantages over other certifiably robust
classifiers proposed in the literature: it makes no assumptions about the base classifier’s architecture, it is simple to
implement and understand, and, most importantly, it permits the use of arbitrarily large neural networks as the base
classifier. ",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1577,Introduction,"['Scales up', 'Security']"," In contrast, other certified defenses do not currently scale to large networks.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1578,Introduction,['Security'],"Indeed, smoothing is the only
certified adversarial defense which has been shown feasible
on the full-resolution ImageNet classification task.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1579,Introduction,"['Robustness', 'Accuracy', 'State-of-the-art']","We use randomized smoothing to train state-of-the-art certifiably `2-robust ImageNet classifiers; for example, one of
them achieves 49% provable top-1 accuracy under adversarial perturbations with `2 norm less than 127/255 (Table
1). ",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1580,Introduction,"['Flexibility/Extensibility', 'Robustness', 'Accuracy']","We also demonstrate that on smaller-scale datasets like CIFAR-10 and SHVN, where competing approaches to certified `2 robustness are feasible, randomized smoothing can
deliver better certified accuracies, both because it enables
the use of larger networks and because it does not constrain
the expressivity of the base classifier.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1581,Conclusion,"['Flexibility/Extensibility', 'Robustness', 'Theoretical guarantees']","Theorem 2 establishes that smoothing with Gaussian noise
naturally confers adversarial robustness in `2 norm: if we
have no knowledge about the base classifier beyond the distribution of f(x + ε), then the set of perturbations to which
the smoothed classifier is provably robust is precisely an `2
ball.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1582,Conclusion,['Robustness'],"We suspect that smoothing with other noise distributions may lead to similarly natural robustness guarantees for
other perturbation sets such as general `p norm balls.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1583,Conclusion,"['Robustness', 'Quantitative evidence (e.g. experiments)', 'Security']","Our strong empirical results suggest that randomized
smoothing is a promising direction for future research
into adversarially robust classification.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1584,Conclusion,['Scales up'],"Many empirical approaches have been “broken,” and provable approaches
based on certifying neural network classifiers have not been
shown to scale to networks of modern size.",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1585,Conclusion,['Scales up'],"It seems to be computationally infeasible to reason in any sophisticated
way about the decision boundaries of a large, expressive neural network. ",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1587,Conclusion,['Robustness'],"To make the smoothed classifier robust, one need simply make the base classifier classify well
under noise. ",Certified Adversarial Robustness via Randomized Smoothing,https://icml.cc/media/Slides/icml/2019/grandball(12-11-00)-12-11-30-4744-certified_adver.pdf
1589,Abstract,"['Robustness', 'Accuracy', 'Security']","We identify a trade-off between robustness and
accuracy that serves as a guiding principle in the
design of defenses against adversarial examples.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1590,Abstract,['Formal description/analysis'],"Although this problem has been widely studied
empirically, much remains unknown concerning
the theory underlying this trade-off.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1591,Abstract,"['Robustness', 'Theoretical guarantees', 'Accuracy']","In this work, we decompose the prediction error for adversarial
examples (robust error) as the sum of the natural
(classification) error and boundary error, and provide a differentiable upper bound using the theory
of classification-calibrated loss, which is shown to
be the tightest possible upper bound uniform over
all probability distributions and measurable predictors.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1592,Abstract,"['Robustness', 'Accuracy', 'Security']","Inspired by our theoretical analysis, we
also design a new defense method, TRADES, to
trade adversarial robustness off against accuracy.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1593,Abstract,"['Quantitative evidence (e.g. experiments)', 'Applies to real world']",Our proposed algorithm performs well experimentally in real-world datasets.,Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1594,Abstract,"['Accuracy', 'State-of-the-art']","The methodology is
the foundation of our entry to the NeurIPS 2018
Adversarial Vision Challenge in which we won
the 1st place out of ~2,000 submissions, surpassing the runner-up approach by 11.41% in terms
of mean `2 perturbation distance.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1595,Introduction,"['Robustness', 'Building on recent work', 'Security']","In response to the vulnerability of deep neural networks
to small perturbations around input data (Szegedy et al.,
2013), adversarial defenses have been an imperative object
of study in machine learning (Huang et al., 2017), computer
vision (Song et al., 2018; Xie et al., 2017; Meng & Chen,
2017), natural language processing (Jia & Liang, 2017),
and many other domains.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1596,Introduction,"['Understanding (for researchers)', 'Security']","In machine learning, study of
adversarial defenses has led to significant advances in understanding and defending against adversarial threat (He et al.,
2017). ",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1597,Introduction,"['Applies to real world', 'Security']","In computer vision and natural language processing, adversarial defenses serve as indispensable building blocks for a range of security-critical systems and applications, such as autonomous cars and speech recognition
authorization. ",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1598,Introduction,"['Accuracy', 'Security']",The problem of adversarial defenses can be stated as that of learning a classifier with high test accuracy on both natural and adversarial examples.,Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1601,Introduction,['Generalization'],"The focus of this
work is the former setting, though our framework can be
generalized to the latter.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1602,Introduction,"['Robustness', 'Building on recent work']","Despite a large literature devoted to improving the robustness of deep-learning models, many fundamental questions
remain unresolved.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1603,Introduction,"['Robustness', 'Accuracy']","One of the most important questions
is how to trade off adversarial robustness against natural
accuracy. ",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1604,Introduction,"['Robustness', 'Accuracy']","Statistically, robustness can be be at odds with
accuracy when no assumptions are made on the data distribution (Tsipras et al., 2019).",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1605,Introduction,['Security'],"This has led to an empirical
line of work on adversarial defense that incorporates various kinds of assumptions (Su et al., 2018; Kurakin et al.,
2017). ",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1606,Introduction,"['Robustness', 'Theoretical guarantees', 'Security']","On the theoretical front, methods such as relaxation
based defenses (Kolter & Wong, 2018; Raghunathan et al.,
2018a) provide provable guarantees for adversarial robustness.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1607,Introduction,"['Robustness', 'Performance', 'Accuracy']","They, however, ignore the performance of classifier
on the non-adversarial examples, and thus leave open the
theoretical treatment of the putative robustness/accuracy
trade-off.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1609,Introduction,"['Robustness', 'Scales up']","For example,
the straightforward empirical risk minimization (ERM) formulation of robust classification involves minimizing the
robust 0-1 loss maxx0:kx0xk✏ 1{c(x0
) 6= y}, a loss which
is NP-hard to optimize even if ✏ = 0 in general.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1610,Introduction,"['Building on recent work', 'Security']","Hence, it
is natural to expect that some prior work on adversarial defense replaced the 0-1 loss 1(·) with a surrogate loss (Madry
et al., 2018; Kurakin et al., 2017; Uesato et al., 2018).",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1611,Introduction,['Theoretical guarantees'],"However, there is little theoretical guarantee on the tightness of
this approximation.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1612,Introduction,"['Theoretical guarantees', 'Quantitative evidence (e.g. experiments)', 'Building on recent work']","We begin with an example that illustrates the trade-off between accuracy and adversarial robustness in Section 2.4, a
phenomenon which has been demonstrated by Tsipras et al.
(2019), but without theoretical guarantees.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1613,Introduction,"['Simplicity', 'Quantitative evidence (e.g. experiments)', 'Accuracy']","We constructed
a toy example where the Bayes optimal classifier achieves
natural error 0% and robust error 100%, while the trivial
all-one classifier achieves both natural error and robust error 50% (Table 1).",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1614,Introduction,"['Simplicity', 'Theoretical guarantees', 'Accuracy', 'Building on recent work']","Despite a large literature on the analysis
of robust error in terms of generalization (Schmidt et al.,
2018; Cullina et al., 2018; Yin et al., 2018) and computational complexity (Bubeck et al., 2018b;a), the trade-off
between the natural error and the robust error has not been
a focus of theoretical study.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1615,Introduction,['Theoretical guarantees'],"We show that the robust error can in general be bounded
tightly using two terms: one corresponds to the natural error measured by a surrogate loss function, and the other
corresponds to how likely the input features are close to the
✏-extension of the decision boundary, termed as the boundary error.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1617,Introduction,"['Theoretical guarantees', 'Scales up', 'Security']","Our theoretical analysis naturally leads to a new formulation
of adversarial defense which has several appealing properties; in particular, it inherits the benefits of scalability to
large datasets exhibited by Tiny ImageNet, and the algorithm achieves state-of-the-art performance on a range of
benchmarks while providing theoretical guarantees.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1618,Introduction,"['Simplicity', 'Accuracy', 'State-of-the-art', 'Building on recent work']","For
example, while the defenses overviewed in (Athalye et al.,
2018) achieve robust accuracy no higher than ~47% under
white-box attacks, our method achieves robust accuracy as
high as ~57% in the same setting.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1619,Introduction,"['Accuracy', 'State-of-the-art', 'Building on recent work']","The methodology is the
foundation of our entry to the NeurIPS 2018 Adversarial
Vision Challenge where we won first place out of ~2,000
submissions, surpassing the runner-up approach by 11.41%
in terms of mean `2 perturbation distance.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1620,Introduction,"['Robustness', 'Accuracy', 'State-of-the-art']","Our work tackles the problem of trading accuracy off against
robustness and advances the state-of-the-art in multiple
ways.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1621,Introduction,"['Robustness', 'Formal description/analysis', 'Accuracy']","Theoretically, we characterize the trade-off between
accuracy and robustness for classification problems
via decomposing the robust error as the sum of the
natural error and the boundary error.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1622,Introduction,['Formal description/analysis'],"We provide differentiable upper bounds on both terms using the theory
of classification-calibrated loss, which are shown to be
the tightest upper bounds uniform over all probability
distributions and measurable predictors.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1623,Introduction,['Security'],"Algorithmically, inspired by our theoretical analysis,
we propose a new formulation of adversarial defense,
TRADES, as optimizing a regularized surrogate loss.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1624,Introduction,"['Simplicity', 'Accuracy', 'Security']","The loss consists of two terms: the term of empirical
risk minimization encourages the algorithm to maximize the natural accuracy, while the regularization term
encourages the algorithm to push the decision boundary away from the data, so as to improve adversarial
robustness (see Figure 1).",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1625,Introduction,"['Quantitative evidence (e.g. experiments)', 'Performance', 'State-of-the-art']","Experimentally, we show that our proposed algorithm
outperforms state-of-the-art methods under both blackbox and white-box threat models.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1626,Introduction,['State-of-the-art'],"In particular, the
methodology won the final round of the NeurIPS 2018
Adversarial Vision Challenge.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1627,Conclusion,['Security'],"In this paper, we study the problem of adversarial defenses
against structural perturbations around input data.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1628,Conclusion,"['Robustness', 'Theoretical guarantees', 'Accuracy']","We focus
on the trade-off between robustness and accuracy, and show
an upper bound on the gap between robust error and optimal
natural error. ",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1629,Conclusion,['State-of-the-art'],"Our result advances the state-of-the-art work
and matches the lower bound in the worst-case scenario.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1631,Conclusion,"['Quantitative evidence (e.g. experiments)', 'Effectiveness', 'Applies to real world']","Experiments on real datasets and adversarial competition
demonstrate the effectiveness of our proposed algorithms.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1632,Conclusion,"['Robustness', 'Accuracy', 'Security']","It would be interesting to combine our methods with other
related line of research on adversarial defenses, e.g., feature
denoising technique (Xie et al., 2018) and network architecture design (Cisse et al., 2017), to achieve more robust
learning systems.",Theoretically Principled Trade-off between Robustness and Accuracy,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf
1634,Abstract,"['Formal description/analysis', 'Theoretical guarantees']",The current paper proves gradient descent achieves zero training loss in polynomial time for a deep overparameterized neural network with residual connections (ResNet).,Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1637,Abstract,['Flexibility/Extensibility'],"We further extend our analysis
to deep residual convolutional neural networks
and obtain a similar convergence result.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1640,Introduction,['Theoretical guarantees'],"For example, Lu et al. (2017) proved that except for a measure zero set, all functions cannot be approximated by ReLU networks with a width less than the input dimension.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1641,Introduction,['Used in practice/Popular'],"In practice, many neural network architectures are
highly over-parameterized.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1645,Introduction,['Formal description/analysis'],"Theoretically, Hardt & Ma (2016) showed that residual links in linear networks prevent gradient vanishing in a large neighborhood of zero, but for neural networks with non-linear
activations, the advantages of using residual connections
are not well understood.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1646,Introduction,"['Formal description/analysis', 'Theoretical guarantees', 'Building on recent work']","In this paper, we demystify these two mysterious phenomena.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1652,Introduction,"['Formal description/analysis', 'Theoretical guarantees']","We show if
m = Ω
poly(n)2O(H)
_x0001_
1
, then randomly initialized
gradient descent converges to zero training loss at a
linear rate.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1653,Introduction,"['Formal description/analysis', 'Theoretical guarantees']","Next, we consider the ResNet architecture. We show
as long as m = Ω (poly(n, H)), then randomly initialized gradient descent converges to zero training loss
at a linear rate",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1657,Introduction,"['Formal description/analysis', 'Theoretical guarantees']","We show if m = poly(n, p, H) where p is the number of patches, then randomly initialized gradient descent achieves zero training loss.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1658,Introduction,['Building on recent work'],Our proof builds on two ideas from previous work on gradient descent for two-layer neural networks,Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1659,Introduction,['Building on recent work'],"First, we use the observation by (Li & Liang, 2018) that if the neural network is over-parameterized, every weight matrix is close
to its initialization.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1660,Introduction,['Building on recent work'],"Second, following (Du et al., 2018b) we analyze the dynamics of the predictions whose convergence is determined by the least eigenvalue of the Gram matrix induced by the neural network architecture and to
lower bound the least eigenvalue, it is sufficient to bound
the distance of each weight matrix from its initialization.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1663,Conclusion,"['Formal description/analysis', 'Theoretical guarantees']","In this paper, we show that gradient descent on deep overparametrized networks can obtain zero training loss",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1664,Conclusion,['Building on recent work'],"Our proof builds on a careful analysis of the random initialization scheme and a perturbation analysis which shows
that the Gram matrix is increasingly stable under overparametrization.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1665,Conclusion,"['Formal description/analysis', 'Theoretical guarantees']","These techniques allow us to show that
every step of gradient descent decreases the loss at a geometric rate.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1669,Conclusion,['Identifying limitations'],"In particular, existing work only demonstrate that gradient descent
works under the same situations as kernel methods and
random feature methods (Daniely, 2017; Li & Liang,
2018; Allen-Zhu et al., 2018a; Arora et al., 2019).",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1670,Conclusion,['Building on recent work'],"To further investigate of generalization behavior, we believe some algorithm-dependent analyses may be useful (Hardt et al., 2016; Mou et al., 2018; Chen et al.,
2018).",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1672,Conclusion,['Used in practice/Popular'],"Realistic networks have number of parameters,
not width, a large constant multiple of n.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1673,Conclusion,"['Generalization', 'Flexibility/Extensibility']",We consider improving the analysis to cover commonly utilized networks an important open problem.,Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1674,Conclusion,['Identifying limitations'],"The current analysis is for gradient descent, instead of
stochastic gradient descent.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1675,Conclusion,"['Generalization', 'Flexibility/Extensibility']","We believe the analysis
can be extended to stochastic gradient, while maintaining the linear convergence rate.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1676,Conclusion,['Building on recent work'],"The convergence rate can be potentially improved if
the minimum eigenvalue takes into account the contribution of all Gram matrices, but this would considerably complicate the initialization and perturbation
analysis.",Gradient Descent Finds Global Minima of Deep Neural Networks,https://arxiv.org/pdf/1811.03804.pdf
1677,Abstract,['Generalization'],"Deep nets generalize well despite having more
parameters than the number of training samples.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1678,Abstract,"['Formal description/analysis', 'Theoretical guarantees', 'Understanding (for researchers)']","Recent works try to give an explanation using
PAC-Bayes and Margin-based analyses, but do
not as yet result in sample complexity bounds
better than naive parameter counting.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1679,Abstract,"['Formal description/analysis', 'Theoretical guarantees', 'Applies to real world']","The current paper shows generalization bounds that are
orders of magnitude better in practice.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1681,Abstract,"['Formal description/analysis', 'Theoretical guarantees']"," Our results also provide some theoretical
justification for widespread empirical success in
compressing deep nets.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1682,Abstract,"['Formal description/analysis', 'Theoretical guarantees', 'Quantitative evidence (e.g. experiments)']","Analysis of correctness
of our compression relies upon some newly identified “noise stability”properties of trained deep
nets, which are also experimentally verified.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1683,Abstract,['Flexibility/Extensibility'],"The
study of these properties and resulting generalization bounds are also extended to convolutional
nets, which had eluded earlier attempts on proving
generalization.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1684,Introduction,['Generalization'],"A mystery about deep nets is that they generalize (i.e., predict well on unseen data) despite having far more parameters
than the number of training samples.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1687,Introduction,"['Quantitative evidence (e.g. experiments)', 'Applies to real world']","Clearly, deep nets trained on real-life data have some properties that reduce effective capacity, but identifying them has
proved difficult —at least in a quantitative way that yields
sample size upper bounds similar to classical analyses in
simpler models such as SVMs (Bartlett and Mendelson,
2002; Evgeniou et al., 2000; Smola et al., 1998) or matrix
factorization (Fazel et al., 2001; Srebro et al., 2005).","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1688,Introduction,['Qualitative evidence (e.g. examples)'],"Qualitatively (Hochreiter and Schmidhuber, 1997; Hinton
and Van Camp, 1993) suggested that nets that generalize
well are flat minima in the optimization landscape of the
training loss. ","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1689,Introduction,['Quantitative evidence (e.g. experiments)'],"Recently Keskar et al. (2016) show using
experiments with different batch-sizes that sharp minima
do correlate with higher generalization error.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1690,Introduction,"['Robustness', 'Quantitative evidence (e.g. experiments)']","A quantitative version of “flatness” was suggested in (Langford
and Caruana, 2001): the net’s output is stable to noise
added to the net’s trainable parameters.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1691,Introduction,"['Simplicity', 'Formal description/analysis', 'Theoretical guarantees']","Using PAC-Bayes
bound (McAllester, 1998; 1999) this noise stability yielded
generalization bounds for fully connected nets of depth 2.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1692,Introduction,"['Flexibility/Extensibility', 'Formal description/analysis', 'Theoretical guarantees']","The theory has been extended to multilayer fully connected
nets (Neyshabur et al., 2017b), although thus far yields sample complexity bounds much worse than naive parameter
counting. (Same holds for the earlier Bartlett and Mendelson (2002); Neyshabur et al. (2015b); Bartlett et al. (2017);
Neyshabur et al. (2017a); Golowich et al. (2017); see Figure 3).","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1693,Introduction,['Quantitative evidence (e.g. experiments)'],"Another notion of noise stability —closely related to
dropout and batch normalization—is stability of the output
with respect to the noise injected at the nodes of the network,
which was recently shown experimentally (Morcos et al.,
2018) to improve in tandem with generalization ability during training, and to be absent in nets trained on random data.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1695,Introduction,"['Generalization', 'Performance', 'Efficiency', 'Applies to real world']","While study of generalization may appear a bit academic —
held-out data easily establishes generalization in practice—
the ultimate hope is that it will help identify simple, measurable and intuitive properties of well-trained deep nets, which
in turn may fuel superior architectures and faster training.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1696,Introduction,"['Formal description/analysis', 'Theoretical guarantees', 'Quantitative evidence (e.g. experiments)']","We hope the detailed study —theoretical and empirical—in
the current paper advances this goal.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1697,Introduction,['Generalization'],"A simple compression framework (Section 2) for proving generalization bounds, perhaps a more explicit and
intuitive form of the PAC-Bayes work.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1698,Introduction,"['Generalization', 'Formal description/analysis', 'Theoretical guarantees']","It also yields
elementary short proofs of recent generalization results (Section 2.2).","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1699,Introduction,"['Robustness', 'Building on recent work']","Identifying new form of noise-stability for deep nets:
the stability of each layer’s computation to noise injected at lower layers.  (Earlier papers worked only
with stability of the output layer.)","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1700,Introduction,['Quantitative evidence (e.g. experiments)'],"Figure 1 visualizes
the stability of network w.r.t. Gaussian injected noise.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1701,Introduction,['Formal description/analysis'],"Formal statements require a string of other properties
(Section 3).","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1702,Introduction,"['Generalization', 'Quantitative evidence (e.g. experiments)']","All are empirically studied, including their
correlation with generalization (Section 6).","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1703,Introduction,"['Generalization', 'Formal description/analysis', 'Theoretical guarantees', 'Efficiency', 'Memory efficiency']","Using the above properties to derive efficient and provably correct algorithms that reduce the effective number of parameters in the nets, yielding generalization
bounds that: (a) are better than naive parameter counting (Section 6) (b) depend on simple, intuitive and
measurable properties of the network (Section 4) (c)
apply also to convolutional nets (Section 5) (d) empirically correlate with generalization (Section 6).","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1716,Introduction,"['Generalization', 'Building on recent work']","Recently Kawaguchi et al. (2017) connects PathNorm (Neyshabur et al., 2015a) to generalization.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1717,Introduction,"['Generalization', 'Formal description/analysis', 'Theoretical guarantees']","However,
the proved generalization bound depends on the distribution
and measuring it requires vector operations on exponentially
high dimensional vectors.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1718,Introduction,"['Generalization', 'Quantitative evidence (e.g. experiments)']","Other works have designed experiments to empirically evaluate potential properties of the network that helps generalization(Arpit et al., 2017; Neyshabur
et al., 2017b; Dinh et al., 2017).","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1719,Introduction,['Energy efficiency'],"The idea of compressing
trained deep nets is very popular for low-power applications;
for a survey see Cheng et al. (2018).","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1727,Introduction,['Generalization'],Generalization error is the difference between the two.,"Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1738,Conclusion,['Building on recent work'],"With a new compression-based approach, the paper has
made progress on several open issues regarding generalization properties of deep nets. ","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1740,Conclusion,"['Quantitative evidence (e.g. experiments)', 'Energy efficiency']","Another possibility is a more rigorous understanding of
deep net compression, which sees copious empirical work
motivated by low-power applications.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1741,Conclusion,"['Energy efficiency', 'Practical']","Perhaps our p-wise independence idea used for compressing convnets (Section 5)
has practical implications.","Stronger generalization bounds for deep nets via a compression
approach",http://proceedings.mlr.press/v80/arora18b/arora18b.pdf
1742,Abstract,"['Robustness', 'Security']","Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model.",Black-box Adversarial Attacks with Limited Queries and Information,http://proceedings.mlr.press/v80/ilyas18a/ilyas18a.pdf
1743,Abstract,"['Applies to real world', 'Security']"," In practice, the threat
model for real-world systems is often more restrictive than the typical black-box model where
the adversary can observe the full output of the
network on arbitrarily many chosen inputs.",Black-box Adversarial Attacks with Limited Queries and Information,http://proceedings.mlr.press/v80/ilyas18a/ilyas18a.pdf
1744,Abstract,"['Applies to real world', 'Security']","We
define three realistic threat models that more
accurately characterize many real-world classifiers: the query-limited setting, the partialinformation setting, and the label-only setting.",Black-box Adversarial Attacks with Limited Queries and Information,http://proceedings.mlr.press/v80/ilyas18a/ilyas18a.pdf
1745,Abstract,['Security'],"We develop new attacks that fool classifiers under these more restrictive threat models, where
previous methods would be impractical or ineffective.",Black-box Adversarial Attacks with Limited Queries and Information,http://proceedings.mlr.press/v80/ilyas18a/ilyas18a.pdf
1746,Abstract,['Security'],We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models.,Black-box Adversarial Attacks with Limited Queries and Information,http://proceedings.mlr.press/v80/ilyas18a/ilyas18a.pdf
1747,Abstract,"['Applies to real world', 'Security']","We also demonstrate a
targeted black-box attack against a commercial
classifier, overcoming the challenges of limited
query access, partial information, and other practical issues to break the Google Cloud Vision
API.",Black-box Adversarial Attacks with Limited Queries and Information,http://proceedings.mlr.press/v80/ilyas18a/ilyas18a.pdf
1748,Introduction,"['Robustness', 'Building on recent work', 'Security']","Neural network-based image classifiers are susceptible to adversarial examples, minutely perturbed inputs that fool classifiers (Szegedy et al., 2013; Biggio et al., 2013).",Black-box Adversarial Attacks with Limited Queries and Information,http://proceedings.mlr.press/v80/ilyas18a/ilyas18a.pdf
1751,Abstract,['Novelty'],"This paper
presents a novel dual coordinate descent
method for linear SVM with L1- and L2-
loss functions.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1752,Abstract,"['Simplicity', 'Efficiency', 'Scales up']","The proposed method is simple and reaches an _x000F_-accurate solution in
O(log(1/_x000F_)) iterations.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1753,Abstract,"['Quantitative evidence (e.g. experiments)', 'State-of-the-art', 'Building on recent work']","Experiments indicate
that our method is much faster than state
of the art solvers such as Pegasos, TRON,
SVMperf, and a recent primal coordinate descent implementation.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1767,Introduction,['Applies to real world'],"We indicate such cases as linear
SVM; these are often encountered in applications such
as document classification.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1768,Introduction,['Large scale'],"In this paper, we aim at
solving very large linear SVM problems.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1769,Introduction,"['Large scale', 'Building on recent work']","Recently, many methods have been proposed for linear SVM in large-scale scenarios.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1775,Introduction,['Efficiency'],These algorithms focus on different aspects of the training speed.,A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1776,Introduction,['Efficiency'],"Some aim at
quickly obtaining a usable model, but some achieve
fast final convergence of solving the optimization problem in (1) or (4).",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1781,Introduction,"['Quantitative evidence (e.g. experiments)', 'Efficiency']","Experiments show that their approach more quickly
obtains a useful model than some of the above methods.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1783,Introduction,['Efficiency']," If one can efficiently solve
this sub-problem, then it can be a competitive optimization method. ",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1788,Introduction,"['Formal description/analysis', 'Theoretical guarantees']"," We prove that an _x000F_-optimal solution is obtained
in O(log(1/_x000F_)) iterations.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1789,Introduction,['Efficiency'],"We propose an implementation using a random order of sub-problems at each
iteration, which leads to very fast training.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1790,Introduction,"['Quantitative evidence (e.g. experiments)', 'Efficiency']","Experiments indicate that our method is more efficient than
the primal coordinate descent method.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1792,Introduction,['Applies to real world'],"However, in
practice one often has an easier access of values per instance.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1793,Introduction,['Simplicity']," Solving the dual takes this advantage, so our
implementation is simpler than Chang et al. (2007).",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1795,Introduction,['Large scale'],"However, they do not
focus on large data using the linear kernel.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1796,Introduction,['Large scale']," Crammer
and Singer (2003) proposed an online setting for multiclass SVM without considering large sparse data.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1798,Introduction,"['Large scale', 'Performance']"," In this paper, we point out that
dual coordinate descent methods make crucial advantage of the linear kernel and outperform other solvers
when the numbers of data and features are both large.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1800,Introduction,"['Large scale', 'Efficiency', 'Building on recent work']","In this paper, we show their key differences and explain why earlier studies on decomposition methods failed to modify their algorithms in an efficient
way like ours for large-scale linear SVM.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1806,Introduction,"['Quantitative evidence (e.g. experiments)', 'Large scale']","In Section 5, we compare our method with state of the art implementations for large linear SVM.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1807,Introduction,['Efficiency'],"Results show that the new method
is more efficient.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1808,Introduction,"['Formal description/analysis', 'Theoretical guarantees']","Proofs can be found at http://www.
csie.ntu.edu.tw/~cjlin/papers/cddual.pdf.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1809,Conclusion,['Flexibility/Extensibility'],"We can apply the proposed method to solve regularized least square problems, which have the loss function (1−yiwT xi)
2
in (1). ",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1810,Conclusion,"['Simplicity', 'Easy to implement']","The dual is simply (4) without constraints, so the implementation is simpler.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1811,Conclusion,"['Large scale', 'Efficiency']","In summary, we present and analyze an efficient dual
coordinate decent method for large linear SVM. ",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1812,Conclusion,"['Simplicity', 'Easy to implement']","It is
very simple to implement, and possesses sound optimization properties.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1813,Conclusion,"['State-of-the-art', 'Efficiency']","Experiments show that our
method is faster than state of the art implementations.",A Dual Coordinate Descent Method for Large-scale Linear SVM,http://icml2008.cs.helsinki.fi/papers/166.pdf
1814,Abstract,['Novelty'],"A new algorithm for training Restricted
Boltzmann Machines is introduced.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1816,Abstract,['Quantitative evidence (e.g. experiments)']," It is compared to
some standard Contrastive Divergence and
Pseudo-Likelihood algorithms on the tasks
of modeling and classifying various types of
data.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1817,Abstract,"['Simplicity', 'Performance', 'Efficiency']","The Persistent Contrastive Divergence
algorithm outperforms the other algorithms,
and is equally fast and simple.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1821,Introduction,['Used in practice/Popular'],"At present, the most popular gradient approximation is the Contrastive Divergence (CD) approximation (Hinton et al., 2006; Hinton, 2002; Bengio & Delalleau, 2007); more specifically the CD-1 approximation.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1824,Introduction,['Novelty'],"In this paper, a new gradient approximation algorithm
is presented and compared to a variety of CD-based algorithms.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1825,Introduction,['Quantitative evidence (e.g. experiments)'],"The quantitative measures of test data likelihood (for unsupervised learning) and classification
error rate (for supervised learning) are investigated,
and the type of feature detectors that are developed
are also shown.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1826,Introduction,"['Performance', 'Understanding (for researchers)']","We find that the new algorithm produces more meaningful feature detectors, and outperforms the other algorithms.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1828,Introduction,['Generalization'],"However, this special case can easily
be generalized to other harmoniums (Smolensky, 1986;
Welling et al., 2005) in which the units have Gaussian,
Poisson, multinomial, or other distributions in the exponential family, and the training algorithms described
here require only minor modifications to work in most
of those models.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1831,Introduction,['Quantitative evidence (e.g. experiments)'],"In
Sections 4 and 5, the experiments and results are described, and Section 6 concludes with a discussion and
some plans for future work.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1835,Conclusion,['Performance'],"For many tasks, however, large weights may be required for good performance, so strong weight decay is undesirable if it can
be avoided.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1836,Conclusion,['Performance'],"Also, the amount of training time used in these experiments is insufficient to find the asymptotic performance.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1838,Conclusion,['Performance'],"To find
out what its performance would be with more training
time is future work, but we have seen runs (with more training time and more hidden units) where as few
as 104 out of the 10,000 test cases were misclassified.
Clearly, this is worth investigating further.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1839,Conclusion,['Performance'],"Another issue suggesting future work is that the classification RBMs in these experiments were not trained
to maximize classification performance.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1840,Conclusion,['Accuracy']," They were
trained to accurately model the joint distribution over
images and labels.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1841,Conclusion,['Performance'],"It is possible to train classification
RBMs directly for classification performance; the gradient is fairly simple and certainly tractable.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1843,Conclusion,['Performance'],"However, in preliminary experiments we found that this
procedure begins to overfit very quickly (often after
improving performance by less than 0.1%), so we did
not include it in this paper.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1844,Conclusion,['Performance'],"It is, however, still possible that combining the classification gradient with the
density modeling gradient is a method that could yield
more improvements.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1847,Conclusion,"['Formal description/analysis', 'Theoretical guarantees']","A
theoretical analysis of this requirement can be found in
(Yuille, 2004) and (Younes, 1999).","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1848,Conclusion,['Efficiency'],"Some preliminary
experiments, however, suggest that PCD can be made
to work well even when the learning rate is much larger
than the one suggested by the asymptotic justification
of PCD and we are currently exploring variations that
allow much larger learning rates.","Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient",http://icml2008.cs.helsinki.fi/papers/638.pdf
1852,Abstract,['Performance'],"We present an
evaluation of different learning algorithms for
RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers.",Classification using Discriminative Restricted Boltzmann Machines,http://icml2008.cs.helsinki.fi/papers/601.pdf
1853,Abstract,['Simplicity'],"This
approach is simple in that RBMs are used
directly to build a classifier, rather than as a
stepping stone.",Classification using Discriminative Restricted Boltzmann Machines,http://icml2008.cs.helsinki.fi/papers/601.pdf
1855,Introduction,['Applies to real world'],"Restricted Boltzmann Machines (RBMs) (Smolensky,
1986) are generative models based on latent (usually
binary) variables to model an input distribution, and
have seen their applicability grow to a large variety
of problems and settings in the past few years.",Classification using Discriminative Restricted Boltzmann Machines,http://icml2008.cs.helsinki.fi/papers/601.pdf
1856,Introduction,['Flexibility/Extensibility'],"From
binary inputs, they have been extended to model various types of input distributions (Welling et al., 2005;
Hinton et al., 2006). ",Classification using Discriminative Restricted Boltzmann Machines,http://icml2008.cs.helsinki.fi/papers/601.pdf
1858,Introduction,['Successful'],"RBMs have been particularly successful in classification problems either as feature extractors for text and image data (Gehler et al., 2006) or as a good initial
training phase for deep neural network classifiers (Hinton, 2007).",Classification using Discriminative Restricted Boltzmann Machines,http://icml2008.cs.helsinki.fi/papers/601.pdf
1860,Introduction,"['Theoretical guarantees', 'Useful']","When
trained in an unsupervised fashion, RBMs provide no
guarantees that the features implemented by their hidden layer will ultimately be useful for the supervised
task that needs to be solved.",Classification using Discriminative Restricted Boltzmann Machines,http://icml2008.cs.helsinki.fi/papers/601.pdf
1867,Introduction,['Scales up'],"Finally, the algorithms investigated in
this paper are well suited for online learning on large
datasets.",Classification using Discriminative Restricted Boltzmann Machines,http://icml2008.cs.helsinki.fi/papers/601.pdf
1868,Conclusion,['Flexibility/Extensibility'],"We argued that RBMs can and should be used as
stand-alone non-linear classifiers alongside other standard and more popular classifiers, instead of merely
being considered as simple feature extractors.",Classification using Discriminative Restricted Boltzmann Machines,http://icml2008.cs.helsinki.fi/papers/601.pdf
1869,Conclusion,['Quantitative evidence (e.g. experiments)'],We evaluated different training objectives that are more appropriate to train an RBM in a classification setting.,Classification using Discriminative Restricted Boltzmann Machines,http://icml2008.cs.helsinki.fi/papers/601.pdf
1870,Conclusion,['Simplicity'],"These discriminative versions of RBMs integrate the
process of discovering features of inputs with their use
in classification, without relying on a separate classi-fier. ",Classification using Discriminative Restricted Boltzmann Machines,http://icml2008.cs.helsinki.fi/papers/601.pdf
1872,Conclusion,['Scales up'],"We also
presented a novel but straightforward semi-supervised
learning algorithm for RBMs and demonstrated its
usefulness for complex or high dimensional data.",Classification using Discriminative Restricted Boltzmann Machines,http://icml2008.cs.helsinki.fi/papers/601.pdf
1873,Conclusion,['Flexibility/Extensibility'],"For future work, we would like to investigate the use
of discriminative versions of RBMs in more challenging settings such as in multi-task or structured output problems.",Classification using Discriminative Restricted Boltzmann Machines,http://icml2008.cs.helsinki.fi/papers/601.pdf
1875,Conclusion,['Approximation'],"Exact computation of the conditional distribution for the target is not tractable anymore, but there exists promising techniques such as mean-field approximations that
could estimate that distribution.",Classification using Discriminative Restricted Boltzmann Machines,http://icml2008.cs.helsinki.fi/papers/601.pdf
1877,Conclusion,['Efficiency'],"Motivated by this observation, we intend to explore
ways to introduce generative learning in RBMs and
HDRBMs which would be less computationally expensive when the input vectors are large but sparse.",Classification using Discriminative Restricted Boltzmann Machines,http://icml2008.cs.helsinki.fi/papers/601.pdf
1881,Abstract,['Unifying ideas or integrating components'],"Previous methods on the problem typically adopt an inconsistent strategy:
feature extraction is performed in the Euclidean space while non-Euclidean distances
are used.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1882,Abstract,['Unifying ideas or integrating components'],"In our approach, we treat each subspace as a point in the Grassmann space, and perform feature extraction and classification in the same space.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1884,Abstract,"['Performance', 'State-of-the-art']","Experiments with
real image databases show that the proposed
method performs well compared with stateof-the-art algorithms.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1886,Introduction,['Robustness'],"Suppose we want to recognize a person from
multiple pictures of the individual, taken from different angles, under different illumination or at different
places.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1888,Introduction,['Approximation'],"In this paper, we specifically focus on those data that
can be modeled as a collection of linear subspaces. In
the example above, let’s assume that the set of images
of a single person is well approximated by a low dimensional subspace (Turk & Pentland, 1991), and the
whole data is the collection of such subspaces.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1889,Introduction,"['Robustness', 'Efficiency']","The
benefits of using subspaces are two-fold: 1) comparing two subspaces is cheaper than comparing two sets
directly when those sets are very large, and 2) it is
more robust to missing data since the subspace can
‘fill-in’ the missing pictures.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1892,Introduction,['Formal description/analysis'],"With this unifying framework we can
make analytic comparisons of the various distances of
subspaces.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1894,Introduction,['Unifying ideas or integrating components'],"The Grassmann kernels allow us to use the
usual kernel-based algorithms on this unconventional
space and to avoid ad hoc approaches to the problem.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1895,Introduction,"['Quantitative evidence (e.g. experiments)', 'Applies to real world']","We demonstrate the proposed framework by using the
Projection metric and the Binet-Cauchy metric and by
applying kernel Linear Discriminant Analysis to classification problems with real image databases.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1897,Introduction,['Formal description/analysis'],"In this work, we
provide an analytic exposition of the two metrics as
examples of the Grassmann kernels, and contrast the two metrics with other metrics used in the literature.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1899,Introduction,['Unifying ideas or integrating components'],"However, these methods adopt an inconsistent
strategy: feature extraction is performed in the Euclidean space when non-Euclidean distances are used.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1900,Introduction,"['Simplicity', 'Theoretical guarantees']","This inconsistency can result in complications and
weak guarantees.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1901,Introduction,"['Simplicity', 'Theoretical guarantees']","In our approach, the feature extraction and the distance measurement are integrated
around the Grassmann kernel, resulting in a simpler
and better-understood formulation.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1905,Introduction,"['Quantitative evidence (e.g. experiments)', 'Applies to real world']","In Sec. 6 we test the proposed algorithm for
face recognition and object categorization tasks.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1909,Conclusion,"['Formal description/analysis', 'Theoretical guarantees', 'Performance', 'State-of-the-art', 'Applies to real world']"," In addition to having
theoretically sound grounds, the proposed method also
outperformed state-of-the-art methods in two experiments with real data. ","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1910,Conclusion,['Understanding (for researchers)'],"As a future work, we are pursuing a better understanding of probabilistic distributions on the Grassmann manifold.","Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning",http://icml2008.cs.helsinki.fi/papers/312.pdf
1911,abstract,['Generality'],Clustering data in high dimensions is believed to be a hard problem in general. ,Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1912,abstract,"['Efficiency', 'Building on recent work']","A number of efficient clustering algorithms developed in recent years address this problem by projecting the data into a lowerdimensional subspace, e.g. via Principal Components Analysis (PCA) or random projections, before clustering. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1914,abstract,['Successful'],"Under the assumption that the views are uncorrelated given the cluster label, we show that the separation conditions required for the algorithm to be successful are significantly weaker than prior results in the literature. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1915,abstract,['Quantitative evidence (e.g. experiments)'],We provide results for mixtures of Gaussians and mixtures of log concave distributions. ,Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1916,abstract,['Quantitative evidence (e.g. experiments)'],We also provide empirical support from audio-visual speaker clustering (where we desire the clusters to correspond to speaker ID) and from hierarchical Wikipedia document clustering (where one view is the words in the document and the other is the link structure).,Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1917,introduction ,['Flexibility/Extensibility'],"The multi-view approach to learning is one in which we have ‘views’ of the data (sometimes in a rather abstract sense) and the goal is to use the relationship between these views to alleviate the difficulty of a learning problem of interest (Blum & Mitchell, 1998; Kakade & Foster, 2007; Ando & Zhang, 2007). ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1919,introduction ,['Understanding (for researchers)'],Much recent work has been done on understanding under what conditions we can learn a mixture model.,Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1921,introduction ,['Easy to work with'],"Under no restrictions on the underlying mixture, this problem is considered to be hard. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1923,introduction ,"['Efficiency', 'Building on recent work']","In fact, the focus of recent clustering algorithms (Dasgupta, 1999; Vempala & Wang, 2002; Achlioptas & McSherry, 2005; Brubaker & Vempala, 2008) is on efficiently learning with as little separation as possible. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1926,introduction ,['Applies to real world'],"There are many natural applications for which this assumption applies. For example, we can consider multi-modal views, with one view being a video stream and the other an audio stream, of a speaker — here, conditioned on the speaker identity and maybe the phoneme (both of which could label the generating cluster), the views may be uncorrelated. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1927,introduction ,"['Quantitative evidence (e.g. experiments)', 'Qualitative evidence (e.g. examples)']","A second example is the words and link structure in a document from a corpus such as Wikipedia – here, conditioned on the category of each document, the words in it and its link structure may be uncorrelated. In this paper, we provide experiments for both settings.",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1928,introduction ,"['Simplicity', 'Efficiency']","Under this multi-view assumption, we provide a simple and efficient subspace learning method, based on Canonical Correlation Analysis (CCA). ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1930,introduction ,['Approximation'],"The intuitive reason for this is that under our multi-view assumption, we are able to (approximately) find the low-dimensional subspace spanned by the means of the component distributions. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1931,introduction ,['Important'],"This subspace is important, because, when projected onto this subspace, the means of the distributions are well-separated, yet the typical distance between points from the same distribution is smaller than in the original space. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1932,introduction ,['Scales up'],"The number of samples we require to cluster correctly scales as O(d), where d is the ambient dimension. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1933,introduction ,"['Quantitative evidence (e.g. experiments)', 'Performance']","Finally, we show through experiments that CCA-based algorithms consistently provide better performance than standard PCA-based clustering methods when applied to datasets in the two quite different domains of audiovisual speaker clustering and hierarchical Wikipedia document clustering by category.",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1935,Related Work,"['Simplicity', 'Efficiency']",Related Work. Most provably efficient clustering algorithms first project the data down to some lowdimensional space and then cluster the data in this lower dimensional space (an algorithm such as single linkage usually suffices here). ,Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1937,Related Work,"['Efficiency', 'Building on recent work', 'Improvement']","One of the first provably efficient algorithms for learning mixture models is due to (Dasgupta, 1999), who learns a mixture of spherical Gaussians by randomly projecting the mixture onto a low-dimensional subspace. (Vempala & Wang, 2002) provide an algorithm with an improved separation requirement that learns a mixture of k spherical Gaussians, by projecting the mixture down to the k-dimensional subspace of highest variance.",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1938,Related Work,['Building on recent work'],"(Kannan et al., 2005; Achlioptas & McSherry, 2005) extend this result to mixtures of general Gaussians; however, they require a separation proportional to the maximum directional standard deviation of any mixture component. (Chaudhuri & Rao, 2008) use a canonical correlations-based algorithm to learn mixtures of axis aligned Gaussians with a separation proportional to σ* , the maximum directional standard deviation in the subspace containing the means of the distributions. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1939,Related Work,['Building on recent work'],"Their algorithm requires a coordinateindependence property, and an additional “spreading” condition. None of these algorithms are affine invariant.",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1940,Related Work,"['Generality', 'Building on recent work']","Finally, (Brubaker & Vempala, 2008) provide an affineinvariant algorithm for learning mixtures of general Gaussians, so long as the mixture has a suitably low Fisher coefficient when in isotropic position. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1941,Related Work,['Building on recent work'],"However, their separation involves a large polynomial dependence on 1 wmin. The two results most closely related to ours are the work of (Vempala & Wang, 2002) and (Chaudhuri & Rao, 2008). (Vempala & Wang, 2002) show that it is sufficient to find the subspace spanned by the means of the distributions in the mixture for effective clustering. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1944,Related Work,['Building on recent work'],"We borrow techniques from both of these papers. (Blaschko & Lampert, 2008) propose a similar algorithm for multi-view clustering, in which data is projected onto the top directions obtained by kernel CCA across the views. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1945,Related Work,"['Quantitative evidence (e.g. experiments)', 'Performance']","They show empirically that for clustering images using the associated text as a second view (where the target clustering is a human-defined category), CCA-based clustering methods out-perform PCA-based algorithms.",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1946,This Work,['Formal description/analysis'],"This Work. Our input is data on a fixed set of objects from two views, where View j is assumed to be generated by a mixture of k Gaussians (D j 1 , . . . , Dj k ), for j = 1, 2. To generate a sample, a source i is picked with probability wi, and x) and x(2) in Views 1 and 2are drawn from distributions D1 and D2.",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1947,This Work,['Building on classic work'],"Followin prior theoretical work, our goal is to show that our algorithm recovers the correct clustering, provided th input mixture obeys certain conditons. We impose two requirements on these mixtures. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1949,This Work,['Generality'],"Moreover, this condition allows the distributions in the mixture within each view to be completely general, so long as they are uncorrelated across views. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1950,This Work,['Robustness'],"Although we do not prove this, our algorithm seems robust to small deviations from this assumption. Second, we require the rank of the CCA matrix across the views to be at least k − 1, when each view is in isotropic position, and the k − 1-th singular value of this matrix to be at least λmin. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1953,This Work,['Large scale'],"Here µ 1 i is the mean of the i-th component in View 1 and σ ∗ is the maximum directional standard deviation in the subspace containing the means in View 1. Moreover, the number of samples required to learn this mixture grows (almost) linearly with d.",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1956,This Work,"['Building on recent work', 'Improvement']","To get our improved sample complexity bounds, we use a result due to (Rudelson & Vershynin, 2007) which may be of independent interest.",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1957,This Work,['Improvement'],We stress that our improved results are really due to the multi-view condition. ,Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1958,This Work,['Simplicity'],"Had we simply combined the data from both views, and applied previous algorithms on the combined data, we could not have obtained our guarantees. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1959,This Work,['Successful'],"We also emphasize that for our algorithm to cluster successfully, it is sufficient for the distributions in the mixture to obey the separation condition in one view, so long as the multi-view and rank conditions are obeyed.",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1960,This Work,"['Quantitative evidence (e.g. experiments)', 'Performance']","Finally, we study through experiments the performance of CCA-based algorithms on data sets from two different domains. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1962,This Work,"['Robustness', 'Performance']","Our experiments show that CCA-based algorithms perform better than PCAbased algorithms on audio data and just as well on image data, and are more robust to occlusions of the images. ",Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1964,This Work,['Performance'],Our experiments show that a CCAbased hierarchical clustering algorithm out-performs PCA-based hierarchical clustering for this data.,Multi-View Clustering via Canonical Correlation Analysis,https://dl.acm.org/doi/pdf/10.1145/1553374.1553391?casa_token=aNz0Wqwm8E0AAAAA:MdMCPQZZnRpr3523PDE07KVNiF-i8o4zuykkMjVmzRarLahr-ABLH689DHsMcJuBBzJLzPzLntT_5Q
1965,abstract,['Theoretical guarantees'],This paper presents a theoretical analysis of the problem of domain adaptation with multiple sources. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1966,abstract,['Formal description/analysis'],"For each source domain, the distribution over the input points as well as a hypothesis with error at most ǫ are given. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1967,abstract,['Scientific methodology'],The problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1968,abstract,['Theoretical guarantees'],We present several theoretical results relating to his problem. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1969,abstract,['Theoretical guarantees'],"In particular, we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that, instead, combinations weighted by the source distributions benefit from favorable theoretical guarantees.",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1970,abstract,['Quantitative evidence (e.g. experiments)'],"Our main result shows that, remarkably, for any fixed target function, there exists a distribution weighted combining rule that has a loss of at most ǫ with respect to any target mixture of the source distributions. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1971,abstract,['Generalization'],We further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3ǫ. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1972,abstract,"['Quantitative evidence (e.g. experiments)', 'Applies to real world']","Finally, we report empirical results for a multiple source adaptation problem with a real-world dataset.",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1973,introduction ,['Valid assumptions'],"A common assumption in theoretical models of learning such as the standard PAC model [16], as well as in the design of learning algorithms, is that training instances are drawn according to the same distribution as the unseen test examples. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1974,introduction ,['Valid assumptions'],"In practice, however, there are many cases where this assumption does not hold. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1975,introduction ,"['Generalization', 'Successful']","There can be no hope for generalization, of course, when the training and test distributions vastly differ, but when they are less dissimilar, learning can be more successful.",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1976,introduction ,['Data efficiency'],"A typical situation is that of domain adaptation where little or no labeled data is at one’s disposal for the target domain, but large amounts of labeled data from a source domain somewhat similar to the target, or hypotheses derived from that source, are available instead. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1977,introduction ,['Used in practice/Popular'],"This problem arises in a variety of applications in natural language processing [4, 7, 10], speech processing [8, 9, 11, 13–15], computer vision [12], and many other areas.",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1979,introduction ,['Qualitative evidence (e.g. examples)'],"An example is the problem of sentiment analysis which consists of classifying a text sample such as a movie review, restaurant rating, or discussion boards, or other web pages. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1981,introduction ,['Formal description/analysis'],"We will consider the following problem of multiple source adaptation. For each source i ∈ [1, k], the learner receives the distribution Di of the input points corresponding to that source as well as a hypothesis hi with loss at most ǫ on that source. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1986,introduction ,['Large scale'],"In practice, Di is estimated from large amounts of unlabeled points typically available from source i.",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1987,introduction ,['Data efficiency'],An alternative set-up for domain adaptation with multiple sources is one where the learner is not supplied with a good hypothesis hi for each source but where instead he has access to the labeled training data for each source domain. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1988,introduction ,['Label efficiency (reduced need for labeled data)'],A natural solution consists then of combining the raw labeled data from each source domain to form a new sample more representative of the target distribution and use that to train a learning algorithm. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1990,introduction ,"['Simplicity', 'Theoretical guarantees']","However, several empirical observations motivated our study of hypothesis combination, in addition to the theoretical simplicity and clarity of this framework.",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1992,introduction ,['Memory efficiency'],"This is because such models are typically obtained as a result of training based on many hours of speech with files occupying hundreds of gigabytes of disk space, while the models derived require orders of magnitude less space. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1994,introduction ,"['Low cost', 'Reduced training time']","Secondly, a combined data set can be substantially larger than each domain-specific data set, which can significantly increase the computational cost of training and make it prohibitive for some algorithms. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1996,introduction ,['Theoretical guarantees'],Few theoretical studies have been devoted to the problem of adaptation with multiple sources. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1997,introduction ,"['Building on classic work', 'Building on recent work']","BenDavid et al. [1] gave bounds for single source adaptation, then Blitzer et al. [3] extended the work to give a bound on the error rate of a hypothesis derived from a weighted combination of the source data sets for the specific case of empirical risk minimization. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
1998,introduction ,['Building on recent work'],"Crammer et al. [5, 6] also addressed a problem where multiple sources are present but the nature of the problem differs from adaptation since the distribution of the input points is the same for all these sources, only the labels change due to varying amounts of noise. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2000,introduction ,['Theoretical guarantees'],We present several theoretical results relating to this problem. We examine two types of hypothesis combination. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2001,introduction ,['Simplicity'],The first type is simply based on convex combinations of the k hypotheses hi. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2003,introduction ,"['Simplicity', 'Qualitative evidence (e.g. examples)']","Namely, we give a simple example of two distributions and two matching hypotheses each with zero error for their respective distribution, but such that any convex combination ha expected absolute loss of 1/2 for the equal mixture of the distributions. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2004,introduction ,['Identifying limitations'],This points out a potentiall significant weakness of a convex combination. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2008,introduction ,['Quantitative evidence (e.g. experiments)'],"Our main result shows that, remarkably, for any fixed target function, there exists a distribution weighted combining rule that has a loss of at most ǫ with respect to any mixture of the k distributions. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2009,introduction ,['Theoretical guarantees'],We also show that there exists a distribution weighted combining rule that has loss at most 3ǫ with respect to any consistent target function (one for which each hi has loss ǫ on Di) and any mixture of the k distributions. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2010,introduction ,['Theoretical guarantees'],"In some sense, our results establish that the distribution weighted hypothesis combination is the “right” combination rule, and that it also benefits from a well-founded theoretical guarantee.",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2013,introduction ,['Simplicity'],"In Section 4, we give a simple method to produce an error of Θ(kǫ) that does not require the prior knowledge of the mixture parameters of the target distribution. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2014,introduction ,['Performance'],"Our main results showing the existence of a combined hypothesis performing well regardless of the target mixture are given in Section 5 for the case of a fixed target function, and in Section 6 for the case of multiple target functions. ",Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2015,introduction ,['Quantitative evidence (e.g. experiments)'],Section 7 reports empirical results for a multiple source adaptation problem with a real-world dataset.,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2016,conclusion ,['Theoretical guarantees'],We presented a theoretical analysis of the problem of adaptation with multiple sources. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2017,conclusion ,"['Label efficiency (reduced need for labeled data)', 'Important']",Domain adaptation is an important problem that arises in a variety of modern applications where limited or no labeled data is available for a target application and our analysis can be relevant in a variety of situations. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2018,conclusion ,"['Theoretical guarantees', 'Quantitative evidence (e.g. experiments)']",The theoretical guarantees proven for the distribution weight combining rule provide it with a strong foundation. Its empirical performance with a real-world data set further motivates its use in applications. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2019,conclusion ,['Valid assumptions'],Much of the results presented were based on the assumption that the target distribution is some mixture of the source distributions. ,Domain Adaptation with Multiple Sources,https://papers.nips.cc/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf
2021,abstract,['Novelty'],This paper investigates a new machine learning strategy called translated learning. ,Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2022,abstract,['Flexibility/Extensibility'],"Unlike many previous learning tasks, we focus on how to use labeled data from one feature space to enhance the classification of other entirely different learning spaces. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2023,abstract,"['Qualitative evidence (e.g. examples)', 'Data efficiency']","For example, we might wish to use labeled text data to help learn a model for classifying image data, when the labeled images are difficult to obtain. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2024,abstract,['Unifying ideas or integrating components'],An important aspect of translated learning is to build a “bridge” to link one feature space (known as the “source space”) to another space (known as the “target space”) through a translator in order to migrate the knowledge from source to target. ,Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2028,abstract,"['Quantitative evidence (e.g. experiments)', 'Performance', 'State-of-the-art']","Through experiments on the text-aided image classification and cross-language classification tasks, we demonstrate that our translated learning framework can greatly outperform many state-of-the-art baseline methods.",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2029,introduction,['Large scale'],Traditional machine learning relies on the availability of a large amount of labeled data to train a model in the same feature space. ,Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2031,introduction,"['Data efficiency', 'Label efficiency (reduced need for labeled data)', 'Building on classic work']","In order to save much labeling work, various machine learning strategies have been proposed, including semi-supervised learning [13], transfer learning [3, 11, 10], self-taught learning [9], etc. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2033,introduction,"['Qualitative evidence (e.g. examples)', 'Data efficiency', 'Label efficiency (reduced need for labeled data)']","For example, if the training data are documents, then the classifiers cannot accept test data from a video space. However, in practice, we often face the problem where the labeled data are scarce in its own feature space, whereas there are sufficient labeled data in other feature spaces. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2034,introduction,"['Qualitative evidence (e.g. examples)', 'Data efficiency', 'Label efficiency (reduced need for labeled data)', 'Facilitating use (e.g. sharing code)']","For example, there may be few labeled images available, but there are often plenty of labeled text documents on the Web (e.g., through the Open Directory Project, or ODP, http://www.dmoz.org/).",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2035,introduction,['Qualitative evidence (e.g. examples)'],"Another example is cross-language classification where labeled documents in English are much more than ones in some other languages such as Bangla, which has only 21 Web pages in the ODP.",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2036,introduction,"['Data efficiency', 'Label efficiency (reduced need for labeled data)']","Therefore, it would be great if we could learn the knowledge across different feature spaces and to save a substantial labeling effort.",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2037,introduction,"['Flexibility/Extensibility', 'Building on classic work']","To address the transferring of knowledge across different feature spaces, researchers have proposed multi-view learning [2, 8, 7] in which each instance has multiple views in different feature spaces. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2040,introduction,"['Novelty', 'Formal description/analysis', 'Data efficiency']","To solve this novel learning problem, we develop a novel framework named as translated learning, where training data and test data can be in totally different feature spaces. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2042,introduction,['Generality'],"Clearly, the translated learning framework is more general and difficult than traditional learning problems. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2043,introduction,"['Quantitative evidence (e.g. experiments)', 'Qualitative evidence (e.g. examples)']","Figure 1 presents an intuitive illustration of six different learning strategies, including supervised learning, semi-supervised learning [13], transfer learning [10], self-taught learning [9], multi-view learning [2], and finally, translated learning.",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2044,introduction,"['Data efficiency', 'Label efficiency (reduced need for labeled data)']","An intuitive idea for translated learning is to somehow translate all the training data into a target feature space, where learning can be done within a single feature space. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2045,introduction,"['Successful', 'Building on classic work']",This idea has already been demonstrated successful in several applications in cross-lingual text classification [1]. ,Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2046,introduction,['Generality'],"However, for the more general translated learning problem, this idea is hard to be realized, since machine translation between different feature spaces is very difficult to accomplish in many non-natural language cases, such as translating documents to images. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2049,introduction,"['Quantitative evidence (e.g. experiments)', 'Data efficiency', 'Label efficiency (reduced need for labeled data)', 'Effectiveness']","While these data may not be sufficient in building a good classifier for the target domain, as we will demonstrate in our experimental study in the paper, by leveraging the available labeled data in the source domain, we can indeed build effective translators.",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2050,introduction,['Qualitative evidence (e.g. examples)'],An example is to translate between the text and image feature spaces using the social tagging data from Web sites such as Flickr (http://www.flickr.com/).,Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2051,introduction,['Unifying ideas or integrating components'],The main contribution of our work is to combine the feature translation and the nearest neighbor learning into a unified model by making use of a language model [5]. ,Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2052,introduction,['Formal description/analysis'],"Intuitively, our model can be represented using a Markov chain c → y → x, where y represents the features of the data instances x. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2053,introduction,['Formal description/analysis'],"In translated learning, the training data xs are represented by the features ys in the source feature space, while the test data xt are represented by the features yt in the target feature space. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2054,introduction,['Formal description/analysis'],"We model the learning in the source space through a Markov chain c → ys → xs, which can be connected to another Markov chain c → yt → xt in the target space. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2055,introduction,"['Formal description/analysis', 'Unifying ideas or integrating components', 'Important']","An important contribution of our work then is to show how to connect these two paths, so that the new chain c → ys → yt → xt, can be used to translate the knowledge from the source space to the target one, where the mapping ys → yt is acting as a feature-level translator. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2058,conclusion,['Data efficiency'],"In this paper, we proposed a translated learning framework for classifying target data using data from another feature space. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2059,conclusion,"['Performance', 'Data efficiency', 'Label efficiency (reduced need for labeled data)', 'Unifying ideas or integrating components']","We have shown that in translated learning, even though we have very little labeled data in the target space, if we can find a bridge to link the two spaces through feature translation, we can achieve good performance by leveraging the knowledge from the source data.",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2060,conclusion,"['Formal description/analysis', 'Approximation', 'Quantitative evidence (e.g. experiments)', 'Effectiveness']","We formally formulated our translated learning framework using risk minimization, and presented an approximation method for model estimation. In our experiments, we have demonstrated how this can be done effectively through the co-occurrence data in TLRisk. ",Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2061,conclusion,"['Quantitative evidence (e.g. experiments)', 'Performance', 'State-of-the-art']",The experimental results on the text-aided image classification and the cross-language classification show that our algorithm can greatly outperform the state-of-the-art baseline methods.,Translated Learning: Transfer Learning across Different Feature Spaces,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5832&rep=rep1&type=pdf
2063,abstract,['Used in practice/Popular'],"Such formulation finds applications in many machine learning tasks including multi-task learning, matrix classification, and matrix completion. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2065,abstract,['Optimal'],"In addition, due to the non-smooth nature of the trace norm, the optimal first-order black-box method for solving such class of problems converges as O( √ 1 k ), where k is the iteration counter. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2067,abstract,['Optimal'],"We further propose an accelerated gradient algorithm, which achieves the optimal convergence rate of O( 1/k2) for smooth problems. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2068,abstract,"['Quantitative evidence (e.g. experiments)', 'Efficiency']",Experiments on multi-task learning problems demonstrate the efficiency of the proposed algorithms.,An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2069,introduction ,['Used in practice/Popular'],"The problem of minimizing the rank of a matrix variable subject to certain constraints arises in many fields including machine learning, automatic control, and image compression. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2070,introduction ,['Qualitative evidence (e.g. examples)'],"For example, in collaborative filtering we are given a partially filled rating matrix and the task is to predict the missing entries. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2071,introduction ,['Approximation'],"Since it is commonly believed that only a few factors contribute to an individual’s tastes, it is natural to approximate the given rating matrix by a low-rank matrix. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2072,introduction ,['Generality'],"However, the matrix rank minimization problem is NPhard in general due to the combinatorial nature of the rank function. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2073,introduction ,"['Building on classic work', 'Used in practice/Popular']","A commonly-used convex relaxation of the rank function is the trace norm (nuclear norm) (Fazel et al., 2001), defined as the sum of the singular values of the matrix, since it is the convex envelope of the rank function over the unit ball of spectral norm. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2074,introduction ,['Building on recent work'],"A number of recent work has shown that the low rank solution can be recovered exactly via minimizing the trace norm under certain conditions (Recht et al., 2008a; Recht et al., 2008b; Cand´es & Recht, 2008).",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2075,introduction ,['Used in practice/Popular'],"In practice, the trace norm relaxation has been shown to yield low-rank solutions and it has been used widely in many scenarios. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2076,introduction ,['Building on classic work'],"In (Srebro et al., 2005; Rennie & Srebro, 2005; Weimer et al., 2008a; Cai et al., 2008; Ma et al., 2008) the matrix completion problem was formulated as a trace norm minimization problem. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2077,introduction ,['Unifying ideas or integrating components'],"In problems where multiple related tasks are learned simultaneously, the models for different tasks can be constrained to share certain information. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2078,introduction ,['Building on recent work'],"Recently, this constraint has been expressed as the trace norm regularization on the weight matrix in the context of multitask learning (Abernethy et al., 2006; Argyriou et al., 2008; Abernethy et al., 2009; Obozinski et al., 2009), multi-class classification (Amit et al., 2007), and multivariate linear regression (Yuan et al., 2007; Lu et al., 2008). ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2079,introduction ,['Data efficiency'],"For two-dimensional data such as images, the matrix classification formulation (Tomioka & Aihara, 2007; Bach, 2008) applies a weight matrix, regularized by its trace norm, on the data. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2080,introduction ,"['Performance', 'Improvement']","It was shown (Tomioka & Aihara, 2007) that such formulation leads to improved performance over conventional methods.",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2081,introduction ,"['Efficiency', 'Practical', 'Optimal']",A practical challenge in employing the trace norm regularization is to develop efficient algorithms to solve the resulting non-smooth optimization problems. ,An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2083,introduction ,"['Building on classic work', 'Identifying limitations']","However, such formulation is computationally expensive. To overcome this limitation, a number of algorithms have been developed recently (Rennie & Srebro, 2005; Weimer et al., 2008a; Weimer et al., 2008b; Cai et al., 2008; Ma et al., 2008). ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2084,introduction ,['Approximation'],In these algorithms some form of approximation is usually employed to deal with the non-smooth trace norm term. ,An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2085,introduction ,['Fast'],"However, a fast global convergence rate for these algorithms is difficult to guarantee.",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2086,introduction ,"['Simplicity', 'Formal description/analysis']","Due to the non-smooth nature of the trace norm, a simple approach to solve these problems is the subgradient method (Bertsekas, 1999; Nesterov, 2003), which converges as O( √ 1 k)  where k is the iteration counter.",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2087,introduction ,"['Building on classic work', 'Optimal']","It is known from the complexity theory of convex optimization (Nemirovsky & Yudin, 1983; Nesterov, 2003 that this convergence rate is already optimal for nonsmooth optimization under the first-order black-bo model, where only the function values and first-orde derivatives are used",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2088,introduction ,"['Efficiency', 'Fast']",In this paper we propose efficient algorithms with fast global convergence rates to solve trace norm regularized problems. ,An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2091,introduction ,"['Building on classic work', 'Optimal']","Following the Nesterov’s method for accelerating the gradient method (Nesterov, 1983; Nesterov, 2003), we show that the extended gradient algorithm can be further accelerated to converge as O( 1 k2 ), which is the optimal convergence rate for smooth problems. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2092,introduction ,['Effectiveness'],"Hence, the nonsmoothness effect of the trace norm regularization is effectively removed. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2093,introduction ,['Flexibility/Extensibility'],"The proposed algorithms extend the algorithms in (Nesterov, 2007; Tseng, 2008; Beck & Teboulle, 2009) to the matrix case. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2094,introduction ,"['Quantitative evidence (e.g. experiments)', 'Efficiency']",Experiments on multi-task learning problems demonstrate the efficiency of the proposed algorithms in comparison with existing ones. ,An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2096,conclusion ,['Efficiency'],In this paper we propose efficient algorithms to solve trace norm regularized problems. ,An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2097,conclusion ,"['Generality', 'Improvement', 'Optimal']","We show that by exploiting the special structure of the trace norm, the optimal convergence rate of O( √ 1 k ) for general nonsmooth problems can be improved to O(1k ). ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2068,conclusion ,"['Quantitative evidence (e.g. experiments)', 'Efficiency']",Experiments on multi-task learning problems demonstrate the efficiency of the proposed algorithms.,An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2099,conclusion ,['Important'],"As pointed out in the paper, another important application of the trace norm regularization is in matrix completion problems. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2101,conclusion ,['Large scale'],"The proposed algorithms require the computation of SVD, which may be computationally expensive for largescale problems. ",An accelerated gradient method for trace norm minimization,https://dl.acm.org/doi/pdf/10.1145/1553374.1553434
2103,abstract,"['Building on classic work', 'Useful']",Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. ,Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2104,abstract,"['Novelty', 'Robustness']",We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. ,Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2107,abstract,['Quantitative evidence (e.g. experiments)'],Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.,Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2108,introduction,"['Generalization', 'Performance', 'Efficiency', 'Building on recent work']","Recent theoretical studies indicate that deep architectures (Bengio & Le Cun, 2007; Bengio, 2007) may be needed to efficiently model complex distributions and achieve better generalization performance on challenging recognition tasks. ",Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2109,introduction,['Powerful'],"The belief that additional levels of functional composition will yield increased representational and modeling power is not new (McClelland et al., 1986; Hinton, 1989; Utgoff & Stracuzzi, 2002). ",Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2112,introduction,"['Building on classic work', 'Optimal']","Also looking back at the history of multi-layer neural networks, their difficult optimization (Bengio et al., 2007; Bengio, 2007) has long prevented reaping the expected benefits of going beyond one or two hidden layers. ",Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2113,introduction,['Successful'],"However this situation has recently changed with the successful approach of (Hinton et al., 2006; Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Ranzato et al., 2007; Lee et al., 2008) for training Deep Belief Networks and stacked autoencoders.",Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2114,introduction,"['Performance', 'Successful', 'Optimal']","One key ingredient to this success appears to be the use of an unsupervised training criterion to perform a layer-by-layer initialization: each layer is at first trained to produce a higher level (hidden) representation of the observed patterns, based on the representation it receives as input from the layer below, by optimizing a local unsupervised criterion. ",Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2116,introduction,['Performance'],"This initialization yields a starting point, from which a global fine-tuning of the model’s parameters is then performed using another training criterion appropriate for the task at hand. ",Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2117,introduction,['Scientific methodology'],This technique has been shown empirically to avoid getting stuck in the kind of poor solutions one typically reaches with random initializations. ,Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2118,introduction,['Understanding (for researchers)'],"While unsupervised learning of a mapping that produces “good” intermediate representations of the input pattern seems to be key, little is understood regarding what constitutes “good” representations for initializing deep architectures, or what explicit criteria may guide learning such representations. ",Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2122,introduction,['Building on recent work'],"A supplemental criterion that has been proposed for such models is sparsity of the representation (Ranzato et al., 2008; Lee et al., 2008).",Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2123,introduction,['Robustness'],"Here we hypothesize and investigate an additional specific criterion: robustness to partial destruction of the input, i.e., partially destroyed inputs should yield almost the same representation. ",Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2127,introduction,['Learning from humans'],A hallmark of this is our human ability to recognize partially occluded or corrupted images. ,Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2128,introduction,['Learning from humans'],Further evidence is our ability to form a high level concept associated to multiple modalities (such as image and sound) and recall it even when some of the modalities are missing.,Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2129,introduction,"['Robustness', 'Scientific methodology', 'Unifying ideas or integrating components', 'Useful']","To validate our hypothesis and assess its usefulness as one of the guiding principles in learning deep architectures, we propose a modification to the autoencoder framework to explicitly integrate robustness to partially destroyed inputs. ",Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2130,introduction,['Building on classic work'],Section 2 describes the algorithm in details. Section 3 discusses links with other approaches in the literature. ,Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2131,introduction,['Theoretical guarantees'],Section 4 is devoted to a closer inspection of the model from different theoretical standpoints. ,Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2132,introduction,"['Quantitative evidence (e.g. experiments)', 'Performance']",In section 5 we verify empirically if the algorithm leads to a difference in performance. Section 6 concludes the study.,Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2133,conclusion,['Simplicity'],"We have introduced a very simple training principle for autoencoders, based on the objective of undoing a corruption process. ",Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2134,conclusion,['Robustness'],This is motivated by the goal of learning representations of the input that are robust to small irrelevant changes in input. ,Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2135,conclusion,['Understanding (for researchers)'],We also motivated it from a manifold learning perspective and gave an interpretation from a generative model perspective.,Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2137,conclusion,"['Novelty', 'Quantitative evidence (e.g. experiments)']",A series of image classification experiments were performed to evaluate this new training principle. ,Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2138,conclusion,['Quantitative evidence (e.g. experiments)'],The empirical results support the following conclusions: unsupervised initialization of layers with an explicit denoising criterion helps to capture interesting structure in the input distribution. ,Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2140,conclusion,"['Robustness', 'Quantitative evidence (e.g. experiments)', 'Performance']","It is possible that the rather good experimental performance of Deep Belief Networks (whose layers are initialized as RBMs) is partly due to RBMs encapsulating a similar form of robustness to corruption in the representations they learn, possibly because of their stochastic nature which introduces noise in the representation during training. ",Extracting and Composing Robust Features with Denoising Autoencoders,http://icml2008.cs.helsinki.fi/papers/592.pdf
2143,Abstract,"['Flexibility/Extensibility', 'Efficiency', 'Successful']","Efficient greedy algorithms for learning
and approximate inference have allowed these
models to be applied successfully in many application domains.",On the Quantitative Analysis of Deep Belief Networks,http://icml2008.cs.helsinki.fi/papers/573.pdf
2146,Abstract,"['Novelty', 'Efficiency']"," We show that Annealed Importance
Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and
we present a novel AIS scheme for comparing
RBM’s with different architectures.",On the Quantitative Analysis of Deep Belief Networks,http://icml2008.cs.helsinki.fi/papers/573.pdf
2148,Abstract,"['Novelty', 'Quantitative evidence (e.g. experiments)']","This is, to our knowledge, the first step
towards obtaining quantitative results that would
allow us to directly assess the performance of
Deep Belief Networks as generative models of
data",On the Quantitative Analysis of Deep Belief Networks,http://icml2008.cs.helsinki.fi/papers/573.pdf
2151,Intro,['Efficiency'],"The
learning procedure also provides an efficient way of performing approximate inference, which makes the values of the latent variables in the deepest layer easy to infer",On the Quantitative Analysis of Deep Belief Networks,http://icml2008.cs.helsinki.fi/papers/573.pdf
2152,Intro,"['Flexibility/Extensibility', 'Successful']","These
deep generative models have been successfully applied in
many application domains (Hinton & Salakhutdinov, 2006;
Bengio & LeCun, 2007).",On the Quantitative Analysis of Deep Belief Networks,http://icml2008.cs.helsinki.fi/papers/573.pdf
2154,Intro,"['Flexibility/Extensibility', 'Successful']","RBM’s, and their generalizations to exponential
family models, have been successfully applied in collaborative filtering (Salakhutdinov et al., 2007), information
and image retrieval (Gehler et al., 2006), and time series
modeling (Taylor et al., 2006)",On the Quantitative Analysis of Deep Belief Networks,http://icml2008.cs.helsinki.fi/papers/573.pdf
2157,Intro,['Generalization'],"A good estimate of the partition function would be extremely helpful
for model selection and for controlling model complexity,
which are important for making RBM’s generalize well.",On the Quantitative Analysis of Deep Belief Networks,http://icml2008.cs.helsinki.fi/papers/573.pdf
2160,Intro,['Performance'],"However, for densely connected MRF’s,
such as RBM’s, these methods are unlikely to perform
well.",On the Quantitative Analysis of Deep Belief Networks,http://icml2008.cs.helsinki.fi/papers/573.pdf
2162,Intro,['Efficiency']," In this paper we show how
one such method, AIS, by taking advantage of the bipartite
structure of an RBM, can be used to efficiently estimate
its partition function. ",On the Quantitative Analysis of Deep Belief Networks,http://icml2008.cs.helsinki.fi/papers/573.pdf
2169,Discussion,['Quantitative evidence (e.g. experiments)'],"Furthermore, we showed that this estimator, along with approximate inference, can be used to obtain an estimate of the
lower bound on the log-probability of the test data, thus allowing us to obtain some quantitative evaluation of the generalization performance of these deep hierarchical models",On the Quantitative Analysis of Deep Belief Networks,http://icml2008.cs.helsinki.fi/papers/573.pdf
2176,Discussion,['Generalization'],"We find AIS and other stochastic methods attractive as they
can just as easily be applied to undirected graphical models
that generalize RBM’s and DBN’s to exponential family
distributions. ",On the Quantitative Analysis of Deep Belief Networks,http://icml2008.cs.helsinki.fi/papers/573.pdf
2177,Discussion,"['Flexibility/Extensibility', 'Applies to real world']","This will allow future application to models of real-valued data, such as image patches (Osindero &
Hinton, 2008), or count data (Gehler et al., 2006)",On the Quantitative Analysis of Deep Belief Networks,http://icml2008.cs.helsinki.fi/papers/573.pdf
2180,Discussion,"['Accuracy', 'Applies to real world']","Indeed, preliminary
results suggest that these methods provide quite inaccurate
estimates of (or very loose upper bounds on) the partition
function, even for small RBM’s when trained on real data.",On the Quantitative Analysis of Deep Belief Networks,http://icml2008.cs.helsinki.fi/papers/573.pdf
2181,Abstract,"['Scientific methodology', 'Effectiveness', 'Practical']","Empirical evidence suggests that hashing is an
effective strategy for dimensionality reduction
and practical nonparametric estimation. ",Feature hashing for large scale multitask learning,https://arxiv.org/abs/0902.2206
2183,Abstract,"['Quantitative evidence (e.g. experiments)', 'Large scale']","We demonstrate the feasibility of
this approach with experimental results for a new
use case — multitask learning with hundreds of
thousands of tasks",Feature hashing for large scale multitask learning,https://arxiv.org/abs/0902.2206
2192,Intro,['Memory efficiency'],"Instead,
limited memory makes storing a kernel matrix infeasible.",Feature hashing for large scale multitask learning,https://arxiv.org/abs/0902.2206
2193,Intro,['Building on recent work'],"For this common scenario several authors have recently
proposed an alternative, but highly complimentary variation of the kernel-trick, which we refer to as the
hashing-trick: one hashes the high dimensional input vectors x into a lower dimensional feature space Rm with
φ : X → Rm (Langford et al., 2007; Shi et al., 2009).",Feature hashing for large scale multitask learning,https://arxiv.org/abs/0902.2206
2196,Intro,['Novelty'],"To our knowledge, we are the first to provide exponential
tail bounds on the canonical distortion of these hashed inner
products. ",Feature hashing for large scale multitask learning,https://arxiv.org/abs/0902.2206
2203,Intro,['Flexibility/Extensibility'],"1. In section 2 we introduce specialized hash functions with unbiased inner-products that are directly applicable to a large
variety of kernel-methods. ",Feature hashing for large scale multitask learning,https://arxiv.org/abs/0902.2206
2204,Intro,['Understanding (for researchers)']," 2. In section 3 we provide exponential tail bounds that help explain why hashed feature vectors have repeatedly lead to, at times surprisingly,
strong empirical results",Feature hashing for large scale multitask learning,https://arxiv.org/abs/0902.2206
2205,Intro,"['Flexibility/Extensibility', 'Large scale']"," 3. Also in section 3 we show that
the interference between independently hashed subspaces
is negligible with high probability, which allows large-scale
multi-task learning in a very compressed space",Feature hashing for large scale multitask learning,https://arxiv.org/abs/0902.2206
2206,Intro,"['Novelty', 'Quantitative evidence (e.g. experiments)', 'Large scale', 'Applies to real world']","4. In section 5 we introduce collaborative email-spam filtering as a
novel application for hash representations and provide experimental results on large-scale real-world spam data sets",Feature hashing for large scale multitask learning,https://arxiv.org/abs/0902.2206
2208,Abstract,"['Realistic output', 'Successful']","The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for
sequences that is able to successfully model (i.e., generate nice-looking samples
of) several very high dimensional sequences, such as motion capture data and the
pixels of low resolution videos of balls bouncing in a box. ",The Recurrent Temporal Restricted Boltzmann Machine,https://proceedings.neurips.cc/paper/2008/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html
2210,Abstract,"['Accuracy', 'Successful']","This difficulty has necessitated the use of a heuristic inference procedure, that
nonetheless was accurate enough for successful learning. ",The Recurrent Temporal Restricted Boltzmann Machine,https://proceedings.neurips.cc/paper/2008/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html
2213,Intro,['Applies to real world'],"Modeling sequences is an important problem since there is a vast amount of natural data, such as
speech and videos, that is inherently sequential",The Recurrent Temporal Restricted Boltzmann Machine,https://proceedings.neurips.cc/paper/2008/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html
2214,Intro,['Useful']," A good model for these data sources could be useful
for finding an abstract representation that is helpful for solving “natural” discrimination tasks (see
[4] for an example of this approach for the non-sequential case).",The Recurrent Temporal Restricted Boltzmann Machine,https://proceedings.neurips.cc/paper/2008/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html
2215,Intro,['Useful'],"In addition, it could be also used
for predicting the future of a sequence from its past, be used as a prior for denoising tasks, and be
used for other applications such as tracking objects in video.",The Recurrent Temporal Restricted Boltzmann Machine,https://proceedings.neurips.cc/paper/2008/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html
2216,Intro,['Accuracy'],"The Temporal Restricted Boltzmann
Machine [14, 13] is a recently introduced probabilistic model that has the ability to accurately model
complex probability distributions over high-dimensional sequences.",The Recurrent Temporal Restricted Boltzmann Machine,https://proceedings.neurips.cc/paper/2008/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html
2217,Intro,['Realistic output'],"It was shown to be able to
generate realistic motion capture data [14], and low resolution videos of 2 balls bouncing in a box
[13], as well as complete and denoise such sequences.",The Recurrent Temporal Restricted Boltzmann Machine,https://proceedings.neurips.cc/paper/2008/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html
2225,Intro,['Realistic output'],"We demonstrate that the RTRBM is able to generate more
realistic samples than an equivalent TRBM for the motion capture data and for the pixels of videos of bouncing balls. ",The Recurrent Temporal Restricted Boltzmann Machine,https://proceedings.neurips.cc/paper/2008/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html
2226,Intro,['Performance'],"The RTRBM’s performance is better than the TRBM mainly because it learns to
convey more information through its hidden-to-hidden connections.",The Recurrent Temporal Restricted Boltzmann Machine,https://proceedings.neurips.cc/paper/2008/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html
2230,Abstract,['Privacy'],"This paper addresses the important tradeoff between privacy and learnability,
when designing algorithms for learning from private databases.",Privacy-preserving logistic regression,https://proceedings.neurips.cc/paper/2008/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf
2235,Abstract,['Privacy'],We prove that our algorithm preserves privacy in the model due to [6].,Privacy-preserving logistic regression,https://proceedings.neurips.cc/paper/2008/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf
2236,Abstract,['Theoretical guarantees'],"We provide learning guarantees for both algorithms, which are tighter for our new
algorithm, in cases in which one would typically apply logistic regression.",Privacy-preserving logistic regression,https://proceedings.neurips.cc/paper/2008/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf
2237,Abstract,"['Quantitative evidence (e.g. experiments)', 'Performance']","Experiments demonstrate improved learning performance of our method, versus the
sensitivity method.",Privacy-preserving logistic regression,https://proceedings.neurips.cc/paper/2008/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf
2238,Abstract,['Flexibility/Extensibility'],"Our privacy-preserving technique does not depend on the sensitivity of the function, and extends easily to a class of convex loss functions",Privacy-preserving logistic regression,https://proceedings.neurips.cc/paper/2008/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf
2239,Abstract,['Understanding (for researchers)'],"Our
work also reveals an interesting connection between regularization and privacy.",Privacy-preserving logistic regression,https://proceedings.neurips.cc/paper/2008/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf
2240,Intro,"['Applies to real world', 'Privacy']","Privacy-preserving machine learning is an emerging problem, due in part to the increased reliance on
the internet for day-to-day tasks such as banking, shopping, and social networking.",Privacy-preserving logistic regression,https://proceedings.neurips.cc/paper/2008/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf
2245,Intro,['Novelty'],"We design a new spectral norm that encodes this a priori assumption, without the prior knowledge of the partition of tasks into groups, resulting
in a new convex optimization formulation for multi-task learning. ",Clustered Multi-Task Learning: a Convex Formulation,http://papers.nips.cc/paper/3499-clustered-multi-task-learning-a-convex-formulation.pdf
2246,Intro,"['Quantitative evidence (e.g. experiments)', 'Performance', 'Building on classic work']","We show in
simulations on synthetic examples and on the IEDB MHC-I binding dataset, that
our approach outperforms well-known convex methods for multi-task learning, as
well as related non-convex methods dedicated to the same problem.",Clustered Multi-Task Learning: a Convex Formulation,http://papers.nips.cc/paper/3499-clustered-multi-task-learning-a-convex-formulation.pdf
2247,Intro,['Building on classic work'],"Regularization has emerged as a dominant theme in machine learning and statistics, providing an
intuitive and principled tool for learning from high-dimensional data. ",Clustered Multi-Task Learning: a Convex Formulation,http://papers.nips.cc/paper/3499-clustered-multi-task-learning-a-convex-formulation.pdf
2248,Intro,"['Efficiency', 'Building on classic work', 'Understanding (for researchers)', 'Practical']"," In particular, regularization
by squared Euclidean norms or squared Hilbert norms has been thoroughly studied in various settings, leading to efficient practical algorithms based on linear algebra, and to very good theoretical
understanding (see, e.g., [1, 2]).",Clustered Multi-Task Learning: a Convex Formulation,http://papers.nips.cc/paper/3499-clustered-multi-task-learning-a-convex-formulation.pdf
2249,Intro,['Building on recent work'],"In recent years, regularization by non Hilbert norms, such as ℓp
norms with p 6= 2, has also generated considerable interest for the inference of linear functions in
supervised classification or regression.",Clustered Multi-Task Learning: a Convex Formulation,http://papers.nips.cc/paper/3499-clustered-multi-task-learning-a-convex-formulation.pdf
2254,Intro,"['Promising', 'Building on recent work']","More precisely, we consider the problem of multi-task learning, which has recently emerged as a
very promising research direction for various applications [4]",Clustered Multi-Task Learning: a Convex Formulation,http://papers.nips.cc/paper/3499-clustered-multi-task-learning-a-convex-formulation.pdf
2262,Intro,"['Performance', 'Understanding (for researchers)']","Besides an improved performance if
the hypothesis turns out to be correct, we also expect this approach to be able to identify the cluster
structure among the tasks as a by-product of the inference step, e.g., to identify outliers or groups of
customers, which can be of interest for further understanding of the structure of the problem.",Clustered Multi-Task Learning: a Convex Formulation,http://papers.nips.cc/paper/3499-clustered-multi-task-learning-a-convex-formulation.pdf
2265,Intro,['Useful'],"We then attempt to optimize the
objective function of the inference algorithm over the set of partitions, a strategy that has proved
useful in other contexts such as multiple kernel learning [8]. ",Clustered Multi-Task Learning: a Convex Formulation,http://papers.nips.cc/paper/3499-clustered-multi-task-learning-a-convex-formulation.pdf
2266,Intro,['Efficiency'],"This optimization problem over the
set of partitions being computationally challenging, we propose a convex relaxation of the problem
which results in an efficient algorithm.",Clustered Multi-Task Learning: a Convex Formulation,http://papers.nips.cc/paper/3499-clustered-multi-task-learning-a-convex-formulation.pdf
2268,Conclusion,"['Quantitative evidence (e.g. experiments)', 'Promising']",Promising results were presented on synthetic examples and on the IEDB dataset.,Clustered Multi-Task Learning: a Convex Formulation,http://papers.nips.cc/paper/3499-clustered-multi-task-learning-a-convex-formulation.pdf
2269,Conclusion,['Performance'],"We are currently investigating more refined convex relaxations and the natural extension to nonlinear multi-task learning as well as the inclusion of specific features on the tasks, which has shown
to improve performance in other settings [6]",Clustered Multi-Task Learning: a Convex Formulation,http://papers.nips.cc/paper/3499-clustered-multi-task-learning-a-convex-formulation.pdf
2270,Abstract,['Formal description/analysis'],"We present a large-margin formulation and
algorithm for structured output prediction
that allows the use of latent variables.",Learning Structural SVMs with Latent Variables,https://www.cs.cornell.edu/people/tj/publications/yu_joachims_09a.pdf
2271,Abstract,"['Flexibility/Extensibility', 'Efficiency']","Our proposal covers a large range of application problems, with an optimization problem
that can be solved efficiently using ConcaveConvex Programming.",Learning Structural SVMs with Latent Variables,https://www.cs.cornell.edu/people/tj/publications/yu_joachims_09a.pdf
2272,Abstract,"['Generality', 'Performance']","The generality and performance of the approach is demonstrated
through three applications including motiffinding, noun-phrase coreference resolution,
and optimizing precision at k in information retrieval",Learning Structural SVMs with Latent Variables,https://www.cs.cornell.edu/people/tj/publications/yu_joachims_09a.pdf
2273,Intro,['Useful'],"In many structured prediction tasks, there is useful
modeling information that is not available as part of
the training data (x1, y1), ..., (xn, yn)",Learning Structural SVMs with Latent Variables,https://www.cs.cornell.edu/people/tj/publications/yu_joachims_09a.pdf
2280,Intro,['Building on recent work'],"Recently, there has been some work on Conditional Random Fields (Wang et al., 2006) with latent variables.",Learning Structural SVMs with Latent Variables,https://www.cs.cornell.edu/people/tj/publications/yu_joachims_09a.pdf
2284,Intro,['Generality'],"We identify a particular, yet rather general, formulation
for which there exists an efficient algorithm to find a
local optimum using the Concave-Convex Procedure.",Learning Structural SVMs with Latent Variables,https://www.cs.cornell.edu/people/tj/publications/yu_joachims_09a.pdf
2286,Intro,"['Qualitative evidence (e.g. examples)', 'Generality']","To illustrate the generality of our Latent Structural
SVM algorithm, we provide experimental results on
three different applications in computational biology,
natural language processing, and information retrieval.",Learning Structural SVMs with Latent Variables,https://www.cs.cornell.edu/people/tj/publications/yu_joachims_09a.pdf
2292,Intro,"['Generality', 'Building on recent work']","The Concave-Convex Procedure (Yuille & Rangarajan, 2003) employed in our work is a general framework for minimizing non-convex functions which falls
into the class of DC (Difference of Convex) programming",Learning Structural SVMs with Latent Variables,https://www.cs.cornell.edu/people/tj/publications/yu_joachims_09a.pdf
2293,Intro,['Flexibility/Extensibility'],"In recent years there have been numerous applications of the algorithm in machine learning, including training non-convex SVMs and transductive SVMs
(Collobert et al., 2006). ",Learning Structural SVMs with Latent Variables,https://www.cs.cornell.edu/people/tj/publications/yu_joachims_09a.pdf
2300,Conclusions,['Formal description/analysis'],"We have presented a framework and formulation for
learning Structural SVMs with latent variables.",Learning Structural SVMs with Latent Variables,https://www.cs.cornell.edu/people/tj/publications/yu_joachims_09a.pdf
2301,Conclusions,"['Flexibility/Extensibility', 'Efficiency']","We identify a particular case that covers a wide range of
application problems, yet affords an efficient training
algorithms using Convex-Concave Programming.",Learning Structural SVMs with Latent Variables,https://www.cs.cornell.edu/people/tj/publications/yu_joachims_09a.pdf
2303,Conclusions,['Generality'],"We demonstrated the generality of the Latent
Structural SVM with three applications, and a future
research direction will be to explore further applications of this algorithm in different domains.",Learning Structural SVMs with Latent Variables,https://www.cs.cornell.edu/people/tj/publications/yu_joachims_09a.pdf
2304,Abstract,['Novelty'],"We propose a new penalty function which, when
used as regularization for empirical risk minimization procedures, leads to sparse estimators.",Group Lasso with Overlap and Graph Lasso,http://imagine.enpc.fr/~obozinsg/overlasso.pdf
2306,Abstract,['Applies to real world'],"We study theoretical properties of the estimator, and illustrate
its behavior on simulated and breast cancer gene
expression data",Group Lasso with Overlap and Graph Lasso,http://imagine.enpc.fr/~obozinsg/overlasso.pdf
2307,Intro,"['Successful', 'Used in practice/Popular']","Estimation of sparse linear models by the minimization of
an empirical error penalized by a regularization term is
a very popular and successful approach in statistics and
machine learning.",Group Lasso with Overlap and Graph Lasso,http://imagine.enpc.fr/~obozinsg/overlasso.pdf
2308,Intro,['Applies to real world'],"We mention various properties
of this penalty, and provide conditions for the consistency
of support estimation in the regression setting. Finally, we
report promising results on both simulated and real data",Group Lasso with Overlap and Graph Lasso,http://imagine.enpc.fr/~obozinsg/overlasso.pdf
2313,Abstract,['Novelty'],"We (i) propose a novel gossip-based stochastic gradient descent algorithm, Choco-SGD, that converges
at rate O 1=(nT) + 1=(T δ2!)2_x0001_ for strongly convex objectives, where T denotes the number of iterations
and δ the eigengap of the connectivity matrix. ",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2315,Abstract,['Novelty']," We (ii) present a novel gossip algorithm, Choco-Gossip, for the
average consensus problem that converges in time O(1=(δ2!) log(1=_x000F_)) for accuracy _x000F_ > 0",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2316,Abstract,['Novelty'],"This is (up to
our knowledge) the first gossip algorithm that supports arbitrary compressed messages for ! > 0 and still
exhibits linear convergence. ",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2317,Abstract,"['Quantitative evidence (e.g. experiments)', 'State-of-the-art']","We (iii) show in experiments that both of our algorithms do outperform the
respective state-of-the-art baselines and Choco-SGD can reduce communication by at least two orders
of magnitudes.",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2318,Intro,"['Used in practice/Popular', 'Scales up', 'Privacy']","Decentralized machine learning methods are becoming core aspects of many important applications, both in
view of scalability to larger datasets and systems, but also from the perspective of data locality, ownership
and privacy.",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2320,Intro,['Privacy']," This covers for instance the classic setting of training machine learning models
in large data-centers, but also emerging applications were the computations are executed directly on the
consumer devices, which keep their part of the data private at all times.",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2326,Intro,['Scales up'],"Decentralized topologies avoid these bottlenecks and thereby offer hugely improved potential in
scalability. ",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2330,Intro,['Formal description/analysis'],"We prove
the first analogue result for the important case of decentralized stochastic gradient descent (SGD), proving
convergence at rate O(1=(nT)) (ignoring for now higher order terms) on strongly convex functions where T
denotes the number of iterations",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2331,Intro,['Efficiency'],"This result is significant since stochastic methods are highly preferred for their efficiency over deterministic
gradient methods in machine learning applications.",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2332,Intro,['Efficiency'],"Our algorithm, Choco-SGD, is as efficient in terms of
iterations as centralized mini-batch SGD (and consequently also achieves a speedup of factor n compared
to the serial setting on a single node) but avoids the communication bottleneck that centralized algorithms
suffer from.",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2335,Intro,['Data efficiency'],"To reduce the amount of data that has to be send, gradient
compression has become a popular strategy. For instance by quantization (Alistarh et al., 2017; Wen et al.,
2017; Lin et al., 2018) or sparsification (Wangni et al., 2018; Stich et al., 2018).",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2336,Intro,['Building on recent work'],These ideas have recently been introduced also to the decentralized setting by Tang et al. (2018a),Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2338,Intro,"['Novelty', 'Accuracy']","Here we propose the first method that supports arbitrary low accuracy and even biased
compression operators, such as in (Alistarh et al., 2018; Lin et al., 2018; Stich et al., 2018).",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2341,Intro,['Formal description/analysis'],"We show that the proposed Choco-SGD converges at rate O(1=(nT) + 1=(T δ2!)2), where T denotes
the number of iterations, n the number of workers, δ the eigengap of the gossip (connectivity) matrix
and ! ≤ 1 the compression quality factor (! = 1 meaning no compression)",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2342,Intro,['Scales up'],"We show that the
decentralized method achieves the same speedup as centralized mini-batch SGD when the number n
of workers grows. ",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2344,Intro,['Quantitative evidence (e.g. experiments)'],"This is verified experimentally on the ring topology and by reducing the communication by a factor of
100 (! = 100 1 ).",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2345,Intro,"['Novelty', 'Formal description/analysis']","We present the first provably-converging gossip algorithm with communication compression, for the
distributed average consensus problem.",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2348,Intro,"['Quantitative evidence (e.g. experiments)', 'State-of-the-art']","Choco-SGD significantly outperforms state-of-the-art methods for decentralized optimization with
gradient compression, such as ECD-SGD and DCD-SGD introduced in (Tang et al., 2018a), in all our
experiments.",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2349,Conclusion,"['Novelty', 'Formal description/analysis', 'Quantitative evidence (e.g. experiments)', 'Performance']","The experiments verify our theoretical findings: Choco-Gossip is the first linearly convergent gossip algorithm with quantized communication and Choco-SGD consistently outperforms the
baselines for decentralized optimization, reaching almost the same performance as the exact algorithm without communication restrictions while significantly reducing communication cost.",Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,https://arxiv.org/pdf/1902.00340.pdf
2350,Abstract,['Performance'],"With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining approaches
based on autoregressive language modeling.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2352,Abstract,['Generalization'],"In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2353,Abstract,"['State-of-the-art', 'Building on recent work', 'Unifying ideas or integrating components']","Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2354,Abstract,"['Scientific methodology', 'Performance']","Empirically, under comparable experiment setting, XLNet outperforms BERT on
20 tasks, often by a large margin, including question answering, natural language
inference, sentiment analysis, and document ranking",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2355,Intro,['Successful'],"Unsupervised representation learning has been highly successful in the domain of natural language
processing",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2357,Intro,['Building on recent work'],"Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2358,Intro,"['Successful', 'Building on recent work']","Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2362,Intro,"['Effectiveness', 'Applies to real world']","Since an AR language model is only trained to encode a uni-directional context
(either forward or backward), it is not effective at modeling deep bidirectional contexts.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2363,Intro,['Building on recent work'],"On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2365,Intro,['State-of-the-art'],"A notable example is BERT [10], which has been
the state-of-the-art pretraining approach.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2368,Intro,['Performance'],"As an immediate benefit, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2371,Intro,['Applies to real world'],"In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimplified as high-order, long-range dependency is
prevalent in natural language",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2372,Intro,"['Generalization', 'Building on recent work']","Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2373,Intro,['Building on recent work'],"Firstly, instead of using a fixed forward or backward factorization order as in conventional AR models,
XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2376,Intro,"['Generalization', 'Building on recent work']","Secondly, as a generalized AR language model, XLNet does not rely on data corruption",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2377,Intro,['Building on recent work'],"Hence,
XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2378,Intro,['Building on recent work'],"Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2379,Intro,['Novelty'],"In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2380,Intro,"['Scientific methodology', 'Performance', 'Building on recent work', 'Unifying ideas or integrating components']","Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2383,Intro,"['Flexibility/Extensibility', 'Scientific methodology', 'Performance']","Empirically, under comparable experiment setting, XLNet consistently outperforms BERT [10] on a
wide spectrum of problems including GLUE language understanding tasks, reading comprehension
tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B
document ranking task",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2384,Intro,['Building on recent work'],"RelatedWork The idea of permutation-based AR modeling has been explored in [32, 12], but there
are several key differences.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2385,Intro,['Building on recent work'],"Firstly, previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2386,Intro,['Building on recent work'],"Technically, to construct a valid target-aware prediction
distribution, XLNet incorporates the target position into the hidden state via two-stream attention
while previous permutation-based AR models relied on implicit position awareness inherent to their
MLP architectures.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2388,Intro,['Flexibility/Extensibility'],"Another related idea is to perform autoregressive denoising in the context of text generation [11],
which only considers a fixed order though",XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding
2389,Abstract,"['Simplicity', 'Generality']",We present a simple and general framework for feature learning from point clouds,PointCNN: Convolution On X-Transformed Points,http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points.pdf
2394,Abstract,['Building on classic work'],"The proposed method
is a generalization of typical CNNs to feature learning from point clouds, thus
we call it PointCNN",PointCNN: Convolution On X-Transformed Points,http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points.pdf
2395,Abstract,"['Scientific methodology', 'Performance', 'State-of-the-art']","Experiments show that PointCNN achieves on par or better
performance than state-of-the-art methods on multiple challenging benchmark
datasets and tasks",PointCNN: Convolution On X-Transformed Points,http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points.pdf
2397,Intro,['Effectiveness'],"For data that is represented in regular domains, such as images, the convolution
operator has been shown to be effective in exploiting that correlation as the key contributor to the
success of CNNs on a variety of tasks [25].",PointCNN: Convolution On X-Transformed Points,http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points.pdf
2414,Intro,['Applies to real world'],"In practice,
we find that the learned X-transformations are far from ideal, especially in terms of the permutation
equivalence aspect.",PointCNN: Convolution On X-Transformed Points,http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points.pdf
2415,Intro,['State-of-the-art'],"Nevertheless, PointCNN built with X-Conv is still significantly better than a
direct application of typical convolutions on point clouds, and on par or better than state-of-the-art
neural networks designed for point cloud input data, such as PointNet++ [35].",PointCNN: Convolution On X-Transformed Points,http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points.pdf
2417,Intro,['Understanding (for researchers)'],"We show our results on
multiple challenging benchmark datasets and tasks in Section 4, together with ablation experiments
and visualizations for a better understanding of PointCNN.",PointCNN: Convolution On X-Transformed Points,http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points.pdf
2418,Conclusion,['Building on classic work'],"We proposed PointCNN, which is a generalization of CNN into leveraging spatially-local correlation
from data represented in point cloud.",PointCNN: Convolution On X-Transformed Points,http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points.pdf
2420,Conclusion,"['Scientific methodology', 'Effectiveness', 'Understanding (for researchers)', 'Applies to real world']","While X-Conv is empirically demonstrated to be effective in practice, a rigorous understanding
of it, especially when being composited into a deep neural network, is still an open problem for
future work",PointCNN: Convolution On X-Transformed Points,http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points.pdf
2421,Conclusion,['Building on classic work'],"It is also interesting to study how to combine PointCNN and image CNNs to jointly
process paired point clouds and images, probably at the early stages",PointCNN: Convolution On X-Transformed Points,http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points.pdf
2422,Conclusion,['Facilitating use (e.g. sharing code)'],"We open source our code at
https://github.com/yangyanli/PointCNN to encourage further development",PointCNN: Convolution On X-Transformed Points,http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points.pdf
2429,Abstract,"['Novelty', 'Unifying ideas or integrating components']","We establish a new
notion of quadratic approximation of the neural network, and connect it to the
SGD theory of escaping saddle points.","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2430,Intro,"['Successful', 'Applies to real world']","Neural network learning has become a key machine learning approach and has achieved remarkable
success in a wide range of real-world domains, such as computer vision, speech recognition, and
game playing [25, 26, 30, 41","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2431,Intro,"['Formal description/analysis', 'Scientific methodology']","In contrast to the widely accepted empirical success, much less
theory is known. ","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2432,Intro,['Understanding (for researchers)'],"Despite a recent boost of theoretical studies, many questions remain largely open,
including fundamental ones about the optimization and generalization in learning neural networks.","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2440,Intro,['Applies to real world'],"Most existing works analyzing the learnability of neural networks [9, 12, 13, 20, 21, 28, 33, 34, 42,
43, 47, 49, 50, 56] make unrealistic assumptions about the data distribution (such as being random
Gaussian), and/or make strong assumptions about the network (such as using linear activations)","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2443,Intro,['Building on classic work'],"Indeed, how to obtain
a result that does not depend on the data distribution, but only on the concept class itself, lies in the
center of PAC-learning which is one of the foundations of machine learning theory [48]","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2450,Intro,['Reduced training time'],"This result implies low-degree polynomials and
compositional kernels can be learned by neural networks in polynomial time","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2452,Intro,['Efficiency'],"Our Result. We prove that an important concept class that contains three-layer (resp. two-layer)
neural networks equipped with smooth activations can be efficiently learned by three-layer (resp.
two-layer) ReLU neural networks via SGD or its variants.","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2457,Intro,['Understanding (for researchers)'],"Our Contributions. We believe our result gives further insights to the fundamental questions about
the learning theory of neural networks","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2458,Intro,['Novelty'],"To the best of our knowledge, this is the first result showing that using hidden layers of neural
networks one can provably learn the concept class containing two (or even three) layer neural
networks with non-trivial activation functions","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2459,Intro,"['Novelty', 'Formal description/analysis']","Our three-layer result gives the first theoretical proof that learning neural networks, even with
non-convex interactions across layers, can still be plausible","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2461,Intro,['Understanding (for researchers)'],"To some extent we explain the reason why overparameterization improves testing accuracy:
with larger overparameterization, one can hope to learn better target functions with possibly
larger size, more complex activations, smaller risk OPT, and to a smaller error ""","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2462,Intro,"['Novelty', 'Useful']","We establish new tools to tackle the learning process of neural networks in general, which can
be useful for studying other network architectures and learning tasks. (E.g., the new tools here have allowed researchers to study also the learning of recurrent neural networks [2].)","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2468,Intro,['Performance'],"There is a known performance gap between the power of real neural
networks and the power of their linearized approximations","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2469,Intro,['Accuracy'],"For instance, ResNet achieves 96%
test error on the CIFAR-10 data set but NTK (even with infinite width) achieves 77%","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2473,Intro,"['Generalization', 'Accuracy', 'Understanding (for researchers)']","So, why does it
generalize to the population risk and give small test error?","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2474,Intro,['Understanding (for researchers)'],"More importantly, why does it generalize
with a number of samples that is (almost) independent of the number of parameters?","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2476,Intro,"['Generalization', 'Understanding (for researchers)']","Several works [6, 11, 24, 39] explain generalization by studying some other “complexity” of the learned networks","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2484,Intro,['Generalization'],"At a high level, our generalization is made possible with the following sequence of conceptual steps","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2490,Intro,"['Understanding (for researchers)', 'Applies to real world']","Since practical neural networks are typically overparameterized, we genuinely hope that our results
can provide theoretical insights to networks used in various applications","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2493,Intro,"['Novelty', 'Formal description/analysis']","For readers interested in our novel techniques, we present in Section 6 an 8-paged proof sketch of our
three-layer result","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2494,Intro,"['Scientific methodology', 'Applies to real world']","For readers more interested in the practical relevance, we give more experiments
in Section 7","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2499,Conclusion,['Efficiency'],"We show by training the hidden layers of two-layer (resp. three-layer) overparameterized neural networks, one can efficiently learn some important concept classes including two-layer (resp.
three-layer) networks equipped with smooth activation functions.","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2501,Conclusion,"['Novelty', 'Generalization', 'Understanding (for researchers)']","We believe our work opens up a new direction in both
algorithmic and generalization perspectives of overparameterized neural networks, and pushing forward can possibly lead to more understanding about deep learning","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2502,Conclusion,['Flexibility/Extensibility'],Our results apply to other more structured neural networks,"Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2507,Conclusion,['Flexibility/Extensibility'],"Our analysis can be adapted to show a
similar result for this type of networks.","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2508,Conclusion,['Flexibility/Extensibility'],"One can also combine this paper with properties of recurrent neural networks (RNNs) [3] to derive
PAC-learning results for RNNs [2], or use the existential tools of this paper to derive PAC-learning
results for three-layer residual networks (ResNet) [1]","Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",http://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers
2510,Abstract,['Large scale'],"In this paper, we develop improved techniques
for defending against adversarial examples at
scale",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2511,Abstract,"['Large scale', 'Effectiveness', 'Building on recent work']","First, we implement the state of the art
version of adversarial training at unprecedented
scale on ImageNet and investigate whether it
remains effective in this setting—an important
open scientific question (Athalye et al., 2018)",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2513,Abstract,['Accuracy'],"When applied to clean examples and their
adversarial counterparts, logit pairing improves
accuracy on adversarial examples over vanilla adversarial training; we also find that logit pairing
on clean examples only is competitive with adversarial training in terms of accuracy on two
datasets.",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2514,Abstract,"['Accuracy', 'State-of-the-art']","Finally, we show that adversarial logit
pairing achieves the state of the art defense on
Imagenet against PGD white box attacks, with
an accuracy improvement from 1.5% to 27.9%.
Adversarial logit pairing also successfully damages the current state of the art defense against
black box attacks on Imagenet (Tram`er et al.,
2018), dropping its accuracy from 66.6% to
47.1%.",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2515,Abstract,"['Accuracy', 'State-of-the-art']","With this new accuracy drop, adversarial logit pairing ties with Tram`er et al. (2018) for
the state of the art on black box attacks on ImageNet",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2520,Intro,['Useful'],so that machine learning is more useful for modelbased optimization,Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2521,Intro,"['Theoretical guarantees', 'Understanding (for researchers)']","to gain a better understanding of how to provide performance guarantees for models under distribution
shift",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2522,Intro,['Understanding (for researchers)']," to gain a better understanding of how to enforce
smoothness assumptions, etc.",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2525,Intro,"['Large scale', 'State-of-the-art', 'Effectiveness']","We implement the state of the art version of adversarial training at unprecedented scale and investigate its
effectiveness on the ImageNet dataset",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2527,Intro,['Low cost'],"We show that clean logit pairing is a method with
minimal computational cost that defends against PGD
black box attacks almost as well as adversarial training for two datasets.",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2528,Intro,"['Accuracy', 'State-of-the-art']","We show that adversarial logit pairing is a method
that leads to higher accuracy when subjected to white
box and black box attacks. We achieve the current
state of the art on black-box and white-box accuracies
with our model trained with adversarial logit pairing",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2530,Conclusion,['Large scale'],"In conclusion, we implement adversarial training at unprecendented scale and present logit pairing as a defense",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2531,Conclusion,['Large scale'],"The experiments in this paper were run on NVIDIA p100s,
but with the recent availability of much more powerful
hardware (NVIDIA v100s, Cloud TPUs, etc.), we believe
that defenses for adversarial examples on ImageNet will
become even more scalable.",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2533,Conclusion,['Large scale'],"We answer the open question as to whether adversarial
training scales to ImageNet",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2534,Conclusion,['Effectiveness'],"We introduce adversarial logit pairing (ALP), an extension to adversarial training that greatly increases
its effectiveness",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2535,Conclusion,"['Robustness', 'Low cost']","We introduce clean logit pairing and logit squeezing,
low-cost alternatives to adversarial training that can
increase the adoption of robust machine learning due
to their requirement of very few resources",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2537,Conclusion,['State-of-the-art'],"We show that adversarial logit pairing achieves the
state of the art defense for white box and black box
attacks on ImageNet",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2538,Conclusion,['Useful'],"Our results suggest that feature pairing (matching adversarial and clean intermediate features instead of logits) may
also prove useful in the future",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2542,Conclusion,['Applies to real world'],"We would like to note that these defense mechanisms are
not yet sufficient to secure machine learning in a real system (see many of the concerns raised by (Brown et al.,
2017) and Gilmer et al. (2018)), and that attacks could be
developed against our work in the future.",Adversarial Logit Pairing,https://arxiv.org/pdf/1803.06373.pdf
2546,Abstract,"['Formal description/analysis', 'Understanding (for researchers)']","We prove that the evolution of an ANN during training can also be described by a
kernel: during gradient descent on the parameters of an ANN, the network function
fθ (which maps input vectors to output vectors) follows the kernel gradient of the
functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new
kernel: the Neural Tangent Kernel (NTK)","Neural Tangent Kernel:
Convergence and Generalization in Neural Networks",http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf
2547,Abstract,['Understanding (for researchers)'],"This kernel is central to describe the
generalization features of ANNs","Neural Tangent Kernel:
Convergence and Generalization in Neural Networks",http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf
2549,Abstract,['Understanding (for researchers)'],"This makes it possible to study the
training of ANNs in function space instead of parameter space. ","Neural Tangent Kernel:
Convergence and Generalization in Neural Networks",http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf
2552,Abstract,"['Formal description/analysis', 'Understanding (for researchers)']","The convergence is fastest along the largest kernel principal components
of the input data with respect to the NTK, hence suggesting a theoretical motivation
for early stopping.","Neural Tangent Kernel:
Convergence and Generalization in Neural Networks",http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf
2568,Intro,"['Formal description/analysis', 'Understanding (for researchers)']","In this paper, we investigate fully connected networks in this infinite-width limit, and describe the
dynamics of the network function fθ during training","Neural Tangent Kernel:
Convergence and Generalization in Neural Networks",http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf
2569,Intro,['Understanding (for researchers)'],"During gradient descent, we show that the dynamics of fθ follows that of the so-called kernel
gradient descent in function space with respect to a limiting kernel, which only depends on
the depth of the network, the choice of nonlinearity and the initialization variance.","Neural Tangent Kernel:
Convergence and Generalization in Neural Networks",http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf
2574,Intro,['Scientific methodology'],"Finally we investigate these theoretical results numerically for an artificial dataset (of points
on the unit circle) and for the MNIST dataset. ","Neural Tangent Kernel:
Convergence and Generalization in Neural Networks",http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf
2576,Conclusion,"['Formal description/analysis', 'Understanding (for researchers)']","This paper introduces a new tool to study ANNs, the Neural Tangent Kernel (NTK), which describes
the local dynamics of an ANN during gradient descent. ","Neural Tangent Kernel:
Convergence and Generalization in Neural Networks",http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf
2577,Conclusion,['Unifying ideas or integrating components'],"This leads to a new connection between ANN
training and kernel methods: in the infinite-width limit, an ANN can be described in the function
space directly by the limit of the NTK, an explicit constant kernel Θ( 1L), which only depends on
its depth, nonlinearity and parameter initialization variance.","Neural Tangent Kernel:
Convergence and Generalization in Neural Networks",http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf
2579,Conclusion,['Understanding (for researchers)'],"The
limit of the NTK is hence a powerful tool to understand the generalization properties of ANNs, and
it allows one to study the influence of the depth and nonlinearity on the learning abilities of the
network. ","Neural Tangent Kernel:
Convergence and Generalization in Neural Networks",http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf
2580,Conclusion,['Unifying ideas or integrating components'],"The analysis of training using NTK allows one to relate convergence of ANN training with
the positive-definiteness of the limiting NTK and leads to a characterization of the directions favored
by early stopping methods.","Neural Tangent Kernel:
Convergence and Generalization in Neural Networks",http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf
2581,Abstract,"['Robustness', 'Scientific methodology']","We demonstrate, theoretically and empirically, that adversarial robustness can
significantly benefit from semisupervised learning",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2582,Abstract,['Building on recent work'],"Theoretically, we revisit the
simple Gaussian model of Schmidt et al. [41] that shows a sample complexity gap
between standard and robust classification",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2583,Abstract,['Accuracy']," We prove that unlabeled data bridges
this gap: a simple semisupervised learning procedure (self-training) achieves high
robust accuracy using the same number of labels required for achieving high standard accuracy.",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2584,Abstract,"['Performance', 'Accuracy', 'State-of-the-art']","Empirically, we augment CIFAR-10 with 500K unlabeled images
sourced from 80 Million Tiny Images and use robust self-training to outperform
state-of-the-art robust accuracies by over 5 points in (i) `1 robustness against several strong attacks via adversarial training and (ii) certified `2 and `1 robustness
via randomized smoothing",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2586,Intro,['Building on recent work'],"The past few years have seen an intense research interest in making models robust to adversarial
examples [44, 4, 3]. ",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2588,Intro,['Building on recent work'],"Recent work points towards sample complexity as a possible
reason for the small gains in robustness: Schmidt et al. [41] show that in a simple model, learning
a classifier with non-trivial adversarially robust accuracy requires substantially more samples than
achieving good “standard” accuracy",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2590,Intro,"['Low cost', 'Label efficiency (reduced need for labeled data)']","While
both theory and experiments suggest that more training data leads to greater robustness, following
this suggestion can be difficult due to the cost of gathering additional data and especially obtaining
high-quality labels.",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2591,Intro,['Label efficiency (reduced need for labeled data)'],"To alleviate the need for carefully labeled data, in this paper we study adversarial robustness through
the lens of semisupervised learning",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2596,Intro,['Accuracy'],"Prior
work on semisupervised learning mostly focuses on improving the standard accuracy by leveraging unlabeled data",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2597,Intro,"['Robustness', 'Accuracy']","However, in our adversarial setting the labeled data alone already produce accurate
(but not robust) classifiers.",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2598,Intro,['Building on classic work'],"We can use such classifiers on the unlabeled data and obtain useful
pseudo-labels, which directly suggests the use of self-training—one of the oldest frameworks for
semisupervised learning [42, 8], which applies a supervised training method on the pseudo-labeled
data",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2599,Intro,"['Scientific methodology', 'Effectiveness']","We provide theoretical and experimental evidence that self-training is effective for adversarial
robustness",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2601,Intro,"['Large scale', 'Accuracy']","We scale the model so that n0 labeled examples allow
for learning a classifier with nontrivial standard accuracy, and roughly n0·✏2pd/n0 examples are
necessary for attaining any nontrivial robust accuracy.",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2603,Intro,['Accuracy'],"In this regime, we prove that self training with O(n0·✏2pd/n0)
unlabeled data and just n0 labels achieves high robust accuracy",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2604,Intro,['Understanding (for researchers)'],"Our analysis provides a refined
perspective on the sample complexity barrier in this model: the increased sample requirement is
exclusively on unlabeled data.",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2605,Intro,['Scientific methodology'],"Our theoretical findings motivate the second, empirical part of our paper, where we test the effect
of unlabeled data and self-training on standard adversarial robustness benchmarks",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2609,Intro,"['Performance', 'State-of-the-art']","Using RST on the CIFAR-10 training set augmented with
the additional unlabeled data, we outperform state-of-the-art heuristic `1-robustness against strong
iterative attacks by 7%. ",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2610,Intro,"['Performance', 'State-of-the-art']","In terms of certified `2-robustness, RST outperforms our fully supervised
baseline by 5% and beats previous state-of-the-art numbers by 10%",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2611,Intro,"['Accuracy', 'State-of-the-art']","Finally, we also match the
state-of-the-art certified `1-robustness, while improving on the corresponding standard accuracy
by over 16%. ",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2612,Intro,['Performance'],"We show that some natural alternatives such as virtual adversarial training [30] and
aggressive data augmentation do not perform as well as RST",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2615,Intro,['Accuracy']," Here,
we apply RST by removing the labels from the 531K extra training data and see 4–10% increases in
robust accuracies compared to the baseline that only uses the labeled 73K training set.",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2616,Intro,['Accuracy'],"Swapping
the pseudo-labels for the true SVHN extra labels increases these accuracies by at most 1%. ",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2625,Conclusion,['Scales up'],"Practically,
what is the best way to leverage unlabeled data for robustness, and can semisupervised learning
similarly benefit alternative (non-adversarial) notions of robustness?",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2626,Conclusion,['Large scale'],"As the scale of data grows,
computational capacities increase, and machine learning moves beyond minimizing average error, we
expect unlabeled data to provide continued benefit",Unlabeled Data Improves Adversarial Robustness,http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness
2627,Abstract,['Building on recent work'],"With the introduction of the variational autoencoder (VAE), probabilistic latent
variable models have received renewed attention as powerful generative models","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2628,Abstract,['Performance'],"However, their performance in terms of test likelihood and quality of generated
samples has been surpassed by autoregressive models without stochastic units","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2629,Abstract,['Scales up'],"Furthermore, flow-based models have recently been shown to be an attractive
alternative that scales well to high-dimensional data","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2630,Abstract,['Performance'],"In this paper we close the
performance gap by constructing VAE models that can effectively utilize a deep
hierarchy of stochastic variables and model complex covariance structures","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2632,Abstract,"['Realistic output', 'State-of-the-art']","We show that BIVA reaches state-of-the-art test
likelihoods, generates sharp and coherent natural images, and uses the hierarchy of
latent variables to capture different aspects of the data distribution","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2635,Abstract,"['Flexibility/Extensibility', 'Performance', 'State-of-the-art']","Finally, we extend BIVA to semi-supervised classification tasks and show
that it performs comparably to state-of-the-art results by generative adversarial
networks.","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2639,Intro,['Flexibility/Extensibility']," The range of applications that come with generative models are
vast, where audio synthesis [55] and semi-supervised classification [38, 31, 44] are examples hereof.","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2644,Intro,"['Performance', 'Effectiveness']","In recent years
autoregressive models, such as the PixelRNN and the PixelCNN [57, 45], have achieved superior
likelihood performance and flow-based models have proven efficacy on large-scale natural image
generation tasks [21]","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2645,Intro,"['Performance', 'Scales up']","However, in the autoregressive models, the runtime performance of generation
is scaling poorly with the complexity of the input distribution","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2646,Intro,['Realistic output'],"The flow-based models do not possess this restriction and do indeed generate visually compelling natural images when sampling close to
the mode of the distribution","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2647,Intro,['Performance'],"However, generation from the actual learned distribution is still not
outperforming autoregressive models","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2649,Intro,['Formal description/analysis'],"They
are characterized by a posterior distribution over the latent variables of the model, derived from
Bayes’ theorem, which is typically intractable and needs to be approximated","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2650,Intro,['Understanding (for researchers)'],"This distribution
most commonly lies on a low-dimensional manifold that can provide insights into the internal
representation of the data [1","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2651,Intro,"['Realistic output', 'Performance']","However, the latent variable models have largely been disregarded as
powerful generative models due to blurry generations and poor likelihood performances on natural
image tasks","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2656,Intro,['Performance'],"However, despite significant improvements, the reported performance
of these models has still been inferior to their autoregressive counterparts","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2662,Intro,['Building on recent work'],"The inference model is reminiscent to the stochastic
top-down path introduced in the Ladder VAE [50] and IAF VAE [50] with the addition that the
bottom-up pass is now also stochastic and there are no autoregressive components.","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2663,Intro,"['Flexibility/Extensibility', 'State-of-the-art', 'Understanding (for researchers)']","We perform
an in-depth analysis of BIVA and show (i) an ablation study that analyses the contributions of the
individual novel components, (ii) that the model is able to improve on state-of-the-art results on
benchmark image datasets, (iii) that a small extension of the model can be used for semi-supervised
classification and performs comparably to current state-of-the-art models, and (iv) that the model,
contrarily to other state-of-the-art explicit density models [34], can be utilized for anomaly detection
on complex data distributions.","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2664,Conclusion,['Performance'],"In this paper, we have introduced BIVA, that significantly improves performances over previously
introduced probabilistic latent variable models and flow-based models","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2665,Conclusion,"['Flexibility/Extensibility', 'Realistic output']"," BIVA is able to generate natural images that are both sharp and coherent, to improve on semi-supervised classification benchmarks
and, contrarily to other models, allows for anomaly detection using the extracted high-level semantics
of the data.","BIVA: A Very Deep Hierarchy of Latent Variables for
Generative Modeling
",http://papers.nips.cc/paper/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.pdf
2669,Abstract,['Generality'],"Our result is more general and sharper than many existing
generalization error bounds for over-parameterized neural networks","Generalization Bounds of Stochastic Gradient
Descent for Wide and Deep Neural Networks",http://papers.nips.cc/paper/9266-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks.pdf
2670,Abstract,['Building on recent work']," In addition,
we establish a strong connection between our generalization error bound and the
neural tangent kernel (NTK) proposed in recent work","Generalization Bounds of Stochastic Gradient
Descent for Wide and Deep Neural Networks",http://papers.nips.cc/paper/9266-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks.pdf
2671,Intro,['Successful'],"Deep learning has achieved great success in a wide range of applications including image processing
[20], natural language processing [17] and reinforcement learning [34]","Generalization Bounds of Stochastic Gradient
Descent for Wide and Deep Neural Networks",http://papers.nips.cc/paper/9266-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks.pdf
2674,Intro,['Scientific methodology'],"In fact, a famous empirical study by Zhang et al. [38] shows the
following phenomena","Generalization Bounds of Stochastic Gradient
Descent for Wide and Deep Neural Networks",http://papers.nips.cc/paper/9266-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks.pdf
2678,Intro,['Understanding (for researchers)'],"While a series of recent work has theoretically shown that a sufficiently over-parameterized (i.e.,
sufficiently wide) neural network can fit random labels [12, 2, 11, 39], the reason why it can generalize
well when trained with real labels is less understood","Generalization Bounds of Stochastic Gradient
Descent for Wide and Deep Neural Networks",http://papers.nips.cc/paper/9266-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks.pdf
2680,Intro,"['Scientific methodology', 'Understanding (for researchers)']","In fact, the empirical observation by
Zhang et al. [38] indicates that in order to understand deep learning, it is important to distinguish the
true data labels from random labels when studying generalization.","Generalization Bounds of Stochastic Gradient
Descent for Wide and Deep Neural Networks",http://papers.nips.cc/paper/9266-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks.pdf
2693,Intro,['Generality'],"In this paper, we aim at providing a sharper and generic analysis on the generalization of deep ReLU
networks trained by SGD","Generalization Bounds of Stochastic Gradient
Descent for Wide and Deep Neural Networks",http://papers.nips.cc/paper/9266-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks.pdf
2700,Intro,"['Generality', 'Accuracy']","Our analysis is general enough to cover recent generalization error bounds for neural networks
with random feature based reference function classes, and provides better bounds.","Generalization Bounds of Stochastic Gradient
Descent for Wide and Deep Neural Networks",http://papers.nips.cc/paper/9266-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks.pdf
2702,Intro,['Generalization'],"Compared with recent results by Yehudai and Shamir [37], E et al. [14] who only studied
two-layer networks, our bound not only works for deep networks, but also uses a larger reference
function class when reduced to the two-layer setting, and therefore is sharper","Generalization Bounds of Stochastic Gradient
Descent for Wide and Deep Neural Networks",http://papers.nips.cc/paper/9266-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks.pdf
2703,Intro,['Building on recent work'],Our result has a direct connection to the neural tangent kernel studied in Jacot et al. [18],"Generalization Bounds of Stochastic Gradient
Descent for Wide and Deep Neural Networks",http://papers.nips.cc/paper/9266-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks.pdf
2705,Intro,['Generality'],"This form of generalization bound is similar to,
but more general and tighter than the bound given by Arora et al. [4].","Generalization Bounds of Stochastic Gradient
Descent for Wide and Deep Neural Networks",http://papers.nips.cc/paper/9266-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks.pdf
2708,Conclusions and Future Work,['Building on recent work'],"The connection to
the neural tangent kernel function studied in Jacot et al. [18] is also discussed.","Generalization Bounds of Stochastic Gradient
Descent for Wide and Deep Neural Networks",http://papers.nips.cc/paper/9266-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks.pdf
2711,Conclusions and Future Work,['Building on recent work'],"Other future directions include proving sample complexity lower bounds in the
over-parameterized regime, implementing the results in Jain et al. [19] to obtain last iterate bound
of SGD, and establishing uniform convergence based generalization bounds for over-parameterized
neural networks with methods developped in Bartlett et al. [6], Neyshabur et al. [27], Long and
Sedghi [26].","Generalization Bounds of Stochastic Gradient
Descent for Wide and Deep Neural Networks",http://papers.nips.cc/paper/9266-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks.pdf
2712,abstract,"['Low cost', 'Fast']","We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design.",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2713,abstract,"['Large scale', 'Efficiency', 'Optimal']","In ENAS, a controller discovers neural network architectures by searching for an optimal subgraph within a large computational graph. ",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2716,abstract,"['Low cost', 'Performance']","Sharing parameters among child models allows ENAS to deliver strong empirical performances, while using much fewer GPUhours than existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search.",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2717,abstract,"['Novelty', 'State-of-the-art']","On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. ",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2718,abstract,['Novelty'],"On the CIFAR-10 dataset, ENAS finds a novel architecture that achieves 2.89% test error, which is on par with the 2.65% test error of NASNet (Zoph et al., 2018).",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2719,introduction,['Successful'],"Neural architecture search (NAS) has been successfully applied to design model architectures for image classification and language models (Zoph & Le, 2017; Zoph et al., 2018; Cai et al., 2018; Liu et al., 2017; 2018). ",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2720,introduction,['Performance'],"In NAS, an RNN controller is trained in a loop: the controller first samples a candidate architecture, i.e. a child model, and then trains it to convergence to measure its performance on the task of desire. ",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2721,introduction,"['Promising', 'Performance']",The controller then uses the performance as a guiding signal to find more promising architectures. This process is repeated for many iterations. ,Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2722,introduction,"['Performance', 'Impressive']","Despite its impressive empirical performance, NAS is computationally expensive and time consuming, e.g. Zoph et al. (2018) use 450 GPUs for 3-4 days (i.e. 32,400-43,200 GPU hours). ",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2723,introduction,['Requires few resources'],"Meanwhile, using less resources tends to produce less compelling results (Negrinho & Gordon, 2017; Baker et al., 2017a). ",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2724,introduction,['Accuracy'],"We observe that the computational bottleneck of NAS is the training of each child model to convergence, only to measure its accuracy whilst throwing away all the trained weights. ",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2725,introduction,"['Efficiency', 'Improvement']",The main contribution of this work is to improve the efficiency of NAS by forcing all child models to share weights to eschew training each child model from scratch to convergence. ,Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2726,introduction,['Building on recent work'],"The idea has apparent complications, as different child models might utilize their weights differently, but was encouraged by previous work on transfer learning and multitask learning, which established that parameters learned for a particular model on a particular task can be used for other models on other tasks, with little to no modifications (Razavian et al., 2014; Zoph et al., 2016; Luong et al., 2016).",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2727,introduction,"['Flexibility/Extensibility', 'Performance']","We empirically show that not only is sharing parameters among child models possible, but it also allows for very strong performance. ",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2729,introduction,"['Novelty', 'Performance', 'State-of-the-art']","On Penn Treebank, our method achieves a test perplexity of 55.8, which significantly outperforms NAS’s test perplexity of 62.4 (Zoph & Le, 2017) and which is a new state-of-the-art among Penn Treebank’s approaches that do not utilize post-training processing. ",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2730,introduction,['Requires few resources'],"Importantly, in all of our experiments, for which we use a single Nvidia GTX 1080Ti GPU, the search for architectures takes less than 16 hours.",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2731,introduction,"['Efficiency', 'Requires few resources']","Compared to NAS, this is a reduction of GPU-hours by more than 1000x. Due to its efficiency, we name our method Efficient Neural Architecture Search (ENAS).",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2734,conclusion,"['Novelty', 'Efficiency', 'Fast']","In this paper, we presented ENAS, a novel method that speeds up NAS by more than 1000x, in terms of GPU hours. ",Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2735,conclusion,['Efficiency'],ENAS’s key contribution is the sharing of parameters across child models during the search for architectures. ,Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2736,conclusion,['Large scale'],This insight is implemented by searching for a subgraph within a larger graph that incorporates architectures in a search space. ,Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2737,conclusion,"['Large scale', 'Efficiency']",We showed that ENAS works well on both CIFAR-10 and Penn Treebank datasets.,Efficient Neural Architecture Search via Parameter Sharing,http://export.arxiv.org/pdf/1802.03268
2738,abstract,"['Identifying limitations', 'Security']","We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2739,abstract,['Optimal'],"While defenses that cause obfuscated gradients appear to defeat iterative optimizationbased attacks, we find defenses relying on this effect can be circumvented. ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2741,abstract,['Used in practice/Popular'],"In a case study, examining noncertified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2742,abstract,"['Novelty', 'Successful']","Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2743,introduction,"['Robustness', 'Building on recent work']","In response to the susceptibility of neural networks to adversarial examples (Szegedy et al., 2013; Biggio et al., 2013), there has been significant interest recently in constructing defenses to increase the robustness of neural networks. ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2744,introduction,"['Qualitative evidence (e.g. examples)', 'Understanding (for researchers)', 'Progress']","While progress has been made in understanding and defending against adversarial examples in the white-box setting, where the adversary has full access to the network, a complete solution has not yet been found. ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2745,introduction,"['Robustness', 'Building on recent work', 'Used in practice/Popular', 'Optimal', 'Powerful']","As benchmarking against iterative optimization-based attacks (e.g., Kurakin et al. (2016a); Madry et al. (2018); Carlini & Wagner (2017c)) has become standard practice in evaluating defenses, new defenses have arisen that appear to be robust against these powerful optimization-based attacks. ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2746,introduction,"['Robustness', 'Optimal']","We identify one common reason why many defenses provide apparent robustness against iterative optimization attacks: obfuscated gradients, a term we define as a special case of gradient masking (Papernot et al., 2017). ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2747,introduction,"['Successful', 'Optimal']","Without a good gradient, where following the gradient does not successfully optimize the loss, iterative optimization-based methods cannot succeed. ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2748,introduction,['Stable'],We identify three types of obfuscated gradients: shattered gradients are nonexistent or incorrect gradients caused either intentionally through non-differentiable operations or unintentionally through numerical instability; stochastic gradients depend on test-time randomness; and vanishing/exploding gradients in very deep computation result in an unusable gradient.,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2749,introduction,['Novelty'],We propose new techniques to overcome obfuscated gradients caused by these three phenomena. ,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2750,introduction,['Approximation'],"We address gradient shattering with a new attack technique we call Backward Pass Differentiable Approximation, where we approximate derivatives by computing the forward pass normally and computing the backward pass using a differentiable approximation of the function. ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2751,introduction,"['Formal description/analysis', 'Building on recent work']","We compute gradients of randomized defenses by applying Expectation Over Transformation (Athalye et al., 2017). ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2752,introduction,['Optimal'],We solve vanishing/exploding gradients through reparameterization and optimize over a space where gradients do not explode/vanish.,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2753,introduction,"['Qualitative evidence (e.g. examples)', 'Understanding (for researchers)']","To investigate the prevalence of obfuscated gradients and understand the applicability of these attack techniques, we use as a case study the ICLR 2018 non-certified defenses that claim white-box robustness. ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2755,introduction,['Novelty'],"Applying the new attack techniques we develop, we overcome obfuscated gradients and circumvent 6 of them completely, and 1 partially, under the original threat model of each paper. ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2756,introduction,['Performance'],"Along with this, we offer an analysis of the evaluations performed in the papers. ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2757,introduction,['Understanding (for researchers)'],"Additionally, we hope to provide researchers with a common baseline of knowledge, description of attack techniques, and common evaluation pitfalls, so that future defenses can avoid falling vulnerable to these same attack approaches. ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2758,introduction,['Facilitating use (e.g. sharing code)'],"To promote reproducible research, we release our reimplementation of each of these defenses, along with implementations of our attacks for each.",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2759,conclusion,['Qualitative evidence (e.g. examples)'],Constructing defenses to adversarial examples requires defending against not only existing attacks but also future attacks that may be developed. ,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2762,conclusion,['Applies to real world'],"To evaluate the applicability of our techniques, we use the ICLR 2018 defenses as a case study, circumventing seven of nine accepted defenses. ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2763,conclusion,"['Robustness', 'Generality']","More generally, we hope that future work will be able to avoid relying on obfuscated gradients (and other methods that only prevent gradient descent-based attacks) for perceived robustness, and use our evaluation approach to detect when this occurs. ",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2764,conclusion,['Performance'],"Defending against adversarial examples is an important area of research and we believe performing a careful, thorough evaluation is a critical step that can not be overlooked when designing defenses.",Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf
2765,abstract,['Generalization'],This work characterizes the generalization ability of algorithms whose predictions are linear in the input vector. ,"On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2766,abstract,['Generalization'],"To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which directly lead to a number of generalization bounds. ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2767,abstract,"['Simplicity', 'Theoretical guarantees', 'Generality']","This derivation provides simplified proofs of a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including both L2 and L1 margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and upper bounds on L2 covering numbers (with Lp norm constraints and relative entropy constraints). ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2768,abstract,['Unifying ideas or integrating components'],"In addition to providing a unified analysis, the results herein provide some of the sharpest risk and margin bounds. ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2771,introduction,"['Generalization', 'Understanding (for researchers)']",A paramount question is to understand the generalization ability of these algorithms in terms of the attendant complexity restrictions imposed by the algorithm. ,"On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2772,introduction,"['Generalization', 'Qualitative evidence (e.g. examples)']","For example, for the sparse methods (e.g. regularizing based on L1 norm of the weight vector) we seek generalization bounds in terms of the sparsity level. ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2773,introduction,['Generalization'],"For margin based methods (e.g. SVMs or boosting), we seek generalization bounds in terms of either the L2 or L1 margins. T","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2774,introduction,"['Scientific methodology', 'Unifying ideas or integrating components']",The focus of this paper is to provide a more unified analysis for methods which use linear prediction,"On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2775,introduction,['Formal description/analysis'],"Given a training set {(x, yi)}n i=1, the paradigm is to compute a weight vector wˆ which minimizes the F-regularized ℓ-risk. More specifically, wˆ = argminw1n Xn i=1 ℓ(hw, xii, yi) + λF(w) (1) where ℓ is the loss function, F is the regularizer, and hw, xi is the inner product between vectors x and w. ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2776,introduction,['Formal description/analysis'],"In a formulation closely related to the dual problem, we have: wˆ = argmin w:F (w)≤c 1 n Xn i=1 ℓ(hw, xii, yi) (2) where, instead of regularizing, a hard restriction over the parameter space is imposed (by the constant c). ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2777,introduction,['Generalization'],This works provides generalization bounds for an extensive family of regularization functions F.,"On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2778,introduction,['Generalization'],"Rademacher complexities (a measure of the complexity of a function class) provide a direct route to obtaining such generalization bounds, and this is the route we take. ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2779,introduction,['Parallelizability / distributed'],"Such bounds are analogous to VC dimensions bounds, but they are typically much sharper and allow for distribution dependent bounds. ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2780,introduction,"['Generalization', 'Scientific methodology']",There are a number of methods in the literature to use Rademacher complexities to obtain either generalization bounds or margin bounds. ,"On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2781,introduction,"['Generalization', 'Building on classic work']",Bartlett and Mendelson [2002] provide a generalization bound for Lipschitz loss functions. ,"On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2782,introduction,['Building on classic work'],"For binary prediction, the results in Koltchinskii and Panchenko [2002] provide means to obtain margin bounds through Rademacher complexities","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2784,introduction,"['Simplicity', 'Generalization', 'Theoretical guarantees', 'Generality', 'Unifying ideas or integrating components']","These bounds provide simplified proofs of a number of corollaries: generalization bounds for the regularization algorithm in Equation 2 (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including L2 and L1 margins, and, more generally, for Lp margins), a proof of the PAC-Bayes theorem, and L2 covering numbers (with Lp norm constraints and relative entropy constraints). ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2785,introduction,"['Theoretical guarantees', 'Unifying ideas or integrating components']",Our bounds are often tighter than previous results and our proofs are all under this more unified methodology.,"On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2786,introduction,"['Theoretical guarantees', 'Generality', 'Building on classic work']",Our proof techniques — reminiscent of those techniques for deriving regret bounds for online learning algorithms — are rooted in convex duality (following Meir and Zhang [2003]) and use a more general notion of strong convexity (as in Shalev-Shwartz and Singer [2006]). ,"On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2790,related work,['Building on classic work'],"Perhaps the most extensively studied are margin bounds for the 0-1 loss. For L2-margins (relevant for SVM’s, perceptron based algorithms, etc.), the sharpest bounds are those provided by Bartlett and Mendelson [2002] (using Rademacher complexities) and Langford and Shawe-Taylor [2003], McAllester [2003] (using the PAC-Bayes theorem). ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2791,related work,"['Scientific methodology', 'Building on classic work']","For L1-margins (relevant for Boosting, winnow, etc), bounds are provided by Schapire et al. [1998] (using a self-contained analysis) and Langford et al. [2001] (using PAC-Bayes, with a different analysis). Another active line of work is on sparse methods — particularly methods which impose sparsity via L1 regularization (in lieu of the non-convex L0 norm). ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2792,related work,"['Generalization', 'Building on classic work']","For L1 regularization, Ng [2004] provides generalization bounds for this case, which follow from the covering number bounds of Zhang [2002]. However, these bounds are only stated as polynomial in the relevant quantities (dependencies are not provided).","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2793,related work,"['Generalization', 'Building on classic work', 'Unifying ideas or integrating components']","Previous to this work, the most unified framework for providing generalization bounds for linear prediction stem from the covering number bounds in Zhang [2002]. ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2794,related work,"['Generalization', 'Building on classic work']","Using these covering number bounds, Zhang [2002] derives margin bounds in a variety of cases. However, providing sharp generalization bounds for problems with L1 regularization (or L1 constraints in the dual) requires more delicate arguments. ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2795,related work,['Building on classic work'],"As mentioned, Ng [2004] provides bounds for this case, but the techniques used by Ng [2004] would result in rather loose dependencies (the dependence on the sample size n would be n −1/4 rather than n −1/2 ). We discuss this later in Section 4.","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2798,discussion,['Performance'],"The algorithm we consider performs the update, wt+1 = ∇F −1 (∇F(wt) − η∇wℓ(hwt, xti, yt))","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2799,discussion,['Building on classic work'],"This algorithm captures both gradient updates, multiplicative updates, and updates based on the Lp norms, through appropriate choices of F. See Shalev-Shwartz [2007] for discussion.","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2800,discussion,['Formal description/analysis'],"For the algorithm given by the above update, the following theorem is a bound on the cumulative regret. ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2801,discussion,['Building on classic work'],"It is a corollary of Theorem 1 in Shalev-Shwartz and Singer [2006] (and also of Corollary 1 in Shalev-Shwartz [2007]), applied to our linear case. ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2802,discussion,['Building on classic work'],"Corollary 10. (Shalev-Shwartz and Singer [2006]) Let S be a closed convex set and let F : S → R be σ-strongly convex w.r.t. k · k∗. Further, let X = {x : kxk ≤ X} and W = {w ∈ S : F(w) ≤ W2∗ }. ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2803,discussion,['Preciseness'],"Then for the update given by Equation 9 if we start with w1 = argmin F(w), we have that for all sequences {(xt, yt)} n t=1, Xn t=1(hwt, xti, yt) − argmin w∈W Xn t=1 ℓ(hw, xti, yt) ≤ LℓXW∗ r 2n σ","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2804,discussion,['Preciseness'],"For completeness, we provide a direct proof in the Appendix. Interestingly, the regret above is precisely our complexity bounds (when Lℓ = 1). Also, our risk bounds are a factor of 2 worse, essentially due to the symmetrization step used in proving Theorem 1.","On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",https://papers.nips.cc/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf
2806,Abstract,['Efficiency'],Recent studies have demonstrated the efficiency of generative pretraining for En- glish natural language understanding.,Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2807,Abstract,['Effectiveness'],"In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2809,Abstract,['State-of-the-art'],"We obtain state-of- the-art results on cross-lingual classification, unsupervised and supervised machine translation.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2810,Abstract,"['Accuracy', 'State-of-the-art']","On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2811,Abstract,"['Quantitative evidence (e.g. experiments)', 'State-of-the-art', 'Improvement']","On unsupervised machine translation, we obtain 34.3 BLEU on WMT’ 16 German-English, improving the previous state of the art by more than 9 BLEU.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2812,Abstract,"['Quantitative evidence (e.g. experiments)', 'Performance', 'State-of-the-art']","On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT’ 16 Romanian-English, outperforming the previous best approach by more than 4 BLEU.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2813,Abstract,['Facilitating use (e.g. sharing code)'],Our code and pretrained models are publicly available'.,Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2815,Intro,"['Quantitative evidence (e.g. experiments)', 'Improvement']","Generative pretraining of sentence encoders [30, 20, 14] has led to strong improvements on numerous natural language understanding benchmarks [40].",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2818,Intro,['Not socially biased'],Recent developments in learning and evaluating cross-lingual sentence representations in many languages [12] aim at mitigating the English-centric bias and suggest that it is possible to build universal cross-lingual encoders that can encode any sentence into a shared embedding space.,Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2819,Intro,"['Quantitative evidence (e.g. experiments)', 'Effectiveness']","In this work, we demonstrate the effectiveness of cross-lingual language model pretraining on multiple cross-lingual understanding (XLU) benchmarks.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2820,Intro,['Preciseness'],"Precisely, we make the following contributions:",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2822,Intro,['Novelty'],We introduce a new unsupervised method for learning cross-lingual representations using cross-lingual language modeling and investigate two monolingual pretraining objectives.,Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2824,Intro,"['Novelty', 'Improvement']",We introduce a new supervised learning objective that improves cross-lingual pretraining when parallel data is available.,Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2826,Intro,"['Performance', 'State-of-the-art']","We significantly outperform the previous state of the art on cross-lingual classification, unsupervised machine translation and supervised machine translation.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2828,Intro,"['Improvement', 'Requires few resources']",We show that cross-lingual language models can provide significant improvements on the perplexity of low-resource languages.,Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2830,Intro,['Facilitating use (e.g. sharing code)'],We make our code and pretrained models publicly available'.,Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2832,Conclusion,['Novelty'],"In this work, we show for the first time the strong impact of cross-lingual language model (XLM) pretraining.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2835,Conclusion,['Effectiveness'],"On unsupervised machine translation, we show that MLM pretraining is extremely effective.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2836,Conclusion,"['Quantitative evidence (e.g. experiments)', 'Performance', 'State-of-the-art']","We reach a new state of the art of 34.3 BLEU on WMT’ 16 German-English, outperforming the previous best approach by more than 9 BLEU.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2837,Conclusion,['Improvement'],"Similarly, we obtain strong improvements on supervised machine translation.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2838,Conclusion,"['Quantitative evidence (e.g. experiments)', 'State-of-the-art', 'Improvement']","We reach a new state of the art on WMT’ 16 Romanian-English of 38.5 BLEU, which corresponds to an improvement of more than 4 BLEU points.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2839,Conclusion,"['Quantitative evidence (e.g. experiments)', 'Data efficiency', 'Improvement', 'Requires few resources']","We also demonstrate that XLMs can be used to improve the perplexity of a Nepali language model, and that it provides unsupervised cross-lingual word embeddings.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2840,Conclusion,"['Performance', 'Accuracy', 'State-of-the-art', 'Data efficiency']","Without using a single parallel sentence, our MLM model fine-tuned on XNLI already outperforms the previous supervised state of the art by 1.3% accuracy on average.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2841,Conclusion,['Improvement'],Our translation language model objective (TLM) leverages parallel data to improve further the alignment of sentence representations.,Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2842,Conclusion,"['Quantitative evidence (e.g. experiments)', 'Accuracy', 'State-of-the-art']","When used together with MLM, we show that this supervised approach beats the previous state of the art on XNLI by 4.9% accuracy on average.",Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2843,Conclusion,['Facilitating use (e.g. sharing code)'],Our code and pretrained models are publicly available.,Cross-lingual Language Model Pretraining,http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining
2848,abstract,"['Quantitative evidence (e.g. experiments)', 'Exactness']","While these theoretical results are only exact in theinfinite width limit, we nevertheless find excellent empirical agreement betweenthe predictions of the original network and those of the linearized version evenfor finite practically-sized networks. ",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2849,abstract,['Robustness'],"This agreement is robust across differentarchitectures, optimization methods, and loss functions",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2850,intro,"['Generalization', 'Performance']","Machine learning models based on deep neural networks have achieved unprecedented performanceacross a wide range of tasks [1,2,3]. ",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2852,intro,['Easy'],"Moreover, characterizing the gradient-based training dynamics of these models is challenging owing to the typically high-dimensional non-convex loss surfaces governing the optimization.",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2853,intro,"['Formal description/analysis', 'Scientific methodology', 'Understanding (for researchers)']","As is common in the physical sciences, investigating theextreme limits of such systems can often shed light on these hard problems",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2856,intro,"['Simplicity', 'Generalization', 'Formal description/analysis', 'Practical.1']","Aside fromits theoretical simplicity, the infinite-width limit is also of practical interest as wider networks havebeen found to generalize better",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2859,intro,"['Quantitative evidence (e.g. experiments)', 'Exactness']","While the linearization is only exact in the infinite width limit, we nevertheless find excellent agreement between the predictions of the original network and those of the linearized version even for finite width configurations.",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2860,intro,['Generalization'],"The agreement persists across different architectures, optimization methods, and loss functions.",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2861,intro,"['Formal description/analysis', 'Unifying ideas or integrating components']","For squared loss, the exact learning dynamics admit a closed-form solution that allows us to charac- terize the evolution of the predictive distribution in terms of a GP.",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2863,intro,"['Quantitative evidence (e.g. experiments)', 'Accuracy']",Our empirical simulations confirm that the result accurately models the variation in predictions...,Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2864,intro,['Unifying ideas or integrating components'],"- Parameter space dynamics: We show that wide network training dynamics in parameter space are equivalent to the training dynamics of a model which is affine in the collection of all network parameters, the weights and biases.",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2865,intro,['Generalization'],This result holds regardless of the choice of loss function.,Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2867,intro,['Formal description/analysis'],"- Sufficient conditions for linearization: We formally prove that there exists a threshold learning rate Yeritical (see Theorem 2.1), such that gradient descent training trajectories with learning rate smaller than 7eritical Stay in an O (n-V ?\-neighborhood of the trajectory of the linearized network when n, the width of the hidden layers, is sufficiently large.
",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2868,intro,"['Formal description/analysis', 'Building on recent work', 'Unifying ideas or integrating components']","- Output distribution dynamics: We formally show that the predictions of a neural network throughout gradient descent training are described by a GP as the width goes to infinity (see Theorem 2.2), extending results from Jacot et al.[13].",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2869,intro,['Formal description/analysis'],We further derive explicit time-dependent expressions for the evolution of this GP during training.,Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2870,intro,['Novelty'],"Finally, we provide a novel interpretationof the result.",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2871,intro,"['Quantitative evidence (e.g. experiments)', 'Unifying ideas or integrating components', 'Understanding (for researchers)']","In particular, it offers a quantitative understanding of the mechanism by which gradient descent differs from Bayesian posterior sampling of the parameters: while both methods generate draws from a GP, gradient descent does not generate samples from the posterior of any probabilistic model.",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2872,intro,"['Generalization', 'Quantitative evidence (e.g. experiments)', 'Large scale', 'Accuracy', 'Understanding (for researchers)', 'Practical.1']","- Large scale experimental support: We empirically investigate the applicability of the theory in the finite-width setting and find that it gives an accurate characterization of both learning dynamics and posterior function distributions across a variety of conditions, including some practical network architectures such as the wide residual network [14].",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2874,intro,['Formal description/analysis'],- Analytic ReLU and erf neural tangent kernels: We compute the analytic neural tangent kernel corresponding to fully-connected networks with ReLU or erf nonlinearities.,Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2875,intro,['Facilitating use (e.g. sharing code)'],Example code investigating both ... linearizedlearning dynamics described in this work is released as open source code within [15].2We alsoprovide accompanying interactive Colab notebooks for both,Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2876,intro,['Facilitating use (e.g. sharing code)'],We also provide accompanying interactive Colab notebooks for both and linearization.,Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2877,discussion,"['Formal description/analysis', 'Exactness', 'Unifying ideas or integrating components']",We showed theoretically that the learning dynamics in parameter space of deep nonlinear neuralnetworks are exactly described by a linearized model in the infinite width limit.,Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2878,discussion,"['Generalization', 'Quantitative evidence (e.g. experiments)', 'Understanding (for researchers)']","Empirical investiga- tion revealed that this agrees well with actual training dynamics and predictive distributions across fully-connected, convolutional, and even wide residual network architectures, as well as with different optimizers (gradient descent, momentum, mini-batching) and loss functions (MSE, cross-entropy).",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2880,discussion,['Building on recent work'],This is further consistent with recent experimental work,Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2882,discussion,"['Simplicity', 'Formal description/analysis', 'Unifying ideas or integrating components']","Furthermore, the infinite width limit gives us a simple characterization of both gradientdescent and Bayesian inference",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2883,discussion,['Understanding (for researchers)'],"By studying properties of the NNGP kernel X and the tangent kernel Q, we may shed light on the inductive bias of gradient descent.",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2886,discussion,"['Generalization', 'Unifying ideas or integrating components']","Furthermore, in Novak et al. [7], it is shown that the comparison of performance between finite- and infinite-width networks is highly architecture-dependent.
",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2887,discussion,"['Generalization', 'Performance']","In particular, it was found that infinite-width networks perform as well as or better than their finite-widthcounterparts for many fully-connected or locally-connected architectures",Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2889,discussion,['Understanding (for researchers)'],It is still an open research question to determine the main factors that determine these performance gaps.,Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2890,discussion,"['Formal description/analysis', 'Understanding (for researchers)']",We believe that examining the behavior of infinitely wide networks provides a strong basis from which to build up a systematic understanding of finite-width networks (and/or networks trained with large learning rates).,Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent,http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf
2891,Abstract,"['Used in practice/Popular', 'Fast', 'Stable']",Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs).,How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2892,Abstract,['Understanding (for researchers)'],"Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2893,Abstract,['Effectiveness'],The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”.,How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2894,Abstract,['Successful'],"In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2895,Abstract,"['Formal description/analysis', 'Understanding (for researchers)']","Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2896,Abstract,"['Fast', 'Stable']","This smoothness induces amore predictive and stable behavior of the gradients, allowing for faster training.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2897,Introduction,"['Generalization', 'Progress', 'Impressive']","Over the last decade, deep learning has made impressive progress on a variety of notoriously difficult tasks in computer vision [16, 7], speech recognition [5], machine translation [29], and game-playing [18, 25].",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2898,Introduction,['Progress'],"This progress hinged on a number of major advances in terms of hardware, datasets [15, 23], and algorithmic and architectural techniques [27, 12, 20, 28].",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2900,Introduction,['Stable'],"At a high level, BatchNorm is a technique that aims to improve the training of neural networks by stabilizing the distributions of layer inputs.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2902,Introduction,"['Successful', 'Practical']",The practical success of BatchNorm is indisputable.,How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2903,Introduction,"['Used in practice/Popular', 'Applies to real world']","By now, it is used by default in most deep learning models, both in research (more than 6,000 citations) and real-world settings.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2904,Introduction,"['Effectiveness', 'Understanding (for researchers)']","Somewhat shockingly, however, despite its prominence, we still have a poor understanding of what the effectiveness of BatchNorm is stemming from.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2905,Introduction,['Understanding (for researchers)'],"In fact, there are now a number of works that provide alternatives to BatchNorm [1, 3, 13, 31], but none of them seem to bring us any closer to understanding this issue.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2911,Introduction,"['Concreteness', 'Understanding (for researchers)']","Even though this explanation is widely accepted, we seem to have little concrete evidence supporting it.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2912,Introduction,['Understanding (for researchers)'],"In particular, we still do not understand the link between ICS and training performance.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2914,Introduction,['Understanding (for researchers)'],Our exploration lead to somewhat startling discoveries.,How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2919,Introduction,['Understanding (for researchers)'],We then turn our attention to identifying the roots of BatchNorm’s success.,How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2920,Introduction,"['Formal description/analysis', 'Understanding (for researchers)']","Specifically, we demon- strate that BatchNorm impacts network training in a fundamental way: it makes the landscape of the corresponding optimization problem significantly more smooth.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2921,Introduction,['Fast'],"This ensures, in particular, that the gradients are more predictive and thus allows for use of larger range of learning rates and faster network convergence.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2922,Introduction,"['Formal description/analysis', 'Quantitative evidence (e.g. experiments)']",We provide an empirical demonstration of these findings as well as their theoretical justification.,How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2923,Introduction,"['Formal description/analysis', 'Improvement']","We prove that, under natural conditions, the Lipschitzness of both the loss and the gradients (also known as (-smoothness [21]) are improved in models with BatchNorm.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2926,Introduction,"['Performance', 'Improvement']","In particular, they all offer similar improvements in the training performance.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2927,Introduction,"['Understanding (for researchers)', 'Progress']","We believe that understanding the roots of such a fundamental techniques as BatchNorm will let us have a significantly better grasp of the underlying complexities of neural network training and, in turn, will inform further algorithmic progress in this context.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2929,Introduction,['Understanding (for researchers)'],"In Section 2, we explore the connections between BatchNorm, optimization, and internal covariate shift.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2930,Introduction,"['Successful', 'Understanding (for researchers)']","Then, in Section 3, we demonstrate and analyze the exact roots of BatchNorm’s success in deep neural network training.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2931,Introduction,['Formal description/analysis'],We present our theoretical analysis in Section 4.,How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2932,Introduction,['Building on recent work'],We discuss further related work in Section 5 and conclude in Section 6.,How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2934,Conclusions,['Understanding (for researchers)'],"In this work, we have investigated the roots of BatchNorm’s effectiveness as a technique for training deep neural networks.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2935,Conclusions,['Understanding (for researchers)'],"We find that the widely believed connection between the performance of BatchNorm and the internal covariate shift is tenuous, at best.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2936,Conclusions,['Understanding (for researchers)'],"In particular, we demonstrate that existence of internal covariate shift, at least when viewed from the — generally adopted — distributional stability perspective, is not a good predictor of training performance.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2938,Conclusions,['Understanding (for researchers)'],"Instead, we identify a key effect that BatchNorm has on the training process: it reparametrizes the underlying optimization problem to make it more stable (in the sense of loss Lipschitzness) and smooth (in the sense of “effective” $-smoothness of the loss).",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2939,Conclusions,"['Effectiveness', 'Fast']","This implies that the gradients used in training are more predictive and well-behaved, which enables faster and more effective optimization.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2940,Conclusions,"['Robustness', 'Building on recent work', 'Unifying ideas or integrating components', 'Understanding (for researchers)']","This phenomena also explains and subsumes some of the other previously observed benefits of BatchNorm, such as robustness to hyperparameter setting and avoiding gradient explosion/vanishing.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2941,Conclusions,['Unifying ideas or integrating components'],We also show that this smoothing effect is not unique to BatchNorm.,How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2943,Conclusions,['Understanding (for researchers)'],We believe that these findings not only challenge the conventional wisdom about BatchNorm but also bring us closer to a better understanding of this technique.,How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2944,Conclusions,"['Effectiveness', 'Understanding (for researchers)']",We also view these results as an opportunity to encourage the community to pursue a more systematic investigation of the algorithmic toolkit of deep learning and the underpinnings of its effectiveness.,How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2945,Conclusions,['Understanding (for researchers)'],"Finally, our focus here was on the impact of BatchNorm on training but our findings might also shed some light on the BatchNorm’s tendency to improve generalization.",How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2948,Conclusions,['Understanding (for researchers)'],We hope that future work will investigate this intriguing possibility.,How Does Batch Normalization Help Optimization?,http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf
2950,Abstract,"['Accuracy', 'State-of-the-art']",Even small perturbations can cause state-of-the-art classifiers with high “standard” accuracy to produce an incorrect prediction with high confidence.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2951,Abstract,"['Generalization', 'Robustness', 'Understanding (for researchers)']","To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2952,Abstract,"['Robustness', 'Formal description/analysis', 'Data efficiency']","We show that already in a simple natural data model, the sample complexity of robust learning can be significantly larger than that of “standard” learning.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2953,Abstract,['Identifying limitations'],This gap is information theoretic and holds irrespective of the training algorithm or the model family.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2954,Abstract,"['Quantitative evidence (e.g. experiments)', 'Used in practice/Popular']",We complement our theoretical results with experiments on popular image classification datasets and show that a similar gap exists here as well.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2955,Abstract,"['Data efficiency', 'Easy to use']","We postulate that the difficulty of training robust classifiers stems, at least partially, from this inherently larger sample complexity.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2956,Introduction,"['Generalization', 'Robustness', 'Accuracy']","Modern machine learning models achieve high accuracy on a broad range of datasets, yet can easily be misled by small perturbations of their input.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2957,Introduction,"['Robustness', 'State-of-the-art']","While such perturbations are often simple noise to a human or even imperceptible, they cause state-of-the-art models to misclassify their input with high confidence.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2958,Introduction,['Security against an adversary'],"This phenomenon has first been studied in the context of secure machine learning for spam filters and malware classification [7, 16, 35].",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2960,Introduction,['Robustness'],"Overall, the existence of such adversarial examples raises concerns about the robustness of current classifiers.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2961,Introduction,"['Robustness', 'Understanding (for researchers)', 'Security against an adversary', 'Safety']","As we increasingly deploy machine learning systems in safety- and security-critical environments, it is crucial to understand the robustness properties of our models in more detail.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2962,Introduction,"['Robustness', 'Security against an adversary']",A growing body of work is exploring this robustness question from the security perspective by proposing attacks (methods for crafting adversarial examples) and defenses (methods for making classifiers robust to such perturbations).,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2964,Introduction,"['Robustness', 'Successful']","While there has been success with robust classifiers on simple datasets [31, 36, 44, 48], more complicated datasets still exhibit a large gap between “‘standard” and robust accuracy [3, 11].",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2965,Introduction,"['Robustness', 'Accuracy']",An implicit assumption underlying most of this work is that the same training dataset that enables good standard accuracy also suffices to train a robust model.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2966,Introduction,['Valid assumptions'],"However, it is unclear if this assumption is valid.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2967,Introduction,"['Generalization', 'Robustness', 'Understanding (for researchers)']","So far, the generalization aspects of adversarially robust classification have not been thoroughly investigated.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2968,Introduction,['Formal description/analysis'],"Since adversarial robustness is a learning problem, the statistical perspective is of integral importance.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2969,Introduction,"['Generalization', 'Robustness']",A key observation is that adversarial examples are not at odds with the standard notion of generalization as long as they occupy only a small total measure under the data distribution.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2970,Introduction,"['Generalization', 'Robustness']","So to achieve adversarial robustness, a classifier must generalize in a stronger sense.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2971,Introduction,"['Generalization', 'Understanding (for researchers)']","We currently do not have a good understanding of how such a stronger notion of generalization compares to standard “benign” generalization, i.e., without an adversary.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2972,Introduction,"['Generalization', 'Robustness', 'Formal description/analysis', 'Understanding (for researchers)']","In this work, we address this gap and explore the statistical foundations of adversarially robust generalization.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2973,Introduction,"['Robustness', 'Data efficiency']",We focus on sample complexity as a natural starting point since it underlies the core question of when it is possible to learn an adversarially robust classifier.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2974,Introduction,['Concreteness'],"Concretely, we pose the following question:",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2975,Introduction,"['Generalization', 'Robustness', 'Data efficiency']",How does the sample complexity of standard generalization compare to that of adversarially robust generalization?,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2976,Introduction,"['Robustness', 'Performance']","Put differently, we ask if a dataset that allows for learning a good classifier also suffices for learning a robust one.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2977,Introduction,"['Generalization', 'Robustness']","To study this question, we analyze robust generalization in two distributional models.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2978,Introduction,"['Generalization', 'Formal description/analysis', 'Exactness']","By focusing on specific distributions, we can establish information-theoretic lower bounds and describe the exact sample complexity requirements for generalization.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2979,Introduction,"['Generalization', 'Robustness', 'Data efficiency']","We find that even for a simple data distribution such as a mixture of two class-conditional Gaussians, the sample complexity of robust generalization is significantly larger than that of standard generalization.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2980,Introduction,['Formal description/analysis'],Our lower bound holds for any model and learning algorithm.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2981,Introduction,"['Formal description/analysis', 'Identifying limitations']",Hence no amount of algorithmic ingenuity is able to overcome this limitation.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2982,Introduction,"['Robustness', 'Progress']","In spite of this negative result, simple datasets such as MNIST have recently seen significant progress in terms of adversarial robustness [31, 36, 44, 48].",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2983,Introduction,"['Robustness', 'Accuracy']","The most robust models achieve accuracy around 90% against large ¢,,-perturbations.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2984,Introduction,['Understanding (for researchers)'],"To better understand this discrepancy with our first theoretical result, we also study a second distributional model with binary features.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2985,Introduction,['Generalization'],This binary data model has the same standard generalization behavior as the previous Gaussian model.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2986,Introduction,"['Generalization', 'Robustness', 'Data efficiency']","Moreover, it also suffers from a significantly increased sample complexity whenever one employs linear classifiers to achieve adversarially robust generalization.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2987,Introduction,"['Generalization', 'Formal description/analysis', 'Data efficiency']","Nevertheless, a slightly non-linear classifier that utilizes thresholding turns out to recover the smaller sample complexity of standard generalization.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2988,Introduction,['Easy to use'],"Since MNIST is a mostly binary dataset, our result provides evidence that £,,-robustness on MNIST is significantly easier than on other datasets.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2989,Introduction,"['Generalization', 'Robustness', 'Formal description/analysis', 'Data efficiency']","Moreover, our results show that distributions with similar sample complexity for standard generalization can still exhibit considerably different sample complexity for robust generalization.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2990,Introduction,"['Formal description/analysis', 'Quantitative evidence (e.g. experiments)']","To complement our theoretical results, we conduct a range of experiments on MNIST, CIFARLO, and SVHN.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2991,Introduction,"['Robustness', 'Quantitative evidence (e.g. experiments)']","By subsampling the datasets at various tates, we study the impact of sample size on adversarial robustness.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2992,Introduction,"['Robustness', 'Quantitative evidence (e.g. experiments)', 'Accuracy']","When plotted as a function of training set size, our results show that the standard accuracy on SVHN indeed plateaus well before the adversarial accuracy reaches its maximum.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2993,Introduction,['Data efficiency'],"On MNIST, explicitly adding thresholding to the model during training significantly reduces the sample complexity, similar to our upper bound in the binary data model.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2994,Introduction,"['Accuracy', 'Security against an adversary']","On CIFAR 10, the situation is more nuanced because there are no known approaches that achieve more than 50% accuracy even against a mild adversary.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2995,Introduction,['Avoiding train/test discrepancy'],"But as we show below, there is clear evidence for overfitting in the current state-of-the-art methods.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2996,Introduction,"['Robustness', 'Accuracy']","Overall, our results suggest that current approaches may be unable to attain higher adversarial accuracy on datasets such as CIFAR1O for a fundamental reason: the dataset may not be large enough to train a standard convolutional network robustly.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2997,Introduction,['Formal description/analysis'],"Moreover, our lower bounds illustrate that the existence of adversarial examples should not necessarily be seen as a shortcoming of specific classification methods.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
2998,Introduction,"['Formal description/analysis', 'Accuracy']","Already in a simple data model, adversarial examples provably occur for any learning approach, even when the classifier already achieves high standard accuracy.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3001,Introduction,"['Generalization', 'Robustness', 'Quantitative evidence (e.g. experiments)']","Before we describe our main results, we briefly highlight the importance of generalization for adversarial robustness via two experiments on MNIST and CIFAR10.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3002,Introduction,"['Robustness', 'Accuracy']","In both cases, our goal is to learn a classifier that achieves good test accuracy even under ¢..-bounded perturbations.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3006,Introduction,"['Robustness', 'Qualitative evidence (e.g. examples)', 'Accuracy', 'Avoiding train/test discrepancy']","The results show that on MNIST, robust optimization is able to learn a model with around 90% adversarial accuracy and a relatively small gap between training and test error.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3008,Introduction,['Avoiding train/test discrepancy'],"Here, the model (a wide residual network [61)]) is still able to fully fit the training set even against an adversary, but the generalization gap is significantly larger.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3009,Introduction,['Accuracy'],"The model only achieves 47% adversarial test accuracy, which is about 50% lower than its training accuracy.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3010,Introduction,"['Generalization', 'Accuracy']","Moreover, the standard test accuracy is about 87%, so the failure of generalization indeed primarily occurs in the context of adversarial robustness.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3011,Introduction,['Avoiding train/test discrepancy'],This failure may be surprising particularly since properly tuned convolutional networks rarely overfit much on standard vision datasets.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3013,Introduction,['Formal description/analysis'],"In the next section, we describe our main theoretical results at a high level.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3014,Introduction,['Quantitative evidence (e.g. experiments)'],Section 3]complements these results with experiments.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3017,Discussion and Conclusions,"['Robustness', 'Understanding (for researchers)']",The vulnerability of neural networks to adversarial perturbations has recently been a source of much discussion and is still poorly understood.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3020,Discussion and Conclusions,['Robustness'],"We show that for a natural data distribution (the Gaussian model), the model class we train does not matter and a standard linear classifier achieves optimal robustness.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3022,Discussion and Conclusions,['Data efficiency'],"For other data models (such as MNIST or the Bernoulli model), our results demonstrate that non-linearities are indispensable to learn from few samples.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3023,Discussion and Conclusions,['Security against an adversary'],"This dichotomy provides evidence that defenses against adversarial examples need to be tailored to the specific dataset (even for the same type of perturbations) and hence may be more complicated than a single, broad approach.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3024,Discussion and Conclusions,"['Generalization', 'Robustness']","Understanding the interactions between robustness, classifier model, and data distribution from the perspective of generalization is an important direction for future work.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3025,Discussion and Conclusions,['Concreteness'],We refer the reader to Section |B]in the appendix for concrete questions in this direction.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3027,Discussion and Conclusions,"['Robustness', 'Security against an adversary']","While this is a natural scenario from a security point of view, other setups can be more relevant in different robustness contexts.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3028,Discussion and Conclusions,['Robustness'],"For instance, we may want a classifier that is robust to small changes between the training and test distribution.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3029,Discussion and Conclusions,['Accuracy'],This can be formalized as the classification accuracy on unperturbed examples coming from an adversarially modified distribution.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3031,Discussion and Conclusions,['Formal description/analysis'],"Interestingly, our lower bound for the Gaussian model also applies to such worst-case distributional shifts.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3032,Discussion and Conclusions,['Formal description/analysis'],"In particular, if the adversary is allowed to shift the mean 6” by a vector in BS,, our proof sketched in Section |C)transfers to the distribution shift setting.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3034,Discussion and Conclusions,"['Robustness', 'Formal description/analysis', 'Applies to real world']","What do our results mean for robust classification of real images? Our Gaussian lower bound implies that if an algorithm works for all (or most) settings of the unknown parameter 6%, then achieving strong &,.-robustness requires a sample complexity increase that is polynomial in the dimension.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3036,Discussion and Conclusions,['Robustness'],"It is conceivable that the noise scale o is significantly smaller for real image datasets, making robust classification easier.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3037,Discussion and Conclusions,['Applies to real world'],"Even if that was not the case, a good algorithm could work for the parameters 6* that correspond to real datasets while not working for most other parameters.",Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3040,Discussion and Conclusions,['Robustness'],Our work suggests that there are trade-offs with robustness here and that adding more prior information could help to learn more robust classifiers.,Adversarially Robust Generalization Requires More Data,http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf
3041,Abstract,"['Realistic output', 'Preciseness']","We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3043,Abstract,['Qualitative evidence (e.g. examples)'],"Without modeling temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3044,Abstract,['Novelty'],"In this paper, we propose a video-to-video synthesis approach under the generative adversarial learning framework.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3045,Abstract,"['Generalization', 'Realistic output', 'Large scale', 'Careful']","Through carefully-designed generators and discriminators, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3046,Abstract,['Quantitative evidence (e.g. experiments)'],Experiments on multiple benchmarks show the advantage of our method compared to strong baselines.,Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3047,Abstract,"['Large scale', 'State-of-the-art']","In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3048,Abstract,['Performance'],"Finally, we apply our method to future video prediction, outperforming several competing systems.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3049,Abstract,['Facilitating use (e.g. sharing code)'],"Code, models, and more results are available at our website.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3050,Introduction,"['Intelligent agents', 'World model']",The capability to model and recreate the dynamics of our visual world is essential to building intelligent agents.,Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3051,Introduction,['Generalization'],"Apart from purely scientific interests, learning to synthesize continuous visual experiences has a wide range of applications in computer vision, robotics, and computer graphics.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3052,Introduction,"['Approximation', 'Data efficiency', 'World model']","For example, in model-based reinforcement learning [2,24], a video synthesis model finds use in approximating visual dynamics of the world for training the agent with less amount of real experience data.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3053,Introduction,"['Realistic output', 'Easy to work with']","Using a learned video synthesis model, one can generate realistic videos without explicitly specifying scene geometry, materials, lighting, and dynamics, which would be cumbersome but necessary when using a standard graphics rendering engine [35].",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3055,Introduction,['Novelty'],"In this paper, we study a new form: video-to-video synthesis.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3057,Introduction,"['Novelty', 'Generalization']","To the best of our knowledge, a general-purpose solution to video-to-video synthesis has not yet been explored by prior work, although its image counterpart, the image-to-image translation problem, is a popular research topic [6, 31,33, 43, 44, 63,65, 72, 81, 82].",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3058,Introduction,['Building on recent work'],"Our method is inspired by previous application-specific video synthesis methods [58, 60, 61,74].",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3059,Introduction,['Realistic output'],"We cast the video-to-video synthesis problem as a distribution matching problem, where the goal is to train a model such that the conditional distribution of the synthesized videos given input videos resembles that of real videos.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3061,Introduction,"['Novelty', 'Realistic output', 'Large scale', 'Careful']","With carefully-designed generators and discriminators, and a new spatio-temporal learning objective, our method can learn to synthesize high-resolution, photore- alistic, temporally coherent videos.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3063,Introduction,"['Qualitative evidence (e.g. examples)', 'Diverse outputs']","Conditioning on the same input, our model can produce videos with diverse appearances.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3064,Introduction,"['Generalization', 'Realistic output', 'Quantitative evidence (e.g. experiments)']",We conduct extensive experiments on several datasets on the task of converting a sequence of segmentation masks to photorealistic videos.,Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3065,Introduction,"['Realistic output', 'Quantitative evidence (e.g. experiments)', 'Qualitative evidence (e.g. examples)']",Both quantitative and qualitative results indicate that our synthesized footage looks more photorealistic than those from strong baselines.,Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3067,Introduction,"['Realistic output', 'Large scale']","We further demonstrate that the proposed approach can generate photorealistic 2K resolution videos, up to 30 seconds long.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3068,Introduction,"['Flexibility/Extensibility', 'Controllability (of model owner)']",Our method also grants users flexible high-level control over the video generation results.,Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3069,Introduction,"['Controllability (of model owner)', 'User influence', 'Easy to work with']","For example, a user can easily replace all the buildings with trees in a street view video.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3070,Introduction,['Generalization'],"In addition, our method works for other input video formats such as face sketches and body poses, enabling many applications from face swapping to human motion transfer.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3071,Introduction,['Performance'],"Finally, we extend our approach to future prediction and show that our method can outperform existing systems.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3072,Introduction,['Facilitating use (e.g. sharing code)'],"Please visit our website for code, models, and more results.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3074,Discussion,['Generality'],We present a general video-to-video synthesis framework based on conditional GANs.,Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3075,Discussion,"['Realistic output', 'Large scale', 'Careful']","Through carefully-designed generators and discriminators as well as a spatio-temporal adversarial objective, we can synthesize high-resolution, photorealistic, and temporally consistent videos.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3076,Discussion,['State-of-the-art'],Extensive experiments demonstrate that our results are significantly better than the results by state-of-the-art methods.,Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3078,Discussion,"['Performance', 'Identifying limitations']","Although our approach outperforms previous methods, our model still fails in a couple of situations.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3081,Discussion,['Realistic output'],"Furthermore, our model still can not guarantee that an object has a consistent appearance across the whole video.",Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3083,Discussion,['Realistic output'],This issue might be alleviated if object tracking information is used to enforce that the same object shares the same appearance throughout the entire video.,Video-to-Video Synthesis,http://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf
3091,abstract,['Novelty'],All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks,A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning,http://icml2008.cs.helsinki.fi/papers/391.pdf
3092,abstract,"['Generalization', 'State-of-the-art']","We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art performance.",A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning,http://icml2008.cs.helsinki.fi/papers/391.pdf
3094,introduction,['Used in practice/Popular'],"Currentend applications include information extraction, machine translation, summarization, search and human-computer interfaces",A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning,http://icml2008.cs.helsinki.fi/papers/391.pdf
3099,introduction,['Performance'],"In particular, many systems possess three failings in thisregard: (i) they are shallowin the sense that the clas-sifier is often linear, (ii) for good performance witha linear classifier they must incorporate many hand-engineered features specific for the task; and (iii) they cascade features learnt separately from other tasks, thus propagating errors.",A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning,http://icml2008.cs.helsinki.fi/papers/391.pdf
3102,introduction,['Generality'],"We define a rather general convolutional network architecture and describe its application to many well known NLP tasks including part-of-speech tagging, chunking, named-entity recognition, learning a language modeland the task of semantic role-labeling",A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning,http://icml2008.cs.helsinki.fi/papers/391.pdf
3103,introduction,['Unifying ideas or integrating components'],All of these tasks are integrated into a single systemwhich is trained jointly,A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning,http://icml2008.cs.helsinki.fi/papers/391.pdf
3106,introduction,['Novelty'],Training this task jointly with the other tasks comprises a novel form ofsemi-supervised learning,A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning,http://icml2008.cs.helsinki.fi/papers/391.pdf
3108,introduction,['Performance'],We show that both (i) multitask learning and (ii) semi-supervised learning significantly improve performanceon this taskin the absence of hand-engineered features,A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning,http://icml2008.cs.helsinki.fi/papers/391.pdf
3110,introduction,['Generality'],"The article is structured as follows. In Section 2 wedescribe each of the NLP tasks we consider, and in Section 3 we define the general architecture that we use to solve all the tasks.",A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning,http://icml2008.cs.helsinki.fi/papers/391.pdf
3112,introduction,['Quantitative evidence (e.g. experiments)'],"Section 6 gives experimental results of our system, and Section 7 concludes with a discussion of our results and possible directions for future research.",A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning,http://icml2008.cs.helsinki.fi/papers/391.pdf
3113,conclusion,['Generality'],We proposed a general deep NN architecture for NLP,A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning,http://icml2008.cs.helsinki.fi/papers/391.pdf
3114,conclusion,"['Large scale', 'Fast']",Our architecture is extremely fast enabling us to take advantage of huge databases (e.g. 631 million wordsfrom Wikipedia).,A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning,http://icml2008.cs.helsinki.fi/papers/391.pdf
3116,conclusion,['Generalization'],We demonstratedthat learning tasks simultaneously can improve generalization performance,A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning,http://icml2008.cs.helsinki.fi/papers/391.pdf
3117,conclusion,['State-of-the-art'],"In particular, when training the SRL task jointly with our language model our architecture achieved state-of-the-art performance in SRL without any explicit syntactic features",A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning,http://icml2008.cs.helsinki.fi/papers/391.pdf
3121,abstract,['Identifying limitations'],"In this work, we demonstrate that the standard PCA deflation procedure is seldom appropriate for the sparse PCA setting. ",Deflation Methods for Sparse PCA,https://proceedings.neurips.cc/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf
3124,abstract,"['Generalization', 'Performance', 'Applies to real world']",The result is a generalized deflation procedure that typically outperforms more standard techniques on real-world datasets.,Deflation Methods for Sparse PCA,https://proceedings.neurips.cc/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf
3125,introduction,['Used in practice/Popular'],"Principal component analysis (PCA) is a popular change of variables technique used in data compression, predictive modeling, and visualization. ",Deflation Methods for Sparse PCA,https://proceedings.neurips.cc/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf
3127,introduction,['Formal description/analysis'],"Often, PCA is formulated as an eigenvalue decomposition problem: each eigenvector of the sample covariance matrix of a data set corresponds to the loadings or coefficients of a principal component. ",Deflation Methods for Sparse PCA,https://proceedings.neurips.cc/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf
3132,introduction,"['Generalization', 'Interpretable (to users)', 'Fast']","Sparsity is desirable as it often leads to more interpretable results, reduced computation time, and improved generalization. ",Deflation Methods for Sparse PCA,https://proceedings.neurips.cc/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf
3135,introduction,['Approximation'],"The former is an NP-hard problem, and a variety of relaxations and approximate solutions have been developed in the literature [1, 2, 9, 10, 12, 16, 17]. ",Deflation Methods for Sparse PCA,https://proceedings.neurips.cc/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf
3121,introduction,['Identifying limitations'],"In this work, we demonstrate that the standard PCA deflation procedure is seldom appropriate for the sparse PCA setting. ",Deflation Methods for Sparse PCA,https://proceedings.neurips.cc/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf
3137,introduction,"['Novelty', 'Approximation']","To rectify the situation, we first develop several heuristic deflation alternatives with more desirable properties. ",Deflation Methods for Sparse PCA,https://proceedings.neurips.cc/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf
3123,introduction,['Formal description/analysis'],We then reformulate the sparse PCA optimization problem to explicitly reflect the maximum additional variance objective on each round. ,Deflation Methods for Sparse PCA,https://proceedings.neurips.cc/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf
3124,introduction,"['Generalization', 'Performance', 'Applies to real world']",The result is a generalized deflation procedure that typically outperforms more standard techniques on real-world datasets.,Deflation Methods for Sparse PCA,https://proceedings.neurips.cc/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf
3138,introduction,['Identifying limitations'],The remainder of the paper is organized as follows. In Section 2 we discuss matrix deflation as it relates to PCA and sparse PCA. We examine the failings of typical PCA deflation in the sparse setting and develop several alternative deflation procedures. ,Deflation Methods for Sparse PCA,https://proceedings.neurips.cc/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf
3139,introduction,"['Novelty', 'Generalization', 'Formal description/analysis']","In Section 3, we present a reformulation of the standard iterative sparse PCA optimization problem and derive a generalized deflation procedure to solve the reformulation. ",Deflation Methods for Sparse PCA,https://proceedings.neurips.cc/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf
3140,introduction,"['Novelty', 'Applies to real world']","Finally, in Section 4, we demonstrate the utility of our newly derived deflation techniques on real-world datasets.
",Deflation Methods for Sparse PCA,https://proceedings.neurips.cc/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf
3141,conclusion,"['Novelty', 'Identifying limitations']","In this work, we have exposed the theoretical and empirical shortcomings of Hotelling’s deflation in the sparse PCA setting and developed several alternative methods more suitable for non-eigenvector deflation. ",Deflation Methods for Sparse PCA,https://proceedings.neurips.cc/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf
3144,abstract,['Fast'],"Learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3145,abstract,['Fast'],"Inspired by local learning,  we propose a method to speed up standard Gaussian  process  regression  (GPR)  with  local  GP  models  (LGP).",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3150,abstract,"['Quantitative evidence (e.g. experiments)', 'Performance', 'Accuracy']",Comparisons with other non-parametric regression methods show that LGP has higher accuracy than LWPR and close to the performance of standard GPR andν-SVR,Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3151,introduction,['Preciseness'],Precise models of technical systems can be crucial in technical applications,Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3152,introduction,"['Controllability (of model owner)', 'Accuracy']","Especially in robot tracking control, only a well-estimated inverse dynamics model can allow both high accuracy and compliant control",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3154,introduction,['Fast'],"For most real-time applications, online model learning poses a difficult regression problem due to three constraints, i.e., firstly, the learning and prediction process should be very fast (e.g., learning needs to take place at a speed of 20-200Hz and prediction at 200Hz to a 1000Hz).",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3155,introduction,"['Large scale', 'Scales up']","Secondly, the learning system needs to be capable at dealing with large amounts of data (i.e., with data arrivingat 200Hz,  less than ten minutes of runtime will result in more than a million data points).",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3158,introduction,['Approximation'],"Here, the true function is approximated with local linear functionscovering the relevant state-space and online learning became computationally feasible due to low computational demands of the local projection regression which can be performed in real-time",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3161,introduction,"['Approximation', 'Accuracy', 'Powerful']",A powerful alternative for accurate function approximation in high-dimensional space is Gaussian process regression (GPR) [1].,Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3162,introduction,"['Flexibility/Extensibility', 'Easy to work with']","Since the hyperparameters of a GP model can be adjusted by maximizing the marginal likelihood, GPR requires little effort and is easy and flexible to use",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3163,introduction,"['Identifying limitations', 'Scales up']","However, the main limitation of GPR is that the computational complexity scales cubically with the training examples n",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3164,introduction,"['Large scale', 'Fast']","This drawback prevents GPR from applications which need large amounts of training data and require fast computation, e.g., online learning of inverse dynamics model for model-based robot control",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3170,introduction,"['Performance', 'Optimal']",The ME performance depends largely on the way of partitioning the training data and the choice of an optimal number oflocal models for a particular data set [4].,Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3171,introduction,"['Accuracy', 'Unifying ideas or integrating components', 'Fast']","In this paper, we combine the basic idea behind both approaches, i.e., LWPR and GPR, attemptingto get as close as possible to the speed of local learning while having a comparable accuracy to Gaussian process regression",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3172,introduction,"['Low cost', 'Building on recent work']","This results in an approach inspired by [6, 8] using many local GPs inorder to obtain a significant reduction of the computational cost during both prediction and learning step allowing the application to online learning",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3175,introduction,['Unifying ideas or integrating components'],"Subsequently, we describe our local Gaussian process models (LGP) approach in Section 3 and discuss how it inherits the advantages of both GPR and LWPR.",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3176,introduction,"['Quantitative evidence (e.g. experiments)', 'Performance', 'Accuracy']","Furthermore,  the learning accuracy and performance of our LGP approach will be compared with other important standard methods in Section 4, e.g., LWPR [8], standard GPR [1], sparse online Gaussian process regression (OGP) [5] and ν-support vector regression (ν-SVR) [11], respectively",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3177,introduction,"['Quantitative evidence (e.g. experiments)', 'Applies to real world']","Finally, our LGP method is evaluated for an online learning of the inverse dynamics models of real robots for accurate tracking control in Section 5",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3180,introduction,"['Novelty', 'Applies to real world']","To our best knowledge, it is the first time that GPR is successfully used for high-speed online model learning in real time control on a physical robot.",Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3181,introduction,"['Accuracy', 'State-of-the-art']",We present the results on a version of the Barrett WAMshowing that with the online learned model using LGP the tracking accuracy is superior compared to state-of-the art model-based methods [10] while remaining fully compliant.,Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3182,Conclusion,"['Accuracy', 'Unifying ideas or integrating components', 'Fast']",We combine with LGP the fast computation of local regression with more accurate regression methods while having little tuning efforts.  ,Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3183,Conclusion,"['Low cost', 'Accuracy']",LGP achieves higher learning accuracy compared to locally linear methods such as LWPR while having less computational cost compared to GPR and ν-SVR.,Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3184,Conclusion,"['Generalization', 'Low cost']",The reducing cost allows LGP for model online learning which is necessary in oder to generalize the model for all trajectories,Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3185,Conclusion,"['Performance', 'State-of-the-art']",Model-based tracking control using online learned model achieves superior control performance compared to the state-of-the-art method as well as offline learned modelfor unknown trajectories.,Local Gaussian Process Regressionfor Real Time Online Model Learning and Control,https://proceedings.neurips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf
3186,abstract,"['Accuracy', 'Building on recent work', 'Useful']","Metric  learning  algorithms  can  provide  useful  distance  functions  for  a  variety of domains, and recent work has shown good accuracy for problems where the learner can access all distance constraints at once",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3187,abstract,['Applies to real world'],"However, in many real applications, constraints are only available incrementally, thus necessitating methods that can perform online updates to the learned metric",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3188,abstract,"['Theoretical guarantees', 'Performance', 'Used in practice/Popular']","Existing online algorithmsoffer  bounds on  worst-case  performance,  but  typically  do  not  perform  well  inpractice as compared to their offline counterparts",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3189,abstract,['Novelty'],We present a new online metric learning algorithm that updates a learned Mahalanobis metric based on LogDetregularization and gradient descent,Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3190,abstract,"['Formal description/analysis', 'Theoretical guarantees', 'Quantitative evidence (e.g. experiments)']","We prove theoretical worst-case performance bounds,  and  empirically  compare  the  proposed method  against  existing  online metric learning algorithms",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3191,abstract,"['Efficiency', 'Practical', 'Fast']","To further boost the practicality of our approach, we develop an online locality-sensitive hashing scheme which leads to efficient updates to data structures used for fast approximate similarity search",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3192,abstract,"['Quantitative evidence (e.g. experiments)', 'Performance']",We demonstrate our algorithm on multiple datasets and show that it outperforms relevant baselines.,Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3194,introduction,['Applies to real world'],"Such algorithms have been applied to a variety of real-world learning tasks,  ranging from object recognition and human body pose estimation [5,  9],  to digit recognition [7], and software support [4] applications.",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3195,introduction,['Successful'],Most successful results have relied on having access to all constraints at the onset of the metric learning.,Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3196,introduction,['Flexibility/Extensibility'],"However, in many real applications, the desired distance function may need to change gradually over time as additional information or constraints are received.",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3197,introduction,['Applies to real world'],"For instance, in image search applications on the internet, online click-through data that is continually collected may impact the desired distance function.",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3198,introduction,['Building on recent work'],"To address this need, recentwork ononline metric learning algorithms attempts to handle constraints that are received one at atime [13, 4].",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3199,introduction,"['Performance', 'Identifying limitations', 'Fast']","Unfortunately, current methods suffer from a number of drawbacks, including speed, bound quality, and empirical performance",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3200,introduction,"['Large scale', 'Fast']",Further complicating this scenario is the fact that fast retrieval methods must be in place on topof the learned metrics for many applications dealing with large-scale databases,Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3201,introduction,"['Large scale', 'Fast']","For example, in image search applications, relevant images within very large collections must be quickly returned to the user, and constraints and user queries may often be intermingled across time",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3202,introduction,['Fast'],Thus a good online metric learner must also be able to support fast similarity search routines,Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3204,introduction,"['Efficiency', 'Applies to real world', 'Practical']",The goal of this work is to make metric learning practical for real-world learning tasks in which both constraints and queries must be handled efficiently in an online manner,Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3206,introduction,"['Novelty', 'Theoretical guarantees', 'Building on recent work']","Thenew algorithm is inspired by the metric learning algorithm studied in [4]; however, while the loss bounds for the latter method are dependent on the input data, our loss bounds are independent ofthe sequence of constraints given to the algorithm",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3207,introduction,['Fast'],"Furthermore, unlike the Pseudo-metric OnlineLearning Algorithm (POLA) [13], another recent online technique, our algorithm requires no eigenvector computation, making it considerably faster in practice",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3208,introduction,['Large scale'],We further show how our algorithm can be integrated with large-scale approximate similarity search,Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3209,introduction,"['Novelty', 'Accuracy']","We devise a method to incrementally update locality-sensitive hash keys during the updates of the metric learner, making it possible to perform accurate sublinear time nearest neighbor searches over the data in an online manner.",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3210,introduction,['Quantitative evidence (e.g. experiments)'],We compare our algorithm to related existing methods using a variety of standard data sets,Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3211,introduction,['Performance'],"We show that our method outperforms existing approaches, and even performs comparably to several offline metric learning algorithms",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3212,introduction,"['Quantitative evidence (e.g. experiments)', 'Large scale', 'Accuracy', 'Effectiveness', 'Fast']","To evaluate our approach for indexing a large-scale database, we include experiments with a set of 300,000 image patches; our online algorithm effectively learns to compare patches, and our hashing construction allows accurate fast retrieval for online queries",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3215,introduction,"['Theoretical guarantees', 'Fast']","The POLA algorithm [13], on the other hand, is an approach for online learning of Mahalanobis metrics that optimizes a large-margin objective and has provable regret bounds, although eigenvector computation is required at each iteration to enforce positive definiteness, whichcan be slow in practice",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3217,introduction,"['Theoretical guarantees', 'Performance', 'Practical']","However, because of the particular form of theonline update, positive-definiteness still must be carefully enforced, which impacts bound quality and empirical performance, making it undesirable for both theoretical and practical purposes.",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3218,introduction,"['Theoretical guarantees', 'Efficiency']","Incontrast, our proposed algorithm has strong bounds, requires no extra work for enforcing positive definiteness, and can be implemented efficiently.",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3220,introduction,"['Low cost', 'Fast']",Fast search methods are becoming increasingly necessary for machine learning tasks that must cope with large databases,Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3221,introduction,"['Approximation', 'Effectiveness']",Locality-sensitive hashing [6] is an effective technique that performs approximate nearest neighbor searches in time that is sub-linear in the size of the database,Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3224,Conclusions,"['Generalization', 'Fast']","We have developed an online metric learning algorithm together with a method to perform online updates to fast similarity search structures, and have demonstrated their applicability and advantages on a variety of data sets",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3225,Conclusions,"['Theoretical guarantees', 'Performance', 'State-of-the-art']","We have proven regret bounds for our online learner that offer improved reliability  over state-of-the-art methods in  terms of regret bounds,  and empirical performance.",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3227,Conclusions,"['Formal description/analysis', 'Automatic']","For future work, we hope to tune the LSH parameters automatically using a deeper theoretical analysis of our hash key updates in conjunction with the relevant statistics of the online similarity search task at hand.",Online Metric Learning and Fast Similarity Search,https://proceedings.neurips.cc/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf
3232,abstract,['Stable'],"This results in a significant reduction in unknowns,  and corresponding stability in estimation",NonrigidStructure from Motion in Trajectory Space,https://proceedings.neurips.cc/paper/2008/file/dc82d632c9fcecb0778afbc7924494a6-Paper.pdf
3233,abstract,"['Quantitative evidence (e.g. experiments)', 'Qualitative evidence (e.g. examples)']","We report empirical performance, quantitatively using motion capture data, and qualitatively on several video sequences exhibiting nonrigid motions including piece-wise rigid motion, partially nonrigid motion (such as a facial expression), and highly nonrigid motion (such as a person dancing).",NonrigidStructure from Motion in Trajectory Space,https://proceedings.neurips.cc/paper/2008/file/dc82d632c9fcecb0778afbc7924494a6-Paper.pdf
3235,introduction,['Flexibility/Extensibility'],"Factorization  approaches, first proposed for recovering rigid structure by Tomasi and Kanade in [1], were extendedto handle nonrigidity in the seminal paper by Bregleret al.in [2].",NonrigidStructure from Motion in Trajectory Space,https://proceedings.neurips.cc/paper/2008/file/dc82d632c9fcecb0778afbc7924494a6-Paper.pdf
3249,introduction,['State-of-the-art'],"We show that while the inherent representative power of both shape and trajectory projections of structure  data  are  equal  (a  duality  exists),  the  significant  reduction  in  number  of  unknowns  that results from knowing the basis apriori allows us to handle much more nonrigidity of deformation than state of the art methods, like [4] and [5].",NonrigidStructure from Motion in Trajectory Space,https://proceedings.neurips.cc/paper/2008/file/dc82d632c9fcecb0778afbc7924494a6-Paper.pdf
3251,introduction,['Novelty'],"To the best of our knowledge, we are the first to show reasonable reconstructions of highly nonrigid motions from a single video sequence without making object specific assumptions",NonrigidStructure from Motion in Trajectory Space,https://proceedings.neurips.cc/paper/2008/file/dc82d632c9fcecb0778afbc7924494a6-Paper.pdf
3255,conclusion,['Applies to real world'],"Unlike earlier approaches that require an object-specific shape basisto be estimated for each new video sequence, we demonstrate that a generic trajectory basis can be defined that can compactly represent the motion of a wide variety of real deformations.",NonrigidStructure from Motion in Trajectory Space,https://proceedings.neurips.cc/paper/2008/file/dc82d632c9fcecb0778afbc7924494a6-Paper.pdf
3257,conclusion,"['Quantitative evidence (e.g. experiments)', 'Stable']","Our experiments show that there is a relationship between camera motion, degree of object deformation, and reconstruction stability",NonrigidStructure from Motion in Trajectory Space,https://proceedings.neurips.cc/paper/2008/file/dc82d632c9fcecb0778afbc7924494a6-Paper.pdf
3258,conclusion,['Stable'],"We observe that as the motion of the camera increases with respect to the degree of deformation, the reconstruction stability increases",NonrigidStructure from Motion in Trajectory Space,https://proceedings.neurips.cc/paper/2008/file/dc82d632c9fcecb0778afbc7924494a6-Paper.pdf
3259,conclusion,['Unifying ideas or integrating components'],"Future directions of research include experimenting with different unitary transform bases to verify that DCT basis are,  in fact,  the best generic basis to use,  and developing a synergistic approach to use both shape and trajectory bases concurrently",NonrigidStructure from Motion in Trajectory Space,https://proceedings.neurips.cc/paper/2008/file/dc82d632c9fcecb0778afbc7924494a6-Paper.pdf
3261,abstract,"['Generality', 'Used in practice/Popular']","We develop a general theory for a variant of the popular error correcting outputcode scheme,  using ideas from compressed sensing for exploiting this sparsity",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3262,abstract,['Simplicity'],The method can be regarded as a simple reduction from multi-label regressionproblems to binary regression problems.,Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3263,abstract,['Efficiency'],"We show that the number of subprob-lems need only be logarithmic in the total number of possible labels, making thisapproach radically more efficient than others.",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3264,abstract,"['Robustness', 'Formal description/analysis', 'Theoretical guarantees', 'Generality']","We also state and prove robustnessguarantees for this method in the form of regret transform bounds (in general),and also provide a more detailed analysis for the linear prediction setting.",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3271,introduction,['Large scale'],"In this work, we consider how this sparsity in the output space, oroutputsparsity, eases the burden of large-scale multi-label learning",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3282,introduction,['Accuracy'],"Thus  we  are  interested  in  reconstruction  accuracy  of  predictions,averaged over the data distribution.",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3283,introduction,['Formal description/analysis'],The main contributions of this work are:1.  A formal application of compressed sensing to prediction problems with output sparsity.,Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3284,introduction,"['Large scale', 'Efficiency']","An efficient output coding method,  in which the number of required predictions is onlylogarithmic in the number of labelsd, making it applicable to very large-scale problems.",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3285,introduction,"['Robustness', 'Formal description/analysis', 'Theoretical guarantees', 'Generality']","Robustness guarantees, in the form of regret transform bounds (in general) and a furtherdetailed analysis for the linear prediction setting",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3286,introduction,['Generality'],Prior work.The ubiquity of multi-label prediction problems in domains ranging from multiple ob-ject recognition in computer vision to automatic keyword tagging for content databases has spurredthe development of numerous general methods for the task,Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3288,introduction,['Efficiency'],"Whenstructure can be imposed on the label space (e.g.class hierarchy), efficient learning and predictionmethods are often possible [5, 6, 7, 8, 9].",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3290,introduction,['Generality'],"Moreover, our method is general enough totake advantage of structured notions of sparsity (e.g.group sparsity) when available [10",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3291,introduction,"['Quantitative evidence (e.g. experiments)', 'Efficiency', 'Building on recent work']","Recently,heuristics have been proposed for discovering structure in large output spaces that empirically offersome degree of efficiency [11].",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3292,introduction,"['Quantitative evidence (e.g. experiments)', 'Building on recent work', 'Used in practice/Popular']","As previously mentioned,  our work is most closely related to the class of output coding methodfor multi-class prediction, which was first introduced and shown to be useful experimentally in [2].",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3293,introduction,['Theoretical guarantees'],"Relative to this work, we expand the scope of the approach to multi-label prediction and providebounds on regret and error which guide the design of codes.",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3295,introduction,['Formal description/analysis'],"However, it does not provide significant guidance in thechoice of encoding method, or the feedback between encoding and decoding which we analyze here.",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3297,introduction,"['Robustness', 'Formal description/analysis']","This is proved and analyzed in [13], where it is also shown that using aHadamard code creates a robust consistent predictor when reduced to binary regression",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3298,introduction,"['Robustness', 'Theoretical guarantees', 'Label efficiency (reduced need for labeled data)']","Compared to this method, our approach achieves the same robustness guarantees up to a constant factor, butrequires training and evaluating exponentially (ind) fewer predictors.",Multi-Label Prediction via Compressed Sensing,https://proceedings.neurips.cc/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf
3300,abstract,['Impressive'],Minimizing the rank of a matrix subject to affine constraints is a fundamental problem with many important applications in machine learning and statistics,Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3301,abstract,"['Simplicity', 'Fast']",Inthis paper we propose a simple and fast algorithmSVP(Singular Value Projec-tion) for rank minimization under affine constraints (ARMP) and show that SVPrecovers the minimum rank solution for affine constraints that satisfy arestrictedisometry property(RIP).,Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3302,abstract,['Theoretical guarantees'],Our method guarantees geometric convergence rate evenin the presence of noise and requires strictly weaker assumptions on theRIPcon-stants than the existing methods,Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3304,abstract,"['Theoretical guarantees', 'Practical']","Next,we address a practically important application ofARMP- the problem of low-rank matrix completion, for which the defining affine constraints do not directlyobey RIP, hence the guarantees of SVP do not hold",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3305,abstract,"['Theoretical guarantees', 'Optimal']","However, we provide partial progress towards a proof of exact recovery for our algorithm by showing a more restricted isometry property and observe empirically that our algorithm recoverslow-rank in coherent matrices from an almost optimal number of uniformly sam-pled entries",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3306,abstract,"['Robustness', 'Quantitative evidence (e.g. experiments)', 'Performance']","We also demonstrate empirically that our algorithms outperform ex-isting methods, such as those of [5, 18, 14], forARMPand the matrix completionproblem by an order of magnitude and are also more robust to noise and samplingschemes",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3307,abstract,"['Robustness', 'Performance', 'Impressive']","In particular, results show that our SVP-Newton method is significantlyrobust to noise and performs impressively on a more realistic power-law samplingscheme for the matrix completion problem",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3308,introduction,"['Formal description/analysis', 'Generality']","In this paper we study the general affine rank minimization problem (ARMP),minrank(X)s.tA(X) =b,  X∈Rm×n, b∈Rd,(ARMP)whereAis an affine transformation fromRm×ntoRd",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3309,introduction,"['Practical', 'Impressive']","The affine rank minimization problem above is of considerable practical interest and many importantmachine learning problems such as matrix completion, low-dimensional metric embedding, low-rank kernel learning can be viewed as instances of the above problem",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3310,introduction,['Approximation'],"Unfortunately,ARMPisNP-hard in general and is also NP-hard to approximate ([22]).",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3311,introduction,"['Theoretical guarantees', 'Building on recent work']","Until recently, most known methods forARMPwere heuristic in nature with few known rigorousguarantees",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3312,introduction,"['Theoretical guarantees', 'Building on recent work']","In a recent breakthrough,  Recht et al. [24] gave the first nontrivial results for the problem obtaining guaranteed rank minimization for affine transformationsAthat satisfy arestrictedisometry property(RIP).",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3313,introduction,['Formal description/analysis'],"Define the isometry constant ofA,δkto be the smallest number such thatfor allX∈Rm×nof rank at mostk,(1−δk)∥X∥2F≤∥A(X)∥22≤(1 +δk)∥X∥2F.",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3314,introduction,['Generalization'],The above RIP condition is a direct generalization of the RIP condition used in the compressive sensing context,Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3315,introduction,"['Generalization', 'Practical']","Moreover, RIP holds for many important practical applications of ARMP suchas image compression, linear time-invariant systems",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3318,introduction,"['Novelty', 'Simplicity', 'Efficiency']",In this paper we propose a simple and efficient algorithm SVP (Singular Value Projection) based on the projected gradient algorithm,Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3319,introduction,"['Simplicity', 'Formal description/analysis', 'Theoretical guarantees']",We present a simple analysis showing that SVP recovers theminimum rank solution for noisy affine constraints that satisfy RIP and prove the following guar-antees.,Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3324,introduction,['Theoretical guarantees'],"Then,SVP(Algorithm 1) with step-sizeηt= 1/(1 +δ2k)converges toX∗.Furthermore,SVPoutputs a matrixXof rank at mostksuch that∥A(X)−b∥22≤εand∥X−X∗∥2F≤ε/(1−δ2k)in at most⌈1log((1−δ2k)/2δ2k)log∥b∥22ε⌉iterations",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3326,introduction,['Theoretical guarantees'],"Then, SVP with step-sizeηt= 1/(1 +δ2k)outputs a matrixXof rank at mostksuch that∥A(X)−b∥22≤C∥e∥2+εand∥X−X∗∥2F≤C∥e∥2+ε1−δ2k,ε≥0, in at most⌈1log(1/D)log∥b∥22(C∥e∥2+ε)⌉iterations for universal constantsC,D",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3327,introduction,['Accuracy'],"As ourSVPalgorithm is based on projected gradient descent, it behaves as a first order methodsand may require a relatively large number of iterations to achieve high accuracy, even after iden-tifying the correct row and column subspaces",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3328,introduction,['Simplicity'],"To this end, we introduce a Newton-type step inour framework (SVP-Newton) rather than using a simple gradient-descent step",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3329,introduction,['Theoretical guarantees'],"Guarantees sim-ilar to Theorems 1.1, 1.2 follow easily for SVP-Newton using the proofs forSVP",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3330,introduction,['Accuracy'],"In practice,SVP-Newtonperforms better than SVP in terms of accuracy and number of iterations",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3331,introduction,['Impressive'],"We next consider an important application ofARMP: the low-rank matrix completion problem(MCP)— given a small number of entries from an unknown low-rank matrix, the task is to completethe missing entries",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3333,introduction,"['Theoretical guarantees', 'Exactness', 'Building on recent work', 'Optimal']","Recently, Candes andRecht [6], Candes and Tao [7] and Keshavan et al. [14] gave the first theoretical guarantees for theproblem obtaining exact recovery from an almost optimal number of uniformly sampled entries",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3334,introduction,['Theoretical guarantees'],"While RIP does not hold for MCP, we show that a similar property holds forincoherentmatrices[6].",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3336,introduction,"['Quantitative evidence (e.g. experiments)', 'Optimal']",We provide strong empirical evidence for our hypothesis and show that that both of our algorithms recover a low-rank matrix from an almost optimal number of uniformly sampled entries,Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3337,introduction,"['Novelty', 'Optimal']","In summary, our main contributions are:•Motivated by [11], we propose a projected gradient based algorithm,SVP, forARMPand showthat our method recovers the optimal rank solution when the affine constraints satisfyRIP",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3338,introduction,['Building on recent work'],"To the best of our knowledge, our isometry constant requirements are least stringent: we only requireδ2k<1/3as opposed toδ5k<1/10by Recht et al.,δ3k<1/4√3by Lee and Bresler [18] andδ4k<0.04by Lee and Bresler [17].",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3339,introduction,"['Novelty', 'Preciseness', 'Useful']",We introduce a Newton-type step in the SVP method which is useful if high precision is criti-cally. ,Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3340,introduction,"['Theoretical guarantees', 'Performance', 'Accuracy']","SVP-Newtonhas similar guarantees to that ofSVP, is more stable and has better empiricalperformance in terms of accuracy",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3341,introduction,['Quantitative evidence (e.g. experiments)'],"For instance, on the Movie-lens dataset [1] and rankk= 3,SVP-Newtonachieves an RMSE of0.89, while SVT method [5] achieves an RMSE of0.98",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3342,introduction,['Performance'],"As observed in [23], most trace-norm based methods perform poorly for matrix completion whenentries are sampled from more realistic power-law distributions",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3343,introduction,"['Robustness', 'Performance']","Our methodSVP-Newtonisrelatively robust to sampling techniques and performs significantly better than the methods of[5, 14, 23] even for power-law distributed samples",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3344,introduction,"['Theoretical guarantees', 'Optimal']","•We show that the affine constraints in the low-rank matrix completion problem satisfy a weakerrestricted isometry property and as supported by empirical evidence, conjecture thatSVP(aswell asSVP-Newton) recovers the underlying matrix from an almost optimal number of uni-formly random samples.",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3345,introduction,"['Quantitative evidence (e.g. experiments)', 'Performance', 'Accuracy']","We evaluate our method on a variety of synthetic and real-world datasets and show that ourmethods consistently outperform, both in accuracy and time, various existing methods [5, 14].",Guaranteed Rank Minimization via Singular ValueProjection,https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf
3346,abstract,"['Novelty', 'Large scale', 'Automatic']","We introduce a two-layer undirected graphical model, called a “Replicated Soft-max”, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of documents",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3347,abstract,"['Accuracy', 'Efficiency']","We present efficient learning and inference algorithms for this model, and show how a Monte-Carlo based method, Annealed Importance Sampling, can be used to pro-duce an accurate estimate of the log-probability the model assigns to test data.",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3348,abstract,"['Generalization', 'Accuracy']",This allows us to demonstrate that the proposed model is ableto generalize much better compared to Latent Dirichlet Allocation in terms of both the log-probability of held-out documents and the retrieval accuracy.,Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3349,introduction,"['Large scale', 'Used in practice/Popular']","Probabilistic topic models [2, 9, 6] are often used to analyze and extract semantic topics from large text collections",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3353,introduction,"['Exactness', 'Accuracy', 'Identifying limitations']","One major drawback is that exact inference in these models is intractable, so one has to resort to slow or inaccurate approxima-tions to compute the posterior distribution over topics",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3354,introduction,['Identifying limitations'],"A second major drawback, that is shared by all mixture models, is that these models can never make predictions for words that are sharper than the distributions predicted by any of the individual topics",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3355,introduction,['Identifying limitations'],They are unable to capture the essential idea of distributed representations which is that the distributions predicted by individual active fea-tures get multiplied together (and renormalized) to give the distribution predicted by a whole set of active features.,Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3356,introduction,"['Generality', 'Preciseness']",This allows individual features to be fairly general but their intersection to be much more precise.,Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3360,introduction,"['Accuracy', 'Identifying limitations']","While these models are able to produce distributed representations of the input and perform well in terms of re-trieval accuracy, they are unable to properly deal with documents of different lengths, which makes learning very unstable and hard",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3361,introduction,['Used in practice/Popular'],This is perhaps the main reason why these potentially powerful models have not found their application in practice,Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3366,introduction,['Novelty'],In the next section we introduce a “Replicated Softmax” model,Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3367,introduction,['Efficiency'],"The model can be efficiently trainedusing Contrastive Divergence, it has a better way of dealing with documents of different lengths, and computing the posterior distribution over the latent topic values is easy",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3368,introduction,"['Generalization', 'Quantitative evidence (e.g. experiments)', 'Accuracy', 'Used in practice/Popular']","We will also demonstrate that the proposed model is able to generalize much better compared to a popular Bayesian mixture model, Latent Dirichlet Allocation (LDA) [2], in terms of both the log-probability on previously unseen documents and the retrieval accuracy",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3369,Conclusions and Extensions,"['Simplicity', 'Large scale', 'Automatic']",We have presented a simple two-layer undirected topic model that be used to model and automati-cally extract distributed semantic representations from large collections of text corpora,Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3372,Conclusions and Extensions,"['Large scale', 'Scales up']","Furthermore, using stochastic gradient descent, scaling up learning to billions of documents would not be particularly difficult",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3374,Conclusions and Extensions,['Identifying limitations'],"Therefore one would have to make further approximations, for example by using particle filtering",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3375,Conclusions and Extensions,"['Generalization', 'Quantitative evidence (e.g. experiments)', 'Accuracy']",We have also demonstrated that the proposed model is able to generalize much better than LDA in terms of both the log-probability on held-out documents and the retrieval accuracy.,Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3376,Conclusions and Extensions,['Flexibility/Extensibility'],"In this paper we have only considered the simplest possible topic model, but the proposed model can be extended in several ways.",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3377,Conclusions and Extensions,"['Flexibility/Extensibility', 'Building on recent work']","For example, similar to supervised LDA [1], the proposed Replicated Softmax can be easily extended to modeling the joint the distribution over words and a document label, as shown in Fig. 4, left panel.",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3378,Conclusions and Extensions,['Building on recent work'],"Recently, [11] introduced a Dirichlet-multinomial regression model,  where a prior on the document-specific topic distributions was modeled as a function of observed metadata of the document",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3379,Conclusions and Extensions,"['Flexibility/Extensibility', 'Building on recent work']","Similarly,  we can define a conditional Replicated Softmax model, where the observed document-specific metadata, such as author, references, etc., can be used  to influence the states of the latent topic units, as shown in Fig. 4, right panel",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3381,Conclusions and Extensions,['Accuracy'],"Once the Replicated Softmax has been trained, we can add more layers to create a DeepBelief Network [8], which could potentially produce a better generative model and further improver etrieval accuracy",Replicated Softmax: an Undirected Topic Model,http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf
3382,abstract,['Building on recent work'],Recent  work  has  shown  local  convergence  of GAN training for absolutely continuous data and generator distributions,Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3384,abstract,['Building on recent work'],"Furthermore, we discuss reg-ularization strategies that were recently proposed to stabilize GAN training",Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3385,abstract,['Formal description/analysis'],Our analysis shows that GAN training with instance noise or zero-centered  gradient  penalties  converges,Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3387,abstract,"['Novelty', 'Understanding (for researchers)']","We discuss these results, leading us to a new explanation for the stability problems of GAN training",Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3388,abstract,"['Formal description/analysis', 'Theoretical guarantees']","Based on our analysis, we extend our convergence results to more general GANs and prove local conver-gence for simplified gradient penalties even if the generator and data distributions lie on lower di-mensional manifolds",Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3390,introduction,['Applies to real world'],"Generative  Adversarial  Networks  (GANs)  (Goodfellowet al., 2014) are powerful latent variable models that can be used to learn complex real-world distributions",Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3391,introduction,"['Realistic output', 'Used in practice/Popular']","Especially for images, GANs have emerged as one of the dominant approaches for generating new realistically looking samples after the model has been trained on some dataset.",Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3393,introduction,"['Building on recent work', 'Understanding (for researchers)']","As a result,  a lot of recent research has focused on finding better training algorithms (Arjovsky et al., 2017; Gulrajaniet al., 2017; Kodali et al., 2017; Sønderby et al., 2016; Rothet al., 2017) for GANs as well as gaining better theoretically understanding of their training dynamics (Arjovsky et al.,2017; Arjovsky & Bottou, 2017; Mescheder et al., 2017;Nagarajan & Kolter, 2017; Heusel et al., 2017).",Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3398,introduction,"['Generality', 'Used in practice/Popular']","While Mescheder et al. (2017) observe eigenvalues close to the imaginary axis in practice, this observation does not answer the question if eigenvalues close to the imaginary axis are a general phenomenon and if yes, whether they are indeed the root cause for the training",Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3402,introduction,['Formal description/analysis'],In this paper we show that this assumption is indeed nec-essary: by considering a simple yet prototypical exampleof GAN training we analytically show that (unregularized) GAN training is not always locally convergent,Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3403,introduction,['Building on recent work'],We also discuss how recent techniques for stabilizing GAN train-ing affect local convergence on our example problem,Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3405,introduction,['Theoretical guarantees'],"On the other hand, we show that instance noise (Sønderbyet al., 2016; Arjovsky & Bottou, 2017), zero-centered gradi-ent penalties (Roth et al., 2017) and consensus optimization(Mescheder et al., 2017) lead to local convergence",Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3406,introduction,['Formal description/analysis'],"Based on our analysis, we give a new explanation for the instabilities commonly observed when training GANs based on discriminator gradients orthogonal to the tangent space of the data manifold.",Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3407,introduction,['Theoretical guarantees'],We also introduce simplified gradient penalties for which we prove local convergence,Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3412,introduction,"['Novelty', 'Theoretical guarantees']",We introduce simplified gradient penalties and prove local convergence for the regularized GAN training dynamics,Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3414,introduction,['Formal description/analysis'],"In this paper, we analyzed the stability of GAN training on a simple yet prototypical example",Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3415,introduction,"['Simplicity', 'Formal description/analysis', 'Identifying limitations']","Due to the simplicity of the example, we were able to analyze the convergence prop-erties of the training dynamics analytically and we showed that (unregularized) gradient based GAN optimization is not always locally convergent",Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3416,introduction,"['Theoretical guarantees', 'Identifying limitations']",Our findings also show that WGANs and WGAN-GP do not always lead to local con-vergence whereas instance noise and zero-centered gradient penalties do.,Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3417,introduction,"['Formal description/analysis', 'Theoretical guarantees']","Based on our analysis, we extended our results to more general GANs and we proved local convergence for simplified zero-centered gradient penalties under suitable assumptions.",Which Training Methods for GANs do actually Converge?,http://www.nowozin.net/sebastian/papers/mescheder2018gan-convergence.pdf
3421,abstract,['Improvement'],We show that it improvesup on β-VAE by providing a better trade-off be-tween disentanglement and reconstruction quality,Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3422,abstract,"['Novelty', 'Identifying limitations']","Moreover, we highlight the problems of a com-monly used disentanglement metric and introduce a new metric that does not suffer from them",Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3423,introduction,['Interpretable (to users)'],Learning interpretable representations of data that expose semantic meaning has important consequences for artificial intelligence,Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3424,introduction,['Useful'],"Such representations are useful not only for standard downstream tasks such as supervised learning and reinforcement learning, but also for tasks such as transfer learning and zero-shot learning where humans excel but machines struggle  (Lake et al., 2016).",Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3432,introduction,['Human-like mechanism'],1. Humans are able to learn factors of variation unsupervised,Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3433,introduction,['Label efficiency (reduced need for labeled data)'],2. Labels are costly as obtaining them requires a human in the loop,Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3435,introduction,['Used in practice/Popular'],"β-VAE (Higgins et al., 2016) is a popular method for un-supervised disentangling based on the Variational Autoen-coder (VAE) framework (Kingma & Welling, 2014; Rezendeet al., 2014) for generative modelling.",Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3436,introduction,['Effectiveness'],"It uses a modified ver-sion of the VAE objective with a larger weight (β >1) on the KL divergence between the variational posterior and the prior, and has proven to be an effective and stable method for disentangling",Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3439,introduction,"['Novelty', 'Formal description/analysis']","In this work ,we analyse the source of this trade-off and propose Factor-VAE, which augments the VAE objective with a penalty that encourages the marginal distribution of representations to be factorial without substantially affecting the quality of reconstructions",Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3442,introduction,"['Novelty', 'Identifying limitations']","We also point out the weaknesses in the disentangling metric of Higgins et al. (2016), and propose a new metric that addresses these shortcomings.",Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3443,introduction,['Used in practice/Popular'],"A popular alternative to β-VAE is InfoGAN (Chen et al.,2016), which is based on the Generative Adversarial Net (GAN) framework (Goodfellow et al., 2014) for generative modelling.",Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3445,introduction,['Quantitative evidence (e.g. experiments)'],"However at least in part due to its training stability issues (Higgins et al., 2016), there has been little empirical comparison between VAE-based methods and InfoGAN",Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3446,introduction,['Quantitative evidence (e.g. experiments)'],"Taking advantage of the recent developmentsin the GAN literature that help stabilise training, we include InfoWGAN-GP, a version of InfoGAN that uses Wasser-stein distance (Arjovsky et al., 2017) and gradient penalty(Gulrajani et al., 2017), in our experimental evaluation",Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3448,introduction,['Novelty'],"1) We introduce FactorVAE, a method for disentangling that gives higher disentanglement scores than β-VAE for the same reconstruction quality.",Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3449,introduction,"['Novelty', 'Identifying limitations']",2) We identify the weaknesses ofthe disentanglement metric of Higgins et al. (2016) and propose a more robust alternative,Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3450,introduction,['Qualitative evidence (e.g. examples)'],3) We give quantitative comparisons of FactorVAE and β-VAE against InfoGAN’s WGAN-GP counterpart for disentanglement,Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3451,conclusion,['Novelty'],"We have introduced FactorVAE, a novel method for disen-tangling that achieves better disentanglement scores thanβ-VAE on the 2D Shapes and 3D Shapes data sets for thesame reconstruction quality",Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3452,conclusion,"['Novelty', 'Simplicity', 'Identifying limitations']","Moreover, we have identified weaknesses of the commonly used disentanglement metric of Higgins et al. (2016), and proposed an alternative metric that is conceptually simpler, is free of hyperparameters, and avoids the failure mode of the former",Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3453,conclusion,"['Quantitative evidence (e.g. experiments)', 'Identifying limitations']","Finally, we have performed an experimental evaluation of disentangling for the VAE-based methods and InfoWGAN-GP, a more stable variant of InfoGAN, and identified its weaknesses relative to the VAE-based methods ",Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3454,conclusion,['Identifying limitations'],One of the limitations of our approach is that low Total Correlation is necessary but not sufficient for disentangling of  independent  factors  of  variation,Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3456,conclusion,['Identifying limitations'],"Our disentanglement metric also requires us to be able to generate samples holding one factor fixed, which may not always be possible, for example when our training  set  does  not  cover  all  possible  combinations  of factors.",Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3457,conclusion,['Identifying limitations'],The metric is also unsuitable for data with non-independent factors of variation,Disentangling by Factorising,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf
3460,abstract,['Novelty'],We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effectson both the actor and the critic,Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3461,abstract,['Building on recent work'],"Our algorithm builds on Double Q-learning, by taking the mini-mum value between a pair of critics to limit over-estimation.",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3462,abstract,['Performance'],"We draw the connection between tar-get networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3463,abstract,"['Quantitative evidence (e.g. experiments)', 'Performance', 'State-of-the-art']","We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every envi-ronment tested.",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3467,introduction,"['Performance', 'State-of-the-art']","Our proposed method addresses these issues, and greatly outperforms the current state of the art",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3469,introduction,['Preciseness'],"In a function approximation setting, this noise is unavoidable given the imprecision of the estimator",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3470,introduction,['Accuracy'],"This inaccuracy is further exaggerated by the nature of temporal difference learning (Sutton, 1988), in which an estimate of the value functionis updated using the estimate of a subsequent state",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3471,introduction,['Preciseness'],This means using an imprecise estimate within each update willlead to an accumulation of error,Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3474,introduction,['Identifying limitations'],"Furthermore, we find the ubiquitous solution in the discrete action setting, Double DQN (Van Hasselt et al., 2016), to be ineffective in an actor-critic setting",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3477,introduction,['Building on classic work'],"This can be dealt with by adapting an older variant, Double Q-learning (Van Hasselt, 2010), to an actor-critic formatby using a pair of independently trained critics",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3479,introduction,['Approximation'],"To address this concern, we propose a clipped Double Q-learning variant which lever-ages the notion that a value estimate suffering from overes-timation bias can be used as an approximate upper-bound tothe true value estimate",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3482,introduction,['Accuracy'],"First, we show that target networks, a com-mon approach in deep Q-learning methods, are critical for variance reduction by reducing the accumulation of errors",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3484,introduction,['Novelty'],"Finally, we introduce a novel regularization strategy, where a SARSA-style update bootstraps similaraction estimates to further reduce variance.",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3485,introduction,"['State-of-the-art', 'Unifying ideas or integrating components']","Our modifications are applied to the state of the art actor-critic method for continuous control, Deep DeterministicPolicy Gradient algorithm (DDPG) (Lillicrap et al., 2015), to form the Twin Delayed Deep Deterministic policy gradient algorithm (TD3), an actor-critic algorithm which consid-ers the interplay between function approximation error inboth policy and value updates",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3486,introduction,"['Quantitative evidence (e.g. experiments)', 'Performance', 'State-of-the-art']","We evaluate our algorithmon seven continuous control domains from OpenAI gym(Brockman et al., 2016), where we outperform the state ofthe art by a wide margin",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3487,introduction,"['Scientific methodology', 'Understanding (for researchers)']","Given the recent concerns in reproducibility (Henderson et al., 2017), we run our experiments across a large num-ber  of  seeds  with  fair  evaluation  metrics,  perform  abla-tion studies across each contribution, and open source bothour code and learning curves",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3490,conclusion,"['Novelty', 'Identifying limitations']","We find the common solutions for reducing overestimation biasin deep Q-learning with discrete actions are ineffective in anactor-critic setting, and develop a novel variant of DoubleQ-learning which limits possible overestimation",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3491,conclusion,['Performance'],Our re-sults demonstrate that mitigating overestimation can greatly improve the performance of modern algorithms,Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3495,conclusion,"['Performance', 'Reduced training time']","Taken together, these improvements define our proposed approach, the Twin Delayed Deep Deterministic policy gra-dient algorithm (TD3),  which greatly improves both the learning speed and performance of DDPG in a number of challenging tasks in the continuous control setting",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3496,conclusion,"['Flexibility/Extensibility', 'Performance', 'State-of-the-art', 'Easy to implement']","Our algorithm exceeds the performance of numerous state of the art algorithms. As our modifications are simple to im-plement, they can be easily added to any other actor-criticalgorithm.",Addressing Function Approximation Error in Actor-Critic Methods,https://arxiv.org/pdf/1802.09477.pdf
3499,abstract,"['Novelty', 'Efficiency', 'Data efficiency', 'Parallelizability / distributed', 'Scales up']",We have developed a new distributed agent IMPALA (Impor-tance Weighted Actor-Learner Architecture) thatnot only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3500,abstract,['Novelty'],We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3501,abstract,['Effectiveness'],"We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al.,2016)) and Atari-57",IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3502,abstract,['Performance'],"Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task ap-proach",IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3503,abstract,['Facilitating use (e.g. sharing code)'],The source code is publicly available atgithub.com/deepmind/scalableagent.,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3505,introduction,"['Performance', 'Progress']","While the improvements on tasks like the game of Go (Sil-ver et al., 2017) and Atari games (Horgan et al., 2018) have been dramatic, the progress has been primarily in single task performance, where an agent is trained on each task separately",IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3506,introduction,['Novelty'],We are interested in developing new methods capable of mastering a diverse set of tasks simultaneously as well as environments suitable for evaluating such methods,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3507,introduction,['Scales up'],One of the main challenges in training a single agent on many tasks at once is scalability,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3508,introduction,['Practical'],"Since the current state-of-the-art methods like A3C (Mnih et al., 2016) or UNREAL (Jaderberg et al., 2017b) can require as much as a billion frames and multiple days to master a single domain, training them on tens of domains at once is too slow to be practical.",IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3510,introduction,"['Data efficiency', 'Parallelizability / distributed', 'Scales up']",IMPALA is capable of scaling to thousands of machines without sacri-ficing training stability or data efficiency.,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3511,introduction,['Used in practice/Popular'],"Unlike the popular A3C-based agents, in which workers communicate gradi-ents with respect to the parameters of the policy to a central parameter server, IMPALA actors communicate trajectories of experience (sequences of states, actions, and rewards) to a centralised learner",IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3512,introduction,['Parallelizability / distributed'],Since the learner in IMPALA has access to full trajectories of experience we use a GPU to perform updates on mini-batches of trajectories while aggressively parallelising all time independent operations.,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3513,introduction,['Scales up'],This type of decoupled architecture can achieve very high throughput,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3516,introduction,"['Reduced training time', 'Unifying ideas or integrating components']","With the scalable architecture and V-trace combined, IM-PALA achieves exceptionally high data throughput rates of 250,000 frames per second, making it over 30 times faster than single-machine A3C.",IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3517,introduction,"['Robustness', 'Data efficiency']","Crucially, IMPALA is also more data efficient than A3C based agents and more robust to hyperparameter values and network architectures,  allow-ing it to make better use of deeper neural networks",IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3518,introduction,['Effectiveness'],"We demonstrate the effectiveness of IMPALA by training a single agent on multi-task problems using DMLab-30, a newchallenge set which consists of 30 diverse cognitive tasks in the 3D DeepMind Lab (Beattie et al., 2016) environment and by training a single agent on all games in the Atari-57 set of tasks.",IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3519,conclusion,"['Novelty', 'Scales up']","We have introduced a new highly scalable distributed agent, IMPALA, and a new off-policy learning algorithm, V-trace.",IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3520,conclusion,"['Simplicity', 'Parallelizability / distributed', 'Scales up']","With its simple but scalable distributed architecture, IM-PALA can make efficient use of available compute at small and  large  scale.",IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3522,conclusion,['Robustness'],V-trace is a general off-policy learning algorithm that is more stable and robust compared to other off-policy correc-tion methods for actor critic agents,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3523,conclusion,"['Performance', 'Data efficiency']","We have demonstrated that  IMPALA  achieves  better  performance  compared  to A3C variants in terms of data efficiency, stability and final performance",IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3524,conclusion,['Novelty'],We have further evaluated IMPALA on thenew DMLab-30 set and the Atari-57 set.,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3525,conclusion,"['Quantitative evidence (e.g. experiments)', 'Large scale', 'Performance']","To the best ofour knowledge, IMPALA is the first Deep-RL agent that has been successfully tested in such large-scale multi-task settings and it has shown superior performance comparedto A3C based agents (49.4% vs. 23.8% human normalisedscore on DMLab-30)",IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3526,conclusion,['Performance'],"Most importantly, our experiments on DMLab-30 show that, in the multi-task setting, positive transfer between individual tasks lead IMPALA to achieve better performance compared to the expert training setting.",IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3527,conclusion,"['Simplicity', 'Robustness', 'Scales up']",We believe that IMPALA provides a simple yet scalable and robust framework for building better Deep-RL agents and has the potential to enable research on new challenges,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://export.arxiv.org/pdf/1802.01561
3531,abstract,"['Simplicity', 'Approximation', 'Performance', 'Efficiency']","Approximation of this prior structure through simple, efficient hy-perparameter optimization steps is sufficient to achieve these performance gains",Rethinking LDA: Why Priors Matter,http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf
3532,abstract,['Robustness'],The  prior  structure  we  advocate  substantially  increases  the  robustness  of  topic models to variations in the number of topics and to the highly skewed word fre-quency distributions common in natural language,Rethinking LDA: Why Priors Matter,http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf
3533,abstract,"['Low cost', 'Efficiency']","Since this prior structure can be implemented using efficient algorithms that add negligible cost beyond standard inference techniques, we recommend it as a new standard for topic modeling.",Rethinking LDA: Why Priors Matter,http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf
3534,introduction,['Useful'],"Topic models such as latent Dirichlet allocation (LDA) [3] have been recognized as useful tools for analyzing large, unstructured collections of documents",Rethinking LDA: Why Priors Matter,http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf
3540,introduction,['Scientific methodology'],"Asuncion et al. [1] recently advocated inferring the concentration parameters of these symmetric Dirichlets from data, but to date there has been no rigorous scientific study of the priors used in LDA—from the choice of prior (symmetric versus asymmetric Dirichlets) to the treatmentof hyperparameters (optimize versus integrate out)—and the effects of these modeling choices on the probability of held-out documents and, more importantly, the quality of inferred topics.",Rethinking LDA: Why Priors Matter,http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf
3543,introduction,['Performance'],"Using MCMC simulations, we find that using an asymmetric, hierarchical Dirichlet prior over the document–topic distributions and a symmetric Dirichlet prior over the topic–word distributions results in significantly better model performance, measured both in terms of the probability of held-out documents and in the quality of inferred topics.",Rethinking LDA: Why Priors Matter,http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf
3545,introduction,['Performance'],"We therefore demonstrate that optimizing the hyperparameters of asymmetric, nonhierarchical Dirichlets as part of an iterative inference algorithm results in similar performance to the full Bayesian model while adding negligible computational cost beyond standard inference techniques.",Rethinking LDA: Why Priors Matter,http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf
3547,introduction,"['Robustness', 'Low cost']","By decreasing the sensitivity of the model to the number of topics,  hyperparameter optimization results in robust,  data-driven models with substantially less model complexity and computational cost than nonparametric models",Rethinking LDA: Why Priors Matter,http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf
3548,introduction,['Efficiency'],"Since the priors we advocate (an asymmetric Dirichlet over the document–topic distributions and asymmetric Dirichlet over the topic–word distributions) have significant modeling benefits and can be implemented using highly efficient algorithms, we recommend them as a new standard for LDA.",Rethinking LDA: Why Priors Matter,http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf
3549,discussion,"['Quantitative evidence (e.g. experiments)', 'Performance']","The previous sections demonstrated that AS results in the best performance over AA, SA and SS, measured in several ways",Rethinking LDA: Why Priors Matter,http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf
3550,discussion,['Performance'],"However, it is worth examining why this combination of priors results in superior performance",Rethinking LDA: Why Priors Matter,http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf
3559,discussion,"['Low cost', 'Reduced training time']","Since these priors can be implemented using efficient algorithms that add negligible cost beyond standard inference techniques, we recommend them as a new standard for LDA.",Rethinking LDA: Why Priors Matter,http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf
3560,abstract,['Building on classic work'],"How well does a classic deep net architecture like AlexNet or VGG19 classify on astandard dataset such as CIFAR-10 when its “width”— namely, number of channelsin convolutional layers, and number of nodes in fully-connected internal layers —is allowed to increase to infinity?",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3561,abstract,"['Generalization', 'Theoretical guarantees', 'Understanding (for researchers)']",Such questions have come to the forefront in thequest to theoretically understand deep learning and its mysteries about optimizationand generalization,On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3565,abstract,"['Exactness', 'Efficiency']","The current paper gives the first efficient exact algorithm for computing the ex-tension of NTK to convolutional neural nets, which we callConvolutional NTK(CNTK), as well as an efficient GPU implementation of this algorithm",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3566,abstract,['Performance'],"This resultsin a significant new benchmark for performance of a pure kernel-based method onCIFAR-10, being10%higher than the methods reported in [Novak et al.,2019],and only6%lower than the performance of the corresponding finite deep netarchitecture (once batch normalization etc. are turned off).",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3567,abstract,['Theoretical guarantees'],"Theoretically, we alsogive the firstnon-asymptoticproof showing that a fully-trained sufficiently widenet is indeed equivalent to the kernel regression predictor using NTK.",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3568,introduction,"['Performance', 'Building on classic work']","How well does a classic deep net architecture like AlexNet or VGG19 perform on a standard datasetsuch as CIFAR-10 when its “width”— namely, number of channels in convolutional layers, andnumber of nodes in fully-connected internal layers — is allowed to increase to infinity",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3569,introduction,['Understanding (for researchers)'],Questionsabout these “infinite limits” of deep nets have naturally emerged in the ongoing effort to understandthe power of deep learning,On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3571,introduction,['Generalization'],"Fur-thermore, the infinite limit could conceivably make sense in deep learning, sinceover-parametrizationseems to help optimization a lot and doesn’t hurt generalization much [Zhang et al.,2017]: deepneural nets with millions of parameters work well even for datasets with50k training examples",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3579,introduction,['Performance'],It has long been known that weakly-trainedconvolutional nets have reasonable performance on MNIST and CIFAR-10,On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3585,introduction,['Generalization'],"In the finitecase, analysis of optimization and generalization of fully-trained nets is of course an open problem",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3586,introduction,['Understanding (for researchers)'],One may also ask:Can we understand the power of fully-trained nets whose width goes to infinity,On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3589,introduction,['Generalization'],"Recent papers suggest that neural nets whose width greatly exceeds the number of training data pointscan rapidly reduce training error to0via gradient descent, and under some conditions, the trainednet also exhibits good generalization",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3596,introduction,['Generalization'],"One may also generalize the NTK to convolutional neural nets, and we call the corresponding kernelConvolutional Neural Tangent Kernel (CNTK).",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3597,introduction,['Understanding (for researchers)'],"Though NTK and CNTK are defined by an infinite limit, a recent paper [Lee et al.,2019] attemptedto understand their properties via a finite approximation of the infinite limit kernel by Monte Carlomethods",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3598,introduction,['Performance'],"However, as will be shown in SectionB, using random features generated from practicallysized nets can degrade the performance a lot",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3601,introduction,"['Exactness', 'Efficiency']","We give an exact and efficient dynamic programming algorithm to computeCNTKs for ReLU activation (namely, to computeker (x,x0)givenxandx0).",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3602,introduction,['Performance'],Using this algorithm— as well as implementation tricks for GPUs — we can settle the question of the performance offully-trained infinitely wide nets with a variety of architectures,On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3603,introduction,['Performance'],"For instance, we find that theirperformance on CIFAR-10 is within5%of the performance of the same architectures in the finite case(note that the proper comparison in the finite case involves turning off batch norm, data augmentation,etc., in the optimization",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3604,introduction,['Accuracy'],"In particular, the CNTK corresponding to a11-layer convolutional netwith global average pooling achieves77%classification accuracy",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3605,introduction,['Performance'],This is10%higher than the bestreported performance of a Gaussian process with fixed kernel on CIFAR-10,On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3606,introduction,['Theoretical guarantees'],"Furthermore, we give a more rigorous, non-asymptotic proof that the NTK captures the behavior of afully-trained wide neural net under weaker condition than previous proofs",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3607,introduction,"['Approximation', 'Quantitative evidence (e.g. experiments)', 'Performance']","We also experimentallyshow that the random feature methods for approximating CNTK in earlier work do not compute goodapproximations, which is clear from their much worse performance on CIFAR",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3608,conclusion,"['Exactness', 'Practical']","By giving the first practical algorithm for computing CNTKs exactly, this paper allows investigationof the behavior of infinitely wide (hence infinitely over-parametrized) deep nets, which turns out to not be much worse than that of their finite counterparts",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3609,conclusion,['Approximation'],"We also give a fully rigorous proof that asufficiently wide net is approximately equivalent to the kernel regression predictor, thus yielding apowerful new off-the-shelf kernel",On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3610,conclusion,['Practical'],We leave it as an open problem to understand the behavior ofinfinitely wide nets with features such as Batch Normalization or Residual Layers.,On Exact Computation with an Infinitely WideNeural Net,http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf
3613,abstract,['Efficiency'],"PyTorch is a machine learning library that shows that these two goalsare in fact compatible: it provides an imperative and Pythonic programming stylethat supports code as a model, makes debugging easy and is consistent with otherpopular scientific computing libraries, while remaining efficient and supportinghardware accelerators such as GPUs","PyTorch: An Imperative Style, High-PerformanceDeep Learning Library",http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
3615,abstract,['Controllability (of model owner)'],PyTorch is a regular Python program under the full control of its user.  ,"PyTorch: An Imperative Style, High-PerformanceDeep Learning Library",http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
3616,abstract,['Performance'],We alsoexplain how the careful and pragmatic implementation of the key components ofits runtime enables them to work together to achieve compelling performance,"PyTorch: An Imperative Style, High-PerformanceDeep Learning Library",http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
3617,abstract,['Performance'],"We demonstrate the efficiency of individual subsystems, as well as the overallspeed of PyTorch on several common benchmarks.","PyTorch: An Imperative Style, High-PerformanceDeep Learning Library",http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
3620,introduction,"['Performance', 'Scales up']","This approach provides visibility into the whole computationahead of time, and can theoretically be leveraged to improve performance and scalability","PyTorch: An Imperative Style, High-PerformanceDeep Learning Library",http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
3621,introduction,['Flexibility/Extensibility'],"However, itcomes at the cost of ease of use, ease of debugging, and flexibility of the types of computation thatcan be represented.","PyTorch: An Imperative Style, High-PerformanceDeep Learning Library",http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
3622,introduction,['Performance'],"Prior work has recognized the value of dynamic eager execution for deep learning, and some recentframeworks implement this define-by-run approach, but do so either at the cost of performance(Chainer [5]) or using a less expressive, faster language (Torch [6], DyNet [7]), which limits theirapplicability","PyTorch: An Imperative Style, High-PerformanceDeep Learning Library",http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
3623,introduction,['Performance'],"However, with careful implementation and design choices, dynamic eager execution can be achievedlargely without sacrificing performance","PyTorch: An Imperative Style, High-PerformanceDeep Learning Library",http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
3624,introduction,['Performance'],"This paper introduces PyTorch,  a Python library thatperforms immediate execution of dynamic tensor computations with automatic differentiation andGPU acceleration, and does so while maintaining performance comparable to the fastest currentlibraries for deep learning","PyTorch: An Imperative Style, High-PerformanceDeep Learning Library",http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
3625,introduction,['Used in practice/Popular'],"This combination has turned out to be very popular in the researchcommunity with, for instance, 296 ICLR 2019 submissions mentioning PyTorch.","PyTorch: An Imperative Style, High-PerformanceDeep Learning Library",http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
3626,Conclusion and future work,"['Performance', 'Used in practice/Popular']",PyTorch has become a popular tool in the deep learning research community by combining a focuson usability with careful performance considerations,"PyTorch: An Imperative Style, High-PerformanceDeep Learning Library",http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
3627,Conclusion and future work,['Scales up'],"In addition to continuing to support the latesttrends and advances in deep learning, in the future we plan to continue to improve the speed andscalability of PyTorch.","PyTorch: An Imperative Style, High-PerformanceDeep Learning Library",http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
3629,Conclusion and future work,['Parallelizability / distributed'],We also intend to improve support for distributed computation by providing efficientprimitives for data parallelism as well as a Pythonic library for model parallelism based aroundremote procedure calls,"PyTorch: An Imperative Style, High-PerformanceDeep Learning Library",http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
3632,abstract,"['Robustness', 'Understanding (for researchers)']","Our aim is to understand the reasons underlying this robustness trade-off, and to train models that are simultaneously robust to multiple perturbation types",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3633,abstract,"['Simplicity', 'Robustness']",We prove that a trade-off in robustness to different types of p-bounded and spatial perturbations must exist in a natural and simple statistical setting.,Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3634,abstract,"['Robustness', 'Formal description/analysis']",We corroborate our formal analysis by demonstrating similar robustness trade-offs on MNIST and CIFAR10.,Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3635,abstract,"['Robustness', 'Efficiency']","We propose new multi-perturbation adversarial training schemes, as well as an efficient attack for the 1-norm, and use these to show that modelstrained against multiple attacks fail to achieve robustness competitive with that of models trained on each attack individually",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3636,abstract,"['Robustness', 'Accuracy']","In particular, we find that adversarial training with first-order l∞, l1and l2 attacks on MNIST achieves merely 50% robust accuracy, partly because of gradient-masking.",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3637,abstract,['Accuracy'],"Finally, we propose affine attacks that linearly interpolate between perturbation types and further degrade the accuracy of adversarially trained models.",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3641,introduction,"['Robustness', 'Theoretical guarantees']","These  defenses  provide  empirical  (or  certifiable)  robustness guarantees for one perturbation type, but typically offer no guarantees against other attacks [35,31].",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3642,introduction,['Robustness'],"Worse, increasing robustness to one perturbation type has sometimes been found to increase vulnerability to others [11, 31].",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3643,introduction,['Robustness'],This leads us to the central problem considered in this paper: Can we achieve adversarial robustness to different types of perturbations simultaneously?,Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3644,introduction,['Robustness'],"Note that even though prior work has attained robustness to different perturbation types [25,31,11], these results may not compose",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3645,introduction,['Robustness'],"For instance, an ensemble of two classifiers—each of which is robust to a single type of perturbation—may be robust to neither perturbation.",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3646,introduction,['Robustness'],Our aim is to study the extent to which it is possible to learn models that are simultaneously robust to multiple types of perturbation.,Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3647,introduction,"['Simplicity', 'Generalization', 'Accuracy', 'Natural']","To gain intuition about this problem, we first study a simple and natural classification task, that has been used to analyze trade-offs between standard and adversarial accuracy [41], and the sample-complexity of adversarial generalization [30].",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3648,introduction,['Robustness'],We define Mutually Exclusive Perturbations (MEPs) as pairs of perturbation types for which robustness to one type implies vulnerability to the other,Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3650,introduction,['Robustness'],"Moreover, for these MEP pairs, we find that robustness to either perturbation type requires fundamentally different features",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3652,introduction,['Formal description/analysis'],"To complement our formal analysis, we introduce new adversarial training schemes for multiple perturbations",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3655,introduction,['Efficiency'],"For adversarial training to be practical, we also need efficient and strong attacks [25].",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3656,introduction,['Efficiency'],"We show that Projected Gradient Descent [22,25] is inefficient in the l1-case, and design a new attack, Sparse l1Descent (SLIDE), that is both efficient and competitive with strong optimization attacks [8],",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3657,introduction,['Quantitative evidence (e.g. experiments)'],We experiment with MNIST and CIFAR10.,Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3658,introduction,['Robustness'],"MNIST is an interesting case-study, as distinct models from prior work attain strong robustness to all perturbations we consider [25,31,11], yet no single classifier is robust to all attacks [31,32,11].",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3659,introduction,['Robustness'],"For models trained on multiple lp-attacks (l1, l2, l∞ for MNIST, and l1, l∞ for CIFAR10), or on both l∞ and spatial transforms [11], we confirm a noticeable robustness trade-off.",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3660,introduction,"['Robustness', 'Accuracy']","Figure 1 plots the test accuracy of models Advmax trained using our “max” strategy. In all cases, robustness to multiple perturbations comes at a cost—usually of 5-10% additional error—compared to models trained against each attack individually (the horizontal lines).",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3661,introduction,['Robustness'],"Robustness to l1, l2 and l∞-noise on MNIST is a striking failure case, where the robustness trade-off is compounded by gradient-masking [27,40,1].",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3662,introduction,['Building on recent work'],"Extending prior observations [25,31,23], we show that models trained against an l∞-adversary learn representations that mask gradients for attacks in other lp-norms.",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3663,introduction,['Robustness'],"When trained against first-order `1, `2 and `∞-attacks, the model learns to resist `∞-attacks while giving the illusion of robustness to `1 and `2 attacks.",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3664,introduction,['Accuracy'],"This model only achieves 52% accuracy when evaluated on gradient-free attacks [3,31].",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3666,introduction,['Robustness'],"We thus argue that attaining robustness to `p-noise on MNIST requires new techniques (e.g., training on expensive gradient-free attacks, or scaling certified defenses to multiple perturbations).",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3668,introduction,['Simplicity'],Our results paint a more nuanced view: the simplicity of these `∞-defenses becomes a disadvantage when training against multiple `p-norms,Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3670,introduction,"['Simplicity', 'Robustness', 'Scales up']",Our inability to achieve multi-`p robustness for this simple dataset raises questions about the viability of scaling current defenses to more complex tasks.,Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3672,introduction,"['Simplicity', 'Robustness']","We prove that for locally-linear models, robustness to a union of`p-perturbation simplies robustness to affine attacks.",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3674,introduction,['Applies to real world'],We show that this discrepancy translates to neural networks trained on real data.,Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3675,introduction,['Robustness'],"Thus, in some cases, attaining robustness to a union of perturbation types remains insufficient against a more creative adversary that composes perturbations",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3676,introduction,['Robustness'],"Our results show that despite recent successes in achieving robustness to single perturbation types, many obstacles remain towards attaining truly robust models",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3677,introduction,"['Robustness', 'Efficiency', 'Scales up']","Beyond the robustness trade-off, efficient computational scaling of current defenses to multiple perturbations remains an open problem.",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3680,Discussion and Open Problems,['Robustness'],"Despite recent success in defending ML models against some perturbation types [25,11,31], extending these defenses to multiple perturbations unveils a clear robustness trade-off",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3682,Discussion and Open Problems,['Robustness'],"Our new adversarial training strategies fail to achieve competitive robustness to more than one attack type, but narrow the gap towards multi-perturbation robustness",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3684,Discussion and Open Problems,['Robustness'],"Thus, for most data points, the models are either robust to all perturbation types or none of them.",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3685,Discussion and Open Problems,['Robustness'],"This hints that some points (sometimes referred to as prototypical examples [4, 36]) are inherently easier to classify robustly, regardless of the perturbation type.",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3687,Discussion and Open Problems,['Robustness'],Achieving better robustness on this simple dataset is an open problem.,Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3688,Discussion and Open Problems,"['Low cost', 'Scales up']","Another challengeis reducing the cost of our adversarial training strategies, which scale linearly in the number of perturbation types",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3689,Discussion and Open Problems,['Efficiency'],"Breaking this linear dependency requires efficient techniques for finding perturbations in a union of sets, which might be hard for sets with near-empty intersection (e.g.,`∞and`1-balls).",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3691,Discussion and Open Problems,['Robustness'],"Hendrycks and Dietterich [17], and Geirhos et al. [13] recently measured robustness of classifiers to multiple common (i.e., non-adversarial) image corruptions (e.g., random image blurring).",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3692,Discussion and Open Problems,"['Robustness', 'Accuracy']","In that setting, they also find that different classifiers achieve better robustness to some corruptions, and that no single classifier achieves the highest accuracy under all forms",Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3693,Discussion and Open Problems,['Robustness'],The interplay between multi-perturbation robustness in the adversarial and common corruption case is worth further exploration,Adversarial Training and Robustness forMultiple Perturbations,http://papers.nips.cc/paper/8821-adversarial-training-and-robustness-for-multiple-perturbations.pdf
3694,abstract,"['State-of-the-art', 'Effectiveness']","Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction",Hierarchical Graph Representation Learning with Differentiable Pooling,http://papers.nips.cc/paper/7729-hierarchical-graph-representation-learning-with-differentiable-pooling.pdf
3695,abstract,['Identifying limitations'],"However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs—a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph.",Hierarchical Graph Representation Learning with Differentiable Pooling,http://papers.nips.cc/paper/7729-hierarchical-graph-representation-learning-with-differentiable-pooling.pdf
3698,abstract,"['Quantitative evidence (e.g. experiments)', 'Accuracy', 'State-of-the-art']","Our experimental results show that combining existing GNN methods with DIFFPOOL yields an average improvement of 5–10% accuracyon graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.",Hierarchical Graph Representation Learning with Differentiable Pooling,http://papers.nips.cc/paper/7729-hierarchical-graph-representation-learning-with-differentiable-pooling.pdf
3699,introduction,"['Generality', 'Building on recent work']","In recent years there has been a surge of interest in developing graph neural networks (GNNs) — general deep learning architectures that can operate over graph structured data, such as social network data [16,21,36] or graph-based representations of molecules [7,11,15].",Hierarchical Graph Representation Learning with Differentiable Pooling,http://papers.nips.cc/paper/7729-hierarchical-graph-representation-learning-with-differentiable-pooling.pdf
3702,introduction,['Identifying limitations'],"However, a major limitation of current GNN architectures is that they are inherently flata s theyonly propagate information across the edges of the graph and are unable to infer and aggregate theinformation in a hierarchical way.",Hierarchical Graph Representation Learning with Differentiable Pooling,http://papers.nips.cc/paper/7729-hierarchical-graph-representation-learning-with-differentiable-pooling.pdf
3706,introduction,['Effectiveness'],"This global pooling approach ignores any hierarchical structure that might be present in the graph, and it prevents researchers from building effective GNN models for predictive tasks over entire graphs.",Hierarchical Graph Representation Learning with Differentiable Pooling,http://papers.nips.cc/paper/7729-hierarchical-graph-representation-learning-with-differentiable-pooling.pdf
3711,introduction,['Generality'],"Moreover, unlike image data, graph data sets often contain graphs with varying numbers of nodes and edges, which makes defining a general graph pooling operator even more challenging.",Hierarchical Graph Representation Learning with Differentiable Pooling,http://papers.nips.cc/paper/7729-hierarchical-graph-representation-learning-with-differentiable-pooling.pdf
3716,introduction,"['Accuracy', 'State-of-the-art']","We show tha tDIFFPOOL can be combined with various GNN approaches, resulting in an average 7% gain in accuracy and a new state of the art on four out of five benchmark graph classification tasks.",Hierarchical Graph Representation Learning with Differentiable Pooling,http://papers.nips.cc/paper/7729-hierarchical-graph-representation-learning-with-differentiable-pooling.pdf
3717,introduction,['Interpretable (to users)'],"Finally, we show that DIFFPOOL can learn interpretable hierarchical clusters that correspond to well-defined communities in the input graphs",Hierarchical Graph Representation Learning with Differentiable Pooling,http://papers.nips.cc/paper/7729-hierarchical-graph-representation-learning-with-differentiable-pooling.pdf
3718,conclusion,['Applies to real world'],We introduced a differentiable pooling method for GNNs that is able to extract the complex hierarchical structure of real-world graphs.,Hierarchical Graph Representation Learning with Differentiable Pooling,http://papers.nips.cc/paper/7729-hierarchical-graph-representation-learning-with-differentiable-pooling.pdf
3719,conclusion,['State-of-the-art'],"By using the proposed pooling layer in conjunction with existing GNN models, we achieved new state-of-the-art results on several graph classification benchmarks",Hierarchical Graph Representation Learning with Differentiable Pooling,http://papers.nips.cc/paper/7729-hierarchical-graph-representation-learning-with-differentiable-pooling.pdf
3724,abstract,['State-of-the-art'],We observesignificant improvements across tasks compared to existing task-specific models –achieving state-of-the-art on all four tasks,ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3731,introduction,"['Large scale', 'Used in practice/Popular', 'Easy to work with']","Thispretrain-then-transferlearning approach to vision-and-language tasks follows naturally from itswidespread use in both computer vision and natural language processing where it has become the defacto standard due to the ease-of-use and strong representational power of large, publicly-availablemodels [11–14] trained on large-scale data sources [15–19",ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3732,introduction,['Useful'],"In these domains, pretrained models canprovide useful information for target tasks,e.g. dog breed-sensitive image features or a well-calibratedsemantic distance between words",ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3733,introduction,['Understanding (for researchers)'],"While visual and linguistic understandings like these are of courseessential to vision-and-language tasks, equally important is how they relate to one another –e.g. aperfect visual representation of dog breeds is of little use if a downstream vision-and-language modelfails to associate it with appropriate phrases like “beagle” or “shepherd”.",ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3734,introduction,['Generalization'],"e are therefore interestedin developing a common model for visual grounding that can learn these connections and leveragethem on a wide array of vision-and-language tasks –i.e., we seek to pretrain for visual grounding",ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3736,introduction,['Automatic'],These proxy tasks leverage structure within the data to generate supervised tasks automatically (e.g. colorizing images [20] or reconstruct-ing masked words in text [12]).,ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3737,introduction,['Promising'],"While work within the vision community has shown increasingpromise [21–23], the greatest impact of self-supervised learning so far is through language modelslike ELMo [13], BERT [12], and GPT [14] which have set new high-water marks on many NLPtasks",ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3739,introduction,['Automatic'],". In this work, we consider the recentlyreleased Conceptual Captions [24] dataset consisting of∼3.3 million images with weakly-associateddescriptive captions automatically collected from alt-text enabled images on the web",ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3741,introduction,['Building on recent work'],Our approach extends the recentlydeveloped BERT [12] language model to jointly reason about text and images,ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3742,introduction,['Novelty'],Our key technicalinnovation is introducing separate streams for vision and language processing that communicatethrough co-attentional transformer layers,ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3744,introduction,"['Quantitative evidence (e.g. experiments)', 'Performance']",We demonstrate that this structure outperforms a single-stream unified model in our experiments,ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3746,introduction,"['Quantitative evidence (e.g. experiments)', 'State-of-the-art']","We apply our pretrained modelas a base for four established vision-and-language tasks – visual question answering [3], visualcommonsense reasoning [25], referring expressions [2], and caption-based image retrieval [26] –setting state-of-the-art on all four tasks.",ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3747,introduction,"['Quantitative evidence (e.g. experiments)', 'State-of-the-art']",We find improvements of 2 to 10 percentage points acrossthese tasks when compared to state-of-the-art task-specific baselines using separately pretrainedvision and language models.,ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3748,introduction,['Flexibility/Extensibility'],"Furthermore, our structure is simple to modify for each of these tasks –serving as a common foundation for visual grounding across multiple vision-and-language tasks",ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3750,Conclusion,['State-of-the-art'],. Our ViLBERT model introduces a novel two-stream architecturewith co-attentional transformer blocks that outperforms sensible ablations and exceeds state-of-the-artwhen transferred to multiple established vision-and-language tasks,ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
3751,Conclusion,['Easy to implement'],"Furthermore, transferring ourmodel to these tasks is simple and easy to implement – requiring only the addition of a classifier foreach task we examined here",ViLBERT: Pretraining Task-Agnostic VisiolinguisticRepresentations for Vision-and-Language Tasks,https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf
